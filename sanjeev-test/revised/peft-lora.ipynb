{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a08d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LoRA FT - Summarization & Classification\n",
    "# ============================================================================\n",
    "# INSTALL DEPENDENCIES\n",
    "# ============================================================================\n",
    "import os\n",
    "import sys\n",
    "# Install required packages if running in Kaggle\n",
    "if os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\") is not None:\n",
    "    # Install evaluate and downgrade protobuf to fix compatibility issues\n",
    "    os.system(\"pip install -q evaluate protobuf==3.20.3\")\n",
    "# else:\n",
    "#     os.system(\"pip install -r /home/requirements.txt\")\n",
    "\n",
    "import os   \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dba32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "import traceback\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "import logging\n",
    "import warnings\n",
    "import json # For saving log_history if needed\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6105ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE DETECTION \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edb869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION \n",
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "SUMMARIZATION_DATASET = \"knkarthick/samsum\"\n",
    "\n",
    "BENCHMARK_GLUE=\"glue\"\n",
    "GLUE_DATASET_TASK_SC = \"sst2\"  # SST-2 for sentiment classification\n",
    "\n",
    "PROGRAM_NAME='peft-lora-sum'\n",
    "DATASET_SIZE = 'full' # 100 or 500 or 'full' \n",
    "# WARNING: DATASET_SIZE=100 is very small and only good for a 'smoke test'.\n",
    "# The resulting performance will be near random chance and not suitable for a real comparison.\n",
    "# Please set to 'full' or a larger number (e.g., 5000) for a meaningful benchmark.\n",
    "RUN_ABLATIONS = False  # Toggle to enable/disable ablation study (modular flag)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "NUM_VIRTUAL_TOKENS = 50 # CHANGE: Increased from 20 to 50 for better adaptation in prefix/prompt - Why: Longer tokens allow stronger task-specific tuning, fixing weak/flat metrics in prefix/prompt\n",
    "MAX_POS = 512\n",
    "\n",
    "# OUTPUT_DIR = f'/kaggle/working/outputs/{PROGRAM_NAME}'\n",
    "OUTPUT_DIR = f'/home/outputs/{PROGRAM_NAME}'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d618a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"LoRA COMPARISON - flan-t5-small\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset size: {DATASET_SIZE}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(\"Methods: LoRA\")\n",
    "if RUN_ABLATIONS:\n",
    "    print(\"Ablations Enabled: Including ablated variants for study\")\n",
    "    print(\"Note: For LoRA ablation, using lora_alpha=0 to nullify adapter effect\")\n",
    "print(\"=\"*60)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a13a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_dataset_size(dataset, size):\n",
    "    if size == 'full':\n",
    "        return dataset\n",
    "    if isinstance(size, int) and size > 0:\n",
    "        return dataset.select(range(min(size, len(dataset))))\n",
    "    raise ValueError(f\"Invalid size: {size}\")\n",
    "\n",
    "def setup_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "def safe_cleanup():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec44861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(log_history, exp_name, task_name, save_dir=f\"{OUTPUT_DIR}/plots\"):\n",
    "    \"\"\"Plot train/eval loss and task-specific metrics vs step.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "   \n",
    "    # Extract data\n",
    "    #steps = [log['step'] for log in log_history if 'step' in log and 'eval_loss' not in log] # Get train steps\n",
    "    eval_steps = [log['step'] for log in log_history if 'eval_loss' in log] # Get eval steps\n",
    "    train_losses = [log['loss'] for log in log_history if 'loss' in log] # 'loss' is train loss\n",
    "    eval_losses = [log['eval_loss'] for log in log_history if 'eval_loss' in log]\n",
    "   \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "   \n",
    "    # Loss curve\n",
    "    # Match train loss steps to eval steps for cleaner plots if they differ\n",
    "    train_steps_for_loss = [log['step'] for log in log_history if 'loss' in log]\n",
    "    axes[0].plot(train_steps_for_loss, train_losses, label='Train Loss', marker='o', alpha=0.7)\n",
    "    if eval_losses:\n",
    "        axes[0].plot(eval_steps, eval_losses, label='Eval Loss', marker='s')\n",
    "    axes[0].set_xlabel('Step')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title(f'{exp_name} - Loss Curve')\n",
    "    axes[0].legend()\n",
    "   \n",
    "    # Task-specific metric\n",
    "    if task_name == \"classification\":\n",
    "        eval_accs = [log['eval_accuracy'] for log in log_history if 'eval_accuracy' in log]\n",
    "        if eval_accs:\n",
    "            axes[1].plot(eval_steps, eval_accs, label='Eval Accuracy', marker='o', color='green')\n",
    "            axes[1].set_ylabel('Accuracy')\n",
    "    else: # summarization\n",
    "        eval_rouge_ls = [log['eval_rougeL'] for log in log_history if 'eval_rougeL' in log]\n",
    "        if eval_rouge_ls:\n",
    "            axes[1].plot(eval_steps, eval_rouge_ls, label='Eval ROUGE-L', marker='o', color='green')\n",
    "            axes[1].set_ylabel('ROUGE-L')\n",
    "   \n",
    "    axes[1].set_xlabel('Step')\n",
    "    axes[1].set_title(f'{exp_name} - {task_name.capitalize()} Metric')\n",
    "    axes[1].legend()\n",
    "   \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(save_dir, f\"{exp_name}_curves.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Learning curves saved to {plot_path}\")\n",
    "    return plot_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cdb237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ablation_comparisons(results, task_name, save_dir=f\"{OUTPUT_DIR}/plots\"):\n",
    "    \"\"\"Graphical analysis: Compare baselines vs ablations for a task.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    methods = list(results.keys())\n",
    "    baselines = [m for m in methods if \"_ablated_\" not in m]\n",
    "    ablations = [m for m in methods if \"_ablated_\" in m]\n",
    "    \n",
    "    if not ablations:\n",
    "        return None\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Trainable params comparison\n",
    "    trainable_pcts = [100 * results[m][\"trainable_params\"] / results[m][\"total_params\"] for m in methods]\n",
    "    sns.barplot(x=methods, y=trainable_pcts, ax=axes[0])\n",
    "    axes[0].set_ylabel('Trainable %')\n",
    "    axes[0].set_title(f'Trainable Params Comparison - {task_name.capitalize()}')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Metric comparison (use key metric)\n",
    "    if task_name == \"classification\":\n",
    "        metrics = [results[m][\"test_metrics\"].get(\"eval_accuracy\", 0) for m in methods]\n",
    "        metric_label = 'Accuracy'\n",
    "    else:\n",
    "        metrics = [results[m][\"test_metrics\"].get(\"eval_rougeL\", 0) for m in methods]\n",
    "        metric_label = 'ROUGE-L'\n",
    "    \n",
    "    sns.barplot(x=methods, y=metrics, ax=axes[1])\n",
    "    axes[1].set_ylabel(metric_label)\n",
    "    axes[1].set_title(f'Performance Comparison - {task_name.capitalize()}')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(save_dir, f\"ablation_comparison_{task_name}.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Ablation comparison plot saved to {plot_path}\")\n",
    "    return plot_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6859adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading datasets\")\n",
    "# Classification dataset - SST-2\n",
    "classification_dataset = load_dataset(BENCHMARK_GLUE, GLUE_DATASET_TASK_SC)\n",
    "Summarization dataset - SAMSum\n",
    "summarization_dataset = load_dataset(SUMMARIZATION_DATASET)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = setup_tokenizer(MODEL_NAME)\n",
    "\n",
    "if DATASET_SIZE != 'full':\n",
    "    print(f\"Limiting dataset size to {DATASET_SIZE} for train.\")\n",
    "    classification_dataset['train'] = limit_dataset_size(classification_dataset['train'], DATASET_SIZE)\n",
    "    classification_dataset['validation'] = limit_dataset_size(classification_dataset['validation'], DATASET_SIZE // 4)\n",
    "    classification_dataset['test'] = limit_dataset_size(classification_dataset.get('test', classification_dataset['validation']), DATASET_SIZE // 4)\n",
    "    \n",
    "    summarization_dataset['train'] = limit_dataset_size(summarization_dataset['train'], DATASET_SIZE)\n",
    "    summarization_dataset['validation'] = limit_dataset_size(summarization_dataset['validation'], DATASET_SIZE // 4)\n",
    "    summarization_dataset['test'] = limit_dataset_size(summarization_dataset['test'], DATASET_SIZE // 4)\n",
    "\n",
    "print(\"Datasets loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27fc1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Sample Datasets\")\n",
    "\n",
    "print(\"Classification Train Samples (Before Preprocessing):\")\n",
    "for i in range(min(10, len(classification_dataset['train']))):\n",
    "    print(classification_dataset[\"train\"][i])\n",
    "\n",
    "print(\"\\nSummarization Train Samples (Before Preprocessing):\")\n",
    "for i in range(min(10, len(summarization_dataset['train']))):\n",
    "    print(summarization_dataset[\"train\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd36e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for Classification\n",
    "def preprocess_classification(examples):\n",
    "    # Create input sentences with the required prefix\n",
    "    inputs = [f\"Classify sentiment: {text}\" for text in examples[\"sentence\"]]\n",
    "    \n",
    "    # Define max length for inputs\n",
    "    max_input_len = MAX_POS - NUM_VIRTUAL_TOKENS\n",
    "    \n",
    "    # Tokenize inputs with truncation and padding\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_len, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Convert labels from numerical to text\n",
    "    labels_text = [\"negative\" if label == 0 else \"positive\" for label in examples[\"label\"]]\n",
    "    \n",
    "    # Tokenize labels similar to inputs\n",
    "    labels = tokenizer(text_target=labels_text, max_length=10, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Add tokenized labels to model inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7854dc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_summarization(examples):\n",
    "    # Create input dialogues with the required prefix\n",
    "    inputs = [f\"Summarize the following conversation:\\n{dialogue}\" for dialogue in examples[\"dialogue\"]]\n",
    "    \n",
    "    # Define max length for inputs\n",
    "    max_input_len = MAX_POS - NUM_VIRTUAL_TOKENS\n",
    "    \n",
    "    # Tokenize inputs with truncation and padding\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_len, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Define max length for summaries\n",
    "    max_label_len = 128 - NUM_VIRTUAL_TOKENS\n",
    "    \n",
    "    # Tokenize summaries with truncation and padding\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=max_label_len, truncation=True, padding=\"max_length\").input_ids\n",
    "    \n",
    "    # Add tokenized summaries to model inputs\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a701a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nApplying preprocessing...\")\n",
    "tokenized_classification = classification_dataset.map(preprocess_classification, batched=True, remove_columns=classification_dataset[\"train\"].column_names)\n",
    "tokenized_summarization = summarization_dataset.map(preprocess_summarization, batched=True, remove_columns=summarization_dataset[\"train\"].column_names)\n",
    "\n",
    "# Print samples from each post preprocessing\n",
    "POST_PROCESS_SAMPLES = 5\n",
    "\n",
    "print(\"\\nPost-Preprocessing Sample Datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4eb846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode a single example (input + label)\n",
    "def _decode_example(example: dict, tokenizer, task: str) -> dict:\n",
    "    \"\"\"\n",
    "    Returns a dict with:\n",
    "        - \"input_text\"   : the original prompt (e.g. \"Classify sentiment: …\")\n",
    "        - \"label_text\"   : the gold label (positive/negative or the full summary)\n",
    "        - \"input_ids\"    : first 30 tokens (for sanity check)\n",
    "        - \"label_ids\"    : first 15 tokens of the label\n",
    "    \"\"\"\n",
    "    # 1. Decode the **input** (skip special tokens, keep the prompt)\n",
    "    input_txt = tokenizer.decode(example[\"input_ids\"], skip_special_tokens=False)\n",
    "    # remove the padding part after the EOS token\n",
    "    input_txt = input_txt.split(tokenizer.eos_token)[0] + tokenizer.eos_token\n",
    "\n",
    "    # 2. Decode the **label**\n",
    "    # Labels contain -100 for ignored positions → replace with pad token first\n",
    "    label_ids = [\n",
    "        tok_id if tok_id != -100 else tokenizer.pad_token_id for tok_id in example[\"labels\"]\n",
    "    ]\n",
    "    label_txt = tokenizer.decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # 3. Short token previews (optional, makes the output tidy)\n",
    "    input_preview = \" \".join(map(str, example[\"input_ids\"][:30]))\n",
    "    label_preview = \" \".join(map(str, label_ids[:15]))\n",
    "\n",
    "    return {\n",
    "        \"input_text\": input_txt,\n",
    "        \"label_text\": label_txt,\n",
    "        \"input_ids_preview\": input_preview,\n",
    "        \"label_ids_preview\": label_preview,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a37a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification samples\n",
    "print(\"\\n=== Classification – post-preprocessing ===\")\n",
    "for i, ex in enumerate(tokenized_classification[\"train\"].select(range(min(POST_PROCESS_SAMPLES, len(tokenized_classification[\"train\"]))))):\n",
    "    decoded = _decode_example(ex, tokenizer, task=\"classification\")\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"INPUT  : {decoded['input_text']}\")\n",
    "    print(f\"LABEL  : {decoded['label_text']}\")\n",
    "    # print(f\"input_ids  (first 30) : {decoded['input_ids_preview']}\")\n",
    "    # print(f\"label_ids  (first 15) : {decoded['label_ids_preview']}\")\n",
    "\n",
    "# Print summarisation samples\n",
    "print(\"\\n=== Summarisation – post-preprocessing (5 examples) ===\")\n",
    "for i, ex in enumerate(tokenized_summarization[\"train\"].select(range(min(POST_PROCESS_SAMPLES, len(tokenized_summarization[\"train\"]))))):\n",
    "    decoded = _decode_example(ex, tokenizer, task=\"summarization\")\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"INPUT  : {decoded['input_text']}\")\n",
    "    print(f\"SUMMARY: {decoded['label_text']}\")\n",
    "    # print(f\"input_ids  (first 30) : {decoded['input_ids_preview']}\")\n",
    "    # print(f\"label_ids  (first 15) : {decoded['label_ids_preview']}\")\n",
    "\n",
    "print(\"\\nPreprocessing complete\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b02e4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a174d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_metrics(eval_pred):\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        \n",
    "        # Handling prediction tensors\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        if len(predictions.shape) == 3:\n",
    "            predictions = np.argmax(predictions, axis=-1)\n",
    "        \n",
    "        # Replace -100 in labels with pad_token_id\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "\n",
    "        # Validate predictions and labels for negative values\n",
    "        if np.any(predictions < 0) or np.any(labels < 0):\n",
    "            logger.warning(f\"Found negative values in predictions or labels. Clamping to 0.\")\n",
    "            predictions = np.clip(predictions, 0, None)\n",
    "            labels = np.clip(labels, 0, None)\n",
    "        \n",
    "        # Decode the predictions and labels\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        # CHANGE: Added sample logging for debug - Why: To diagnose poor generations causing flat/low metrics\n",
    "        logger.info(f\"Sample pred: {decoded_preds[0]}, label: {decoded_labels[0]}\")  # Log first sample\n",
    "        \n",
    "        # Normalize the decoded texts\n",
    "        decoded_preds = [p.strip().lower() for p in decoded_preds]\n",
    "        decoded_labels = [l.strip().lower() for l in decoded_labels]\n",
    "        \n",
    "        # CHANGE: Use exact match instead of 'in' - Why: Prevents false positives from verbose outputs, fixing brittle mapping and low accuracy\n",
    "        pred_binary = [1 if p == 'positive' else 0 for p in decoded_preds]\n",
    "        label_binary = [1 if l == 'positive' else 0 for l in decoded_labels]\n",
    "        \n",
    "        # Compute metrics\n",
    "        acc = accuracy_metric.compute(predictions=pred_binary, references=label_binary)\n",
    "        f1 = f1_metric.compute(predictions=pred_binary, references=label_binary, average=\"weighted\")\n",
    "        \n",
    "        # CHANGE: Ensure keys always returned - Why: Fixes empty plots by guaranteeing 'eval_accuracy' in logs\n",
    "        return {\"accuracy\": acc.get(\"accuracy\", 0.0), \"f1\": f1.get(\"f1\", 0.0)}\n",
    "    \n",
    "    except Exception as e:\n",
    "        # CHANGE: More verbose error logging - Why: Catches silent failures causing empty plots/0.0 metrics\n",
    "        logger.error(f\"Classification metrics error: {e}. Returning defaults.\")\n",
    "        return {\"accuracy\": 0.0, \"f1\": 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c18e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summarization_metrics(eval_pred):\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        \n",
    "        # Handling prediction tensors\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        if len(predictions.shape) == 3:\n",
    "            predictions = np.argmax(predictions, axis=-1)\n",
    "        \n",
    "        # Replace -100 in predictions/labels with pad_token_id\n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "        # Validate predictions and labels for negative values\n",
    "        if np.any(predictions < 0) or np.any(labels < 0):\n",
    "            logger.warning(f\"Found negative values in predictions or labels. Clamping to 0.\")\n",
    "            predictions = np.clip(predictions, 0, None)\n",
    "            labels = np.clip(labels, 0, None)\n",
    "        \n",
    "        # Decode the predictions and labels\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        # CHANGE: Added sample logging for debug - Why: To inspect poor generations causing decreasing ROUGE\n",
    "        logger.info(f\"Sample pred: {decoded_preds[0]}, label: {decoded_labels[0]}\")  # Log first sample\n",
    "        \n",
    "        # Normalize the decoded texts\n",
    "        decoded_preds = [p.strip() if p.strip() else \"empty\" for p in decoded_preds]\n",
    "        decoded_labels = [l.strip() if l.strip() else \"empty\" for l in decoded_labels]\n",
    "        \n",
    "        # Compute ROUGE scores\n",
    "        result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        \n",
    "        # CHANGE: Ensure keys always returned - Why: Fixes empty plots by guaranteeing 'eval_rougeL' in logs\n",
    "        return {\n",
    "            \"rouge1\": result.get(\"rouge1\", 0.0),\n",
    "            \"rouge2\": result.get(\"rouge2\", 0.0),\n",
    "            \"rougeL\": result.get(\"rougeL\", 0.0),\n",
    "            \"rougeLsum\": result.get(\"rougeLsum\", 0.0)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        # CHANGE: More verbose error logging - Why: Catches silent failures in metrics computation\n",
    "        logger.error(f\"Summarization metrics error: {e}. Returning defaults.\")\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871aeeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING ARGS \n",
    "def get_training_args(method_name, task_name):\n",
    "    is_peft = method_name in [\"lora\"] or \"_ablated_\" in method_name\n",
    "    # CHANGE: Lowered LR for PEFT/ablation to 3e-4, Full FT to 1e-5 - Why: High LR caused instability/overfitting/decreasing metrics; matches t5-small recommendations\n",
    "    lr = 3e-4 if is_peft else 1e-5\n",
    "    \n",
    "    if DATASET_SIZE == 'full':\n",
    "        # CHANGE: Increased epochs to 5 for summarization - Why: Smaller dataset needs more passes for convergence, fixing underfitting/low ROUGE\n",
    "        epochs = 5 if task_name == 'summarization' else 3\n",
    "        batch, eval_steps = 8, 500\n",
    "    elif DATASET_SIZE <= 500:\n",
    "        # Use more epochs for very small datasets to allow for learning\n",
    "        epochs, batch, eval_steps = 10, 4, 20 # Eval more frequently\n",
    "    else:\n",
    "        epochs, batch, eval_steps = 3, 8, 100\n",
    "\n",
    "    # Adjust steps based on actual dataset size\n",
    "    if DATASET_SIZE != 'full':\n",
    "        total_steps = (DATASET_SIZE // batch) * epochs\n",
    "        # Ensure eval_steps is not 0 and is reasonable\n",
    "        eval_steps = max(1, min(total_steps // 5, 50)) # Eval 5 times per run, max 50\n",
    "        logging_steps = max(1, eval_steps // 2)\n",
    "        save_steps = eval_steps\n",
    "        eval_strategy = \"steps\"\n",
    "        save_strategy = \"steps\"\n",
    "    else:\n",
    "        eval_strategy = \"epoch\"\n",
    "        save_strategy = \"epoch\"\n",
    "        logging_steps = 100\n",
    "        save_steps = None\n",
    "        eval_steps = None\n",
    "\n",
    "    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    # CHANGE: Set fp16=True if not bf16 - Why: Faster training/mixed precision, fixing slow runs/low metrics if GPU supports\n",
    "    use_fp16 = not use_bf16 and torch.cuda.is_available()  # Enable fp16 on CUDA if bf16 unavailable\n",
    "    load_best = \"lora\" in method_name\n",
    "    \n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"{OUTPUT_DIR}/results/{task_name}/{method_name}\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch,\n",
    "        per_device_eval_batch_size=batch * 2,\n",
    "        learning_rate=lr,\n",
    "        # CHANGE: Increased warmup_steps to 1000 - Why: Smoother optimization start, fixing oscillation/stuck loss in full FT/ablations\n",
    "        warmup_steps=2000 if DATASET_SIZE == 'full' else min(100, DATASET_SIZE // 10),\n",
    "        # CHANGE: Increased weight_decay to 0.1 - Why: Stronger regularization prevents overfitting, fixing loss→0 but metrics drop\n",
    "        weight_decay=0.1,\n",
    "        eval_strategy=eval_strategy,\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy=save_strategy,\n",
    "        save_steps=save_steps,\n",
    "        load_best_model_at_end=load_best,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        save_total_limit=2,\n",
    "        logging_steps=logging_steps,\n",
    "        bf16=use_bf16,\n",
    "        fp16=use_fp16,\n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_drop_last=True, # Avoid incomplete batches for stability\n",
    "        report_to=\"none\",\n",
    "        predict_with_generate=True,\n",
    "        max_grad_norm=1.0,  # Added to prevent gradient explosions\n",
    "        # CHANGE: Added gradient_accumulation_steps=4 - Why: Stabilizes training with small effective batches, fixing oscillation in ablations\n",
    "        gradient_accumulation_steps=4,\n",
    "        # CHANGE: Removed label_smoothing_factor (set to default 0.0) - Why: Caused attribute error in Trainer with Prompt-Tuning; not essential\n",
    "        #label_smoothing_factor=0.1,\n",
    "        # CHANGE: Set optim to 'adamw_torch' - Why: More robust for PEFT, fixing instability in ablations/Full FT\n",
    "        optim='adafactor',\n",
    "        # CHANGE: Set gradient_checkpointing=False - Why: Avoids grad flow issues in PEFT/T5, fixing \"no grad_fn\" error; trade memory for stability\n",
    "        gradient_checkpointing=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e6ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN TRAINING LOOP\n",
    "\n",
    "# base_methods = [\"lora\", \"prefix\", \"prompt\", \"full_ft\"]\n",
    "# ablation_methods = [\"lora_ablated_alpha0\", \"prefix_ablated_no_proj\", \"prompt_ablated_short\"]\n",
    "# methods_to_run = base_methods + (ablation_methods if RUN_ABLATIONS else [])\n",
    "\n",
    "base_methods = [\"lora\"]\n",
    "ablation_methods = [\"lora_ablated_alpha0\"] if RUN_ABLATIONS else []\n",
    "methods_to_run = base_methods + ablation_methods\n",
    "tasks = {\n",
    "    \"classification\": (tokenized_classification, compute_classification_metrics),\n",
    "    \"summarization\": (tokenized_summarization, compute_summarization_metrics)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "os.makedirs(f\"{OUTPUT_DIR}/results\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/plots\", exist_ok=True) \n",
    "\n",
    "for method_name in methods_to_run:\n",
    "    for task_name, (dataset, compute_metrics) in tasks.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EXPERIMENT: {method_name.upper()} on {task_name.upper()}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        try:\n",
    "            config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "            if config.num_heads != 8:\n",
    "                config.num_heads = 8  # Fix the dimension mismatch bug\n",
    "            use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                config=config,\n",
    "                torch_dtype=torch.bfloat16 if use_bf16 else torch.float32,\n",
    "                ignore_mismatched_sizes=True  # Ignore size mismatches during loading\n",
    "            )\n",
    "\n",
    "            model.to(device)\n",
    "            \n",
    "            # Note: t5-small has correct dims (num_heads=8, head_dim=64); PEFT handles DynamicCache natively.\n",
    "            # Create PEFT configs dynamically from model.config\n",
    "            d_model = model.config.d_model\n",
    "            num_heads = model.config.num_heads\n",
    "            total_layers = model.config.num_layers + model.config.num_decoder_layers\n",
    "            peft_configs_local = {\n",
    "                \"lora\": LoraConfig(\n",
    "                    r=32,  # CHANGE: Increased r=32 from 16 for LoRA - Why: Higher rank for better capacity, boosting low metrics\n",
    "                    lora_alpha=32,\n",
    "                    target_modules=[\"q\", \"v\"],\n",
    "                    lora_dropout=0.05,\n",
    "                    bias=\"none\",\n",
    "                    task_type=TaskType.SEQ_2_SEQ_LM\n",
    "                ),\n",
    "                \"lora_ablated_alpha0\": LoraConfig(\n",
    "                    r=32,  # Match baseline r\n",
    "                    lora_alpha=0,  # Ablation: zero scaling, no effect from adapter\n",
    "                    target_modules=[\"q\", \"v\"],\n",
    "                    lora_dropout=0.05,\n",
    "                    bias=\"none\",\n",
    "                    task_type=TaskType.SEQ_2_SEQ_LM\n",
    "                )\n",
    "            }\n",
    "            model = get_peft_model(model, peft_configs_local[method_name])\n",
    "            model.print_trainable_parameters()\n",
    "            \n",
    "            training_args = get_training_args(method_name, task_name)\n",
    "            data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=dataset[\"train\"],\n",
    "                eval_dataset=dataset[\"validation\"],\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_metrics,\n",
    "                tokenizer=tokenizer\n",
    "            )\n",
    "\n",
    "            print(\"Training...\")\n",
    "            train_result = trainer.train()\n",
    "            \n",
    "            # Manual load best model for prefix/prompt methods\n",
    "            if not training_args.load_best_model_at_end and trainer.state.best_model_checkpoint:\n",
    "                print(f\"Loading best checkpoint manually: {trainer.state.best_model_checkpoint}\")\n",
    "                base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    MODEL_NAME,\n",
    "                    config=config,\n",
    "                    torch_dtype=torch.bfloat16 if use_bf16 else torch.float32,\n",
    "                )\n",
    "                base_model.to(device)\n",
    "                model = PeftModel.from_pretrained(base_model, trainer.state.best_model_checkpoint)\n",
    "                trainer.model = model\n",
    "                model.to(device)  # Ensure the full PEFT model is on device\n",
    "            \n",
    "            print(\"Evaluating...\")\n",
    "            test_dataset = dataset.get(\"test\", dataset[\"validation\"])\n",
    "            gen_kwargs = {\n",
    "                # CHANGE: For classification, max_length=5; summ=128; num_beams=6 - Why: Short for classification enforces concise labels (fixes verbose outputs/low acc); more beams improves quality (fixes poor ROUGE)\n",
    "                \"max_length\": 5 if task_name == \"classification\" else 128,\n",
    "                \"num_beams\": 6,\n",
    "                \"early_stopping\": True,\n",
    "            }\n",
    "            \n",
    "            # Set generation kwargs for trainer.evaluate\n",
    "            training_args.generation_max_length = gen_kwargs[\"max_length\"]\n",
    "            training_args.generation_num_beams = gen_kwargs[\"num_beams\"]\n",
    "            test_metrics = trainer.evaluate(test_dataset)\n",
    "            # CHANGE: Added trainer.predict for sample logging post-eval - Why: Debugs generations, fixing empty/low metrics\n",
    "            predictions = trainer.predict(dataset[\"validation\"])\n",
    "            # CHANGE: Clean predictions before decoding - Why: Handles -100/invalid IDs, fixing OverflowError in batch_decode\n",
    "            cleaned_predictions = np.where(predictions.predictions != -100, predictions.predictions, tokenizer.pad_token_id)\n",
    "            cleaned_predictions = np.clip(cleaned_predictions, 0, tokenizer.vocab_size - 1)\n",
    "            logger.info(f\"Sample generations: {tokenizer.batch_decode(cleaned_predictions[:5], skip_special_tokens=True)}\")\n",
    "            exp_name = f\"{method_name}_{task_name}\"\n",
    "            trainable = model.num_parameters(only_trainable=True) if hasattr(model, 'num_parameters') else sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            total = model.num_parameters() if hasattr(model, 'num_parameters') else sum(p.numel() for p in model.parameters())\n",
    "            \n",
    "            results[exp_name] = {\n",
    "                \"train_metrics\": train_result.metrics,\n",
    "                \"test_metrics\": test_metrics,\n",
    "                \"trainable_params\": trainable,\n",
    "                \"total_params\": total,\n",
    "                \"log_history\": trainer.state.log_history # Collect for plotting\n",
    "            }\n",
    "            \n",
    "            save_path = f\"{OUTPUT_DIR}/models/{task_name}/{method_name}\"\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            trainer.save_model(save_path)\n",
    "            print(f\"Completed and saved to {save_path}\\n\")\n",
    "            del model, trainer\n",
    "            safe_cleanup()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"ERROR in {method_name}_{task_name}: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            try:\n",
    "                del model, trainer\n",
    "            except:\n",
    "                pass\n",
    "            safe_cleanup()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL EXPERIMENTS COMPLETED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b253a321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS \n",
    "if results:\n",
    "    print(\"\\nRESULTS SUMMARY:\")\n",
    "    print(\"=\"*60)\n",
    "    for exp_name, exp_data in results.items():\n",
    "        # Handle cases where task name might have underscores\n",
    "        method_task_split = exp_name.split('_', 1)\n",
    "        method = method_task_split[0]\n",
    "        task = method_task_split[1] if len(method_task_split) > 1 else 'unknown'\n",
    "        \n",
    "        metrics = exp_data[\"test_metrics\"]\n",
    "        pct = 100 * exp_data[\"trainable_params\"] / exp_data[\"total_params\"]\n",
    "        print(f\"\\n{method.upper()} - {task.capitalize()}:\")\n",
    "        print(f\" Trainable: {pct:.2f}%\")\n",
    "        if task == \"classification\":\n",
    "            print(f\" Accuracy: {metrics.get('eval_accuracy', 0):.4f}\")\n",
    "            print(f\" F1: {metrics.get('eval_f1', 0):.4f}\")\n",
    "        else:\n",
    "            print(f\" ROUGE-1: {metrics.get('eval_rouge1', 0):.4f}\")\n",
    "            print(f\" ROUGE-L: {metrics.get('eval_rougeL', 0):.4f}\")\n",
    "\n",
    "    # Ablation deltas if enabled\n",
    "    if RUN_ABLATIONS:\n",
    "        print(\"\\nABLATION DELTAS:\")\n",
    "        for exp_name, exp_data in results.items():\n",
    "            if \"_ablated_\" in exp_name:\n",
    "                method_task_split = exp_name.split('_ablated_')[0]\n",
    "                task = exp_name.split('_', 1)[1] # Get task name\n",
    "                base_method_name = f\"{method_task_split}_{task}\"\n",
    "                \n",
    "                if base_method_name in results:\n",
    "                    base_metrics = results[base_method_name][\"test_metrics\"]\n",
    "                    delta = {k: exp_data[\"test_metrics\"].get(k, 0) - base_metrics.get(k, 0) for k in base_metrics if \"eval_\" in k}\n",
    "                    print(f\"Delta for {exp_name.upper()}: {delta}\")\n",
    "\n",
    "    # Plot learning curves for each experiment\n",
    "    print(\"\\nGenerating learning curves...\")\n",
    "    plot_paths = {}\n",
    "    plot_save_dir = f\"{OUTPUT_DIR}/plots\" # [FIX] Define plot save dir\n",
    "    for exp_name, exp_data in results.items():\n",
    "        task_name = exp_name.split(\"_\", 1)[1]\n",
    "        # [FIX] Pass the correct save_dir to the plotting function\n",
    "        plot_path = plot_learning_curves(exp_data[\"log_history\"], exp_name, task_name, save_dir=plot_save_dir)\n",
    "        plot_paths[exp_name] = plot_path\n",
    "    \n",
    "    # Graphical ablation comparisons per task\n",
    "    ablation_plot_paths = {}\n",
    "    if RUN_ABLATIONS:\n",
    "        print(\"\\nGenerating ablation comparison plots...\")\n",
    "        for task_name in tasks.keys():\n",
    "            task_results = {k: v for k, v in results.items() if k.endswith(f\"_{task_name}\")}\n",
    "            if task_results:\n",
    "                # [FIX] Pass the correct save_dir to the plotting function\n",
    "                ablation_plot_path = plot_ablation_comparisons(task_results, task_name, save_dir=plot_save_dir)\n",
    "                if ablation_plot_path:\n",
    "                    ablation_plot_paths[task_name] = ablation_plot_path\n",
    "\n",
    "    # --- Results DataFrame ---\n",
    "    results_df = []\n",
    "    for exp_name, exp_data in results.items():\n",
    "        method, task = exp_name.split(\"_\", 1)\n",
    "        results_df.append({\n",
    "            \"Method\": method.upper(),\n",
    "            \"Task\": task.capitalize(),\n",
    "            \"Trainable %\": 100 * exp_data[\"trainable_params\"] / exp_data[\"total_params\"],\n",
    "            **{k: v for k, v in exp_data[\"test_metrics\"].items() if isinstance(v, (int, float))}\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results_df)\n",
    "    cols = [\"Method\", \"Task\", \"Trainable %\"]\n",
    "    metric_cols = [c for c in df.columns if c.startswith(\"eval_\")]\n",
    "    cols.extend(sorted(metric_cols))\n",
    "    df = df[cols]\n",
    "    df.to_csv(f\"{OUTPUT_DIR}/lora_results.csv\", index=False)\n",
    "    print(f\"\\nResults saved to '{OUTPUT_DIR}/lora_results.csv'\")\n",
    "    \n",
    "    # --- Final Report --- \n",
    "    # Use relative paths for plots in the markdown report\n",
    "    report_path = f\"{OUTPUT_DIR}/lora_final_report.md\"\n",
    "    report_dir = os.path.dirname(report_path)\n",
    "\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(f\"# LoRA Adaptation Results - T5-small\\n\\n\")\n",
    "        f.write(f\"## Configuration\\n\")\n",
    "        f.write(f\"- Model: {MODEL_NAME} (switched from flan-t5-small to fix config dim bug)\\n\")\n",
    "        f.write(f\"- Dataset Size: {DATASET_SIZE}\\n\")\n",
    "        f.write(f\"- Methods: LoRA\\n\")\n",
    "        if RUN_ABLATIONS:\n",
    "            f.write(f\"- Ablations: Enabled (including ablated variants); LoRA ablation uses lora_alpha=0 for no adaptation effect\\n\")\n",
    "        f.write(f\"- Special: Native DynamicCache support; correct dims (num_heads=8, head_dim=64)\\n\\n\")\n",
    "        f.write(f\"## Summary Table\\n\\n\")\n",
    "        f.write(df.to_markdown(index=False))\n",
    "        f.write(\"\\n\\n## Learning Curves\\n\")\n",
    "        for exp_name, plot_path in plot_paths.items():\n",
    "            relative_plot_path = os.path.relpath(plot_path, start=report_dir)\n",
    "            f.write(f\"- [{exp_name}]({relative_plot_path})\\n\")\n",
    "        if RUN_ABLATIONS and ablation_plot_paths:\n",
    "            f.write(\"\\n## Ablation Comparisons\\n\")\n",
    "            for task_name, plot_path in ablation_plot_paths.items():\n",
    "                relative_plot_path = os.path.relpath(plot_path, start=report_dir)\n",
    "                f.write(f\"- [{task_name.capitalize()} Ablation Comparison]({relative_plot_path})\\n\")\n",
    "    \n",
    "    print(f\"Report saved to '{report_path}' (includes plot links)\")\n",
    "\n",
    "    # Generate dynamic outcome insights based on results\n",
    "    print(\"\\nOUTCOME INSIGHTS:\")\n",
    "    if results:\n",
    "        # General insights from trainable params and metrics\n",
    "        for task in tasks.keys():\n",
    "            task_exps = {k: v for k, v in results.items() if k.endswith(task)}\n",
    "            if task_exps:\n",
    "                # Find method with lowest trainable %\n",
    "                min_trainable_method = min(task_exps, key=lambda k: 100 * task_exps[k][\"trainable_params\"] / task_exps[k][\"total_params\"])\n",
    "                min_pct = 100 * task_exps[min_trainable_method][\"trainable_params\"] / task_exps[min_trainable_method][\"total_params\"]\n",
    "                print(f\"- For {task.capitalize()}, {min_trainable_method.split('_')[0].upper()} has the lowest trainable params ({min_pct:.2f}%).\")\n",
    "                \n",
    "                # Find best performing method (use key metric)\n",
    "                key_metric = 'eval_accuracy' if task == 'classification' else 'eval_rougeL'\n",
    "                best_method = max(task_exps, key=lambda k: task_exps[k][\"test_metrics\"].get(key_metric, 0))\n",
    "                best_score = task_exps[best_method][\"test_metrics\"].get(key_metric, 0)\n",
    "                print(f\"- {best_method.split('_')[0].upper()} achieves the highest {key_metric.replace('eval_', '').upper()} score ({best_score:.4f}) on {task.capitalize()}.\")\n",
    "        \n",
    "        # Ablation-specific insights\n",
    "        if RUN_ABLATIONS:\n",
    "            for exp_name, exp_data in results.items():\n",
    "                if \"_ablated_\" in exp_name:\n",
    "                    method_task_split = exp_name.split('_ablated_')[0]\n",
    "                    task = exp_name.split('_', 1)[1] # Get task name\n",
    "                    base_method_name = f\"{method_task_split}_{task}\"\n",
    "                    \n",
    "                    if base_method_name in results:\n",
    "                        base_metrics = results[base_method_name][\"test_metrics\"]\n",
    "                        delta = {k: exp_data[\"test_metrics\"].get(k, 0) - base_metrics.get(k, 0) for k in base_metrics if \"eval_\" in k}\n",
    "                        key_delta = delta.get('eval_accuracy' if task == 'classification' else 'eval_rougeL', 0)\n",
    "                        impact = \"degradation\" if key_delta < 0 else \"improvement\" if key_delta > 0 else \"no change\"\n",
    "                        print(f\"- Ablation in {exp_name.upper()} leads to {impact} in performance (delta: {key_delta:.4f}).\")\n",
    "        \n",
    "        print(f\"View plots in {OUTPUT_DIR}/plots/ for detailed curves (loss/metric vs step) and comparisons.\")\n",
    "else:\n",
    "    print(\"\\nNo results were generated. Check the training loop for errors.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LoRA method completed!\" + (\" With ablations!\" if RUN_ABLATIONS else \"\"))\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
