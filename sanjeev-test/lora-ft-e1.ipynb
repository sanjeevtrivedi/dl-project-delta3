{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130f6e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    PrefixTuningConfig,\n",
    "    PromptTuningConfig,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "import logging\n",
    "import warnings\n",
    "import json # For saving log_history if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6e32120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# KAGGLE TOGGLE \n",
    "IS_KAGGLE = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', None) is not None  # Detect if running on Kaggle\n",
    "if IS_KAGGLE:\n",
    "    os.system('pip install transformers==4.57.1 peft==0.17.1 datasets==4.3.0 torch==2.9.0 evaluate==0.4.6 rouge-score==0.1.2 scikit-learn==1.7.2 accelerate==1.11.0 matplotlib==3.10.7 seaborn==0.13.2 wandb==0.22.3 tabulate==0.9.0 --no-deps')\n",
    "\n",
    "# DEVICE DETECTION \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if IS_KAGGLE else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# CONFIGURATION \n",
    "MODEL_NAME = \"t5-small\" # Switched from flan-t5-small to avoid config dim bug (num_heads=6 mismatch)\n",
    "SUMMARIZATION_DATASET = \"knkarthick/samsum\"\n",
    "\n",
    "BENCHAMARK_GLUE=\"glue\"\n",
    "GLUE_DATASET_TASK_SC = \"sst2\"  # SST-2 for sentiment classification\n",
    "\n",
    "DATASET_SIZE = 50 # or 'full'\n",
    "RUN_ABLATIONS = False  # Toggle to enable/disable ablation study (modular flag)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "NUM_VIRTUAL_TOKENS = 20 # For truncation safety\n",
    "MAX_POS = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "912a3535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PEFT COMPARISON - T5-small\n",
      "============================================================\n",
      "Dataset size: 50\n",
      "Model: t5-small\n",
      "Methods: LoRA, Prefix-Tuning, Prompt-Tuning, Full FT\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PEFT COMPARISON - T5-small\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset size: {DATASET_SIZE}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(\"Methods: LoRA, Prefix-Tuning, Prompt-Tuning, Full FT\")\n",
    "if RUN_ABLATIONS:\n",
    "    print(\"Ablations Enabled: Including ablated variants for study\")\n",
    "    print(\"Note: For LoRA ablation, using lora_alpha=0 to nullify adapter effect\")\n",
    "print(\"=\"*60)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3620aa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# UTILITIES \n",
    "def limit_dataset_size(dataset, size):\n",
    "    if size == 'full':\n",
    "        return dataset\n",
    "    if isinstance(size, int) and size > 0:\n",
    "        return dataset.select(range(min(size, len(dataset))))\n",
    "    raise ValueError(f\"Invalid size: {size}\")\n",
    "\n",
    "def setup_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "def safe_cleanup():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    elif device.type == 'mps':\n",
    "        torch.mps.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f44a598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTS - can be moved down\n",
    "def plot_learning_curves(log_history, exp_name, task_name, save_dir=\"./plots\"):\n",
    "    \"\"\"Plot train/eval loss and task-specific metrics vs step.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "   \n",
    "    # Extract data\n",
    "    steps = [log['step'] for log in log_history if 'step' in log]\n",
    "    train_losses = [log['train_loss'] for log in log_history if 'train_loss' in log]\n",
    "    eval_losses = [log['eval_loss'] for log in log_history if 'eval_loss' in log]\n",
    "   \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "   \n",
    "    # Loss curve\n",
    "    axes[0].plot(steps[:len(train_losses)], train_losses, label='Train Loss', marker='o')\n",
    "    if eval_losses:\n",
    "        axes[0].plot(steps[:len(eval_losses)], eval_losses, label='Eval Loss', marker='s')\n",
    "    axes[0].set_xlabel('Step')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title(f'{exp_name} - Loss Curve')\n",
    "    axes[0].legend()\n",
    "   \n",
    "    # Task-specific metric\n",
    "    if task_name == \"classification\":\n",
    "        eval_accs = [log['eval_accuracy'] for log in log_history if 'eval_accuracy' in log]\n",
    "        if eval_accs:\n",
    "            axes[1].plot(steps[:len(eval_accs)], eval_accs, label='Eval Accuracy', marker='o', color='green')\n",
    "            axes[1].set_ylabel('Accuracy')\n",
    "    else: # summarization\n",
    "        eval_rouge_ls = [log['eval_rougeL'] for log in log_history if 'eval_rougeL' in log]\n",
    "        if eval_rouge_ls:\n",
    "            axes[1].plot(steps[:len(eval_rouge_ls)], eval_rouge_ls, label='Eval ROUGE-L', marker='o', color='green')\n",
    "            axes[1].set_ylabel('ROUGE-L')\n",
    "   \n",
    "    axes[1].set_xlabel('Step')\n",
    "    axes[1].set_title(f'{exp_name} - {task_name.capitalize()} Metric')\n",
    "    axes[1].legend()\n",
    "   \n",
    "    plt.tight_layout()\n",
    "    plot_path = f\"{save_dir}/{exp_name}_curves.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"✓ Learning curves saved to {plot_path}\")\n",
    "    return plot_path\n",
    "\n",
    "\n",
    "def plot_ablation_comparisons(results, task_name, save_dir=\"./plots\"):\n",
    "    \"\"\"Graphical analysis: Compare baselines vs ablations for a task.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    methods = list(results.keys())\n",
    "    baselines = [m for m in methods if \"_ablated_\" not in m]\n",
    "    ablations = [m for m in methods if \"_ablated_\" in m]\n",
    "    \n",
    "    if not ablations:\n",
    "        return None\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Trainable params comparison\n",
    "    trainable_pcts = [100 * results[m][\"trainable_params\"] / results[m][\"total_params\"] for m in methods]\n",
    "    sns.barplot(x=methods, y=trainable_pcts, ax=axes[0])\n",
    "    axes[0].set_ylabel('Trainable %')\n",
    "    axes[0].set_title(f'Trainable Params Comparison - {task_name.capitalize()}')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Metric comparison (use key metric)\n",
    "    if task_name == \"classification\":\n",
    "        metrics = [results[m][\"test_metrics\"].get(\"eval_accuracy\", 0) for m in methods]\n",
    "        metric_label = 'Accuracy'\n",
    "    else:\n",
    "        metrics = [results[m][\"test_metrics\"].get(\"eval_rougeL\", 0) for m in methods]\n",
    "        metric_label = 'ROUGE-L'\n",
    "    \n",
    "    sns.barplot(x=methods, y=metrics, ax=axes[1])\n",
    "    axes[1].set_ylabel(metric_label)\n",
    "    axes[1].set_title(f'Performance Comparison - {task_name.capitalize()}')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = f\"{save_dir}/ablation_comparison_{task_name}.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"✓ Ablation comparison plot saved to {plot_path}\")\n",
    "    return plot_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11d45b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Datasets loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## LOAD DATASETS \n",
    "print(\"Loading datasets\")\n",
    "# Classification dataset - SST-2\n",
    "classification_dataset = load_dataset(BENCHAMARK_GLUE, GLUE_DATASET_TASK_SC)\n",
    "# Summarization dataset - SAMSum\n",
    "summarization_dataset = load_dataset(SUMMARIZATION_DATASET)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = setup_tokenizer(MODEL_NAME)\n",
    "\n",
    "if DATASET_SIZE != 'full':\n",
    "\n",
    "    classification_dataset['train'] = limit_dataset_size(classification_dataset['train'], DATASET_SIZE)\n",
    "    classification_dataset['validation'] = limit_dataset_size(classification_dataset['validation'], DATASET_SIZE // 4)\n",
    "    classification_dataset['test'] = limit_dataset_size(classification_dataset.get('test', classification_dataset['validation']), DATASET_SIZE // 4)\n",
    "    \n",
    "    summarization_dataset['train'] = limit_dataset_size(summarization_dataset['train'], DATASET_SIZE)\n",
    "    summarization_dataset['validation'] = limit_dataset_size(summarization_dataset['validation'], DATASET_SIZE // 4)\n",
    "    summarization_dataset['test'] = limit_dataset_size(summarization_dataset['test'], DATASET_SIZE // 4)\n",
    "\n",
    "print(\"Datasets loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d4fd534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sample Datasets\n",
      "Classification Train Samples (Before Preprocessing):\n",
      "{'sentence': 'hide new secretions from the parental units ', 'label': 0, 'idx': 0}\n",
      "{'sentence': 'contains no wit , only labored gags ', 'label': 0, 'idx': 1}\n",
      "{'sentence': 'that loves its characters and communicates something rather beautiful about human nature ', 'label': 1, 'idx': 2}\n",
      "{'sentence': 'remains utterly satisfied to remain the same throughout ', 'label': 0, 'idx': 3}\n",
      "{'sentence': 'on the worst revenge-of-the-nerds clichés the filmmakers could dredge up ', 'label': 0, 'idx': 4}\n",
      "{'sentence': \"that 's far too tragic to merit such superficial treatment \", 'label': 0, 'idx': 5}\n",
      "{'sentence': 'demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . ', 'label': 1, 'idx': 6}\n",
      "{'sentence': 'of saucy ', 'label': 1, 'idx': 7}\n",
      "{'sentence': \"a depressed fifteen-year-old 's suicidal poetry \", 'label': 0, 'idx': 8}\n",
      "{'sentence': \"are more deeply thought through than in most ` right-thinking ' films \", 'label': 1, 'idx': 9}\n",
      "\n",
      "Summarization Train Samples (Before Preprocessing):\n",
      "{'id': '13818513', 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\nJerry: Sure!\\nAmanda: I'll bring you tomorrow :-)\", 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'}\n",
      "{'id': '13728867', 'dialogue': 'Olivia: Who are you voting for in this election? \\nOliver: Liberals as always.\\nOlivia: Me too!!\\nOliver: Great', 'summary': 'Olivia and Olivier are voting for liberals in this election. '}\n",
      "{'id': '13681000', 'dialogue': \"Tim: Hi, what's up?\\nKim: Bad mood tbh, I was going to do lots of stuff but ended up procrastinating\\nTim: What did you plan on doing?\\nKim: Oh you know, uni stuff and unfucking my room\\nKim: Maybe tomorrow I'll move my ass and do everything\\nKim: We were going to defrost a fridge so instead of shopping I'll eat some defrosted veggies\\nTim: For doing stuff I recommend Pomodoro technique where u use breaks for doing chores\\nTim: It really helps\\nKim: thanks, maybe I'll do that\\nTim: I also like using post-its in kaban style\", 'summary': 'Kim may try the pomodoro technique recommended by Tim to get more stuff done.'}\n",
      "{'id': '13730747', 'dialogue': \"Edward: Rachel, I think I'm in ove with Bella..\\nrachel: Dont say anything else..\\nEdward: What do you mean??\\nrachel: Open your fu**ing door.. I'm outside\", 'summary': 'Edward thinks he is in love with Bella. Rachel wants Edward to open his door. Rachel is outside. '}\n",
      "{'id': '13728094', 'dialogue': \"Sam: hey  overheard rick say something\\nSam: i don't know what to do :-/\\nNaomi: what did he say??\\nSam: he was talking on the phone with someone\\nSam: i don't know who\\nSam: and he was telling them that he wasn't very happy here\\nNaomi: damn!!!\\nSam: he was saying he doesn't like being my roommate\\nNaomi: wow, how do you feel about it?\\nSam: i thought i was a good rommate\\nSam: and that we have a nice place\\nNaomi: that's true man!!!\\nNaomi: i used to love living with you before i moved in with me boyfriend\\nNaomi: i don't know why he's saying that\\nSam: what should i do???\\nNaomi: honestly if it's bothering you that much you should talk to him\\nNaomi: see what's going on\\nSam: i don't want to get in any kind of confrontation though\\nSam: maybe i'll just let it go\\nSam: and see how it goes in the future\\nNaomi: it's your choice sam\\nNaomi: if i were you i would just talk to him and clear the air\", 'summary': 'Sam is confused, because he overheard Rick complaining about him as a roommate. Naomi thinks Sam should talk to Rick. Sam is not sure what to do.'}\n",
      "{'id': '13716343', 'dialogue': \"Neville: Hi there, does anyone remember what date I got married on?\\nDon: Are you serious?\\nNeville: Dead serious. We're on vacation, and Tina's mad at me about something. I have a strange suspicion that this might have something to do with our wedding anniversary, but I have nowhere to check.\\nWyatt: Hang on, I'll ask my wife.\\nDon: Haha, someone's in a lot of trouble :D\\nWyatt: September 17. I hope you remember the year ;)\", 'summary': \"Wyatt reminds Neville his wedding anniversary is on the 17th of September. Neville's wife is upset and it might be because Neville forgot about their anniversary.\"}\n",
      "{'id': '13611672', 'dialogue': \"John: Ave. Was there any homework for tomorrow?\\nCassandra: hello :D Of course, as always :D\\nJohn: What exactly?\\nCassandra: I'm not sure so I'll check it for you in 20minutes. \\nJohn: Cool, thanks. Sorry I couldn't be there, but I was busy as fuck...my stupid boss as always was trying to piss me off\\nCassandra: No problem, what did he do this time?\\nJohn: Nothing special, just the same as always, treating us like children, commanding to do this and that...\\nCassandra: sorry to hear that. but why don't you just go to your chief and tell him everything?\\nJohn: I would, but I don't have any support from others, they are like goddamn pupets and pretend that everything's fine...I'm not gonna fix everything for everyone\\nCassandra: I understand...Nevertheless, just try to ignore him. I know it might sound ridiculous as fuck, but sometimes there's nothing more you can do.\\nJohn: yeah I know...maybe some beer this week?\\nCassandra: Sure, but I got some time after classes only...this week is gonna be busy\\nJohn: no problem, I can drive you home and we can go to some bar or whatever.\\nCassandra: cool. ok, I got this homework. it's page 15 ex. 2 and 3, I also asked the others to study another chapter, especially the vocabulary from the very first pages. Just read it.\\nJohn: gosh...I don't know if I'm smart enough to do it :'D\\nCassandra: you are, don't worry :P Just circle all the words you don't know and we'll continue on Monday.\\nJohn: ok...then I'll try my best :D\\nCassandra: sure, if you will have any questions just either text or call me and I'll help you.\\nJohn: I hope I won't have to waste your time xD\\nCassandra: you're not wasting my time, I'm your teacher, I'm here to help. This is what I get money for, also :P\\nJohn: just kidding :D ok, so i guess we'll stay in touch then\\nCassandra: sure, have a nice evening :D\\nJohn: you too, se ya\\nCassandra: Byeeeee\", 'summary': \"John didn't show up for class due to some work issues with his boss. Cassandra, his teacher told him which exercises to do, and which chapter to study. They are going to meet up for a beer sometime this week after class. \"}\n",
      "{'id': '13730463', 'dialogue': \"Sarah: I found a song on youtube and I think you'll like it\\nJames: What song?\\nSarah: <file_other>\\nJames: Oh. I know it! \\nJames: I heard it before in some compilation\\nSarah: I can't stop playing it over and over\\nJames: That's exactly how I know lyrics to all of the songs on my playlist :D\\nSarah: Haha. No lyrics here though. Instrumental ;D\\nJames: Instrumental songs are different kind of music. \\nJames: But you have to remember that the activity you do when you listen to this song\\nJames: Is the actvity your brain will connect to the song\\nJames: And everytime you play this song at home\\nJames: You'll be thinking of your work\\nSarah: Yeah, I know that. That's why we sometimes say - I used to like that song, but now it just reminds me of bad memories\\nJames: Yup. Everytime you change your partner, you have to get rid of your favorite music :D\\nSarah: Hahaha. True, true.\", 'summary': 'Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to the context they were played in and brings to mind the associated memories.'}\n",
      "{'id': '13809976', 'dialogue': 'Noah: When and where are we meeting? :)\\nMadison: I thought you were busy...?\\nNoah: Yeah, I WAS. I quit my job. \\nMadison: No way! :o :o :o Why? I thought you liked it...?\\nNoah: Well, I used to, until my boss turned into a complete cock... Long story.', 'summary': 'Noah wants to meet, he quit his job, because his boss was a dick.'}\n",
      "{'id': '13809912', 'dialogue': \"Matt: Do you want to go for date?\\nAgnes: Wow! You caught me out with this question Matt.\\nMatt: Why?\\nAgnes: I simply didn't expect this from you.\\nMatt: Well, expect the unexpected.\\nAgnes: Can I think about it?\\nMatt: What is there to think about?\\nAgnes: Well, I don't really know you.\\nMatt: This is the perfect time to get to know eachother\\nAgnes: Well that's true.\\nMatt: So let's go to the Georgian restaurant in Kazimierz.\\nAgnes: Now your convincing me.\\nMatt: Cool, saturday at 6pm?\\nAgnes: That's fine.\\nMatt: I can pick you up on the way to the restaurant.\\nAgnes: That's really kind of you.\\nMatt: No problem.\\nAgnes: See you on saturday.\\nMatt: Yes, looking forward to it.\\nAgnes: Me too.\", 'summary': \"Matt invites Agnes for a date to get to know each other better. They'll go to the Georgian restaurant in Kazimierz on Saturday at 6 pm, and he'll pick her up on the way to the place.\"}\n"
     ]
    }
   ],
   "source": [
    "# Print 10 samples from each train dataset before preprocessing\n",
    "print(\"Original Sample Datasets\")\n",
    "\n",
    "print(\"Classification Train Samples (Before Preprocessing):\")\n",
    "for i in range(10):\n",
    "    print(classification_dataset[\"train\"][i])\n",
    "\n",
    "print(\"\\nSummarization Train Samples (Before Preprocessing):\")\n",
    "for i in range(10):\n",
    "    print(summarization_dataset[\"train\"][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "973e8594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sample Datasets\n",
      "Classification Train Samples (Before Preprocessing):\n",
      "{'sentence': 'hide new secretions from the parental units ', 'label': 0, 'idx': 0}\n",
      "{'sentence': 'contains no wit , only labored gags ', 'label': 0, 'idx': 1}\n",
      "{'sentence': 'that loves its characters and communicates something rather beautiful about human nature ', 'label': 1, 'idx': 2}\n",
      "{'sentence': 'remains utterly satisfied to remain the same throughout ', 'label': 0, 'idx': 3}\n",
      "{'sentence': 'on the worst revenge-of-the-nerds clichés the filmmakers could dredge up ', 'label': 0, 'idx': 4}\n",
      "{'sentence': \"that 's far too tragic to merit such superficial treatment \", 'label': 0, 'idx': 5}\n",
      "{'sentence': 'demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . ', 'label': 1, 'idx': 6}\n",
      "{'sentence': 'of saucy ', 'label': 1, 'idx': 7}\n",
      "{'sentence': \"a depressed fifteen-year-old 's suicidal poetry \", 'label': 0, 'idx': 8}\n",
      "{'sentence': \"are more deeply thought through than in most ` right-thinking ' films \", 'label': 1, 'idx': 9}\n",
      "\n",
      "Summarization Train Samples (Before Preprocessing):\n",
      "{'id': '13818513', 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\nJerry: Sure!\\nAmanda: I'll bring you tomorrow :-)\", 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'}\n",
      "{'id': '13728867', 'dialogue': 'Olivia: Who are you voting for in this election? \\nOliver: Liberals as always.\\nOlivia: Me too!!\\nOliver: Great', 'summary': 'Olivia and Olivier are voting for liberals in this election. '}\n",
      "{'id': '13681000', 'dialogue': \"Tim: Hi, what's up?\\nKim: Bad mood tbh, I was going to do lots of stuff but ended up procrastinating\\nTim: What did you plan on doing?\\nKim: Oh you know, uni stuff and unfucking my room\\nKim: Maybe tomorrow I'll move my ass and do everything\\nKim: We were going to defrost a fridge so instead of shopping I'll eat some defrosted veggies\\nTim: For doing stuff I recommend Pomodoro technique where u use breaks for doing chores\\nTim: It really helps\\nKim: thanks, maybe I'll do that\\nTim: I also like using post-its in kaban style\", 'summary': 'Kim may try the pomodoro technique recommended by Tim to get more stuff done.'}\n",
      "{'id': '13730747', 'dialogue': \"Edward: Rachel, I think I'm in ove with Bella..\\nrachel: Dont say anything else..\\nEdward: What do you mean??\\nrachel: Open your fu**ing door.. I'm outside\", 'summary': 'Edward thinks he is in love with Bella. Rachel wants Edward to open his door. Rachel is outside. '}\n",
      "{'id': '13728094', 'dialogue': \"Sam: hey  overheard rick say something\\nSam: i don't know what to do :-/\\nNaomi: what did he say??\\nSam: he was talking on the phone with someone\\nSam: i don't know who\\nSam: and he was telling them that he wasn't very happy here\\nNaomi: damn!!!\\nSam: he was saying he doesn't like being my roommate\\nNaomi: wow, how do you feel about it?\\nSam: i thought i was a good rommate\\nSam: and that we have a nice place\\nNaomi: that's true man!!!\\nNaomi: i used to love living with you before i moved in with me boyfriend\\nNaomi: i don't know why he's saying that\\nSam: what should i do???\\nNaomi: honestly if it's bothering you that much you should talk to him\\nNaomi: see what's going on\\nSam: i don't want to get in any kind of confrontation though\\nSam: maybe i'll just let it go\\nSam: and see how it goes in the future\\nNaomi: it's your choice sam\\nNaomi: if i were you i would just talk to him and clear the air\", 'summary': 'Sam is confused, because he overheard Rick complaining about him as a roommate. Naomi thinks Sam should talk to Rick. Sam is not sure what to do.'}\n",
      "{'id': '13716343', 'dialogue': \"Neville: Hi there, does anyone remember what date I got married on?\\nDon: Are you serious?\\nNeville: Dead serious. We're on vacation, and Tina's mad at me about something. I have a strange suspicion that this might have something to do with our wedding anniversary, but I have nowhere to check.\\nWyatt: Hang on, I'll ask my wife.\\nDon: Haha, someone's in a lot of trouble :D\\nWyatt: September 17. I hope you remember the year ;)\", 'summary': \"Wyatt reminds Neville his wedding anniversary is on the 17th of September. Neville's wife is upset and it might be because Neville forgot about their anniversary.\"}\n",
      "{'id': '13611672', 'dialogue': \"John: Ave. Was there any homework for tomorrow?\\nCassandra: hello :D Of course, as always :D\\nJohn: What exactly?\\nCassandra: I'm not sure so I'll check it for you in 20minutes. \\nJohn: Cool, thanks. Sorry I couldn't be there, but I was busy as fuck...my stupid boss as always was trying to piss me off\\nCassandra: No problem, what did he do this time?\\nJohn: Nothing special, just the same as always, treating us like children, commanding to do this and that...\\nCassandra: sorry to hear that. but why don't you just go to your chief and tell him everything?\\nJohn: I would, but I don't have any support from others, they are like goddamn pupets and pretend that everything's fine...I'm not gonna fix everything for everyone\\nCassandra: I understand...Nevertheless, just try to ignore him. I know it might sound ridiculous as fuck, but sometimes there's nothing more you can do.\\nJohn: yeah I know...maybe some beer this week?\\nCassandra: Sure, but I got some time after classes only...this week is gonna be busy\\nJohn: no problem, I can drive you home and we can go to some bar or whatever.\\nCassandra: cool. ok, I got this homework. it's page 15 ex. 2 and 3, I also asked the others to study another chapter, especially the vocabulary from the very first pages. Just read it.\\nJohn: gosh...I don't know if I'm smart enough to do it :'D\\nCassandra: you are, don't worry :P Just circle all the words you don't know and we'll continue on Monday.\\nJohn: ok...then I'll try my best :D\\nCassandra: sure, if you will have any questions just either text or call me and I'll help you.\\nJohn: I hope I won't have to waste your time xD\\nCassandra: you're not wasting my time, I'm your teacher, I'm here to help. This is what I get money for, also :P\\nJohn: just kidding :D ok, so i guess we'll stay in touch then\\nCassandra: sure, have a nice evening :D\\nJohn: you too, se ya\\nCassandra: Byeeeee\", 'summary': \"John didn't show up for class due to some work issues with his boss. Cassandra, his teacher told him which exercises to do, and which chapter to study. They are going to meet up for a beer sometime this week after class. \"}\n",
      "{'id': '13730463', 'dialogue': \"Sarah: I found a song on youtube and I think you'll like it\\nJames: What song?\\nSarah: <file_other>\\nJames: Oh. I know it! \\nJames: I heard it before in some compilation\\nSarah: I can't stop playing it over and over\\nJames: That's exactly how I know lyrics to all of the songs on my playlist :D\\nSarah: Haha. No lyrics here though. Instrumental ;D\\nJames: Instrumental songs are different kind of music. \\nJames: But you have to remember that the activity you do when you listen to this song\\nJames: Is the actvity your brain will connect to the song\\nJames: And everytime you play this song at home\\nJames: You'll be thinking of your work\\nSarah: Yeah, I know that. That's why we sometimes say - I used to like that song, but now it just reminds me of bad memories\\nJames: Yup. Everytime you change your partner, you have to get rid of your favorite music :D\\nSarah: Hahaha. True, true.\", 'summary': 'Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to the context they were played in and brings to mind the associated memories.'}\n",
      "{'id': '13809976', 'dialogue': 'Noah: When and where are we meeting? :)\\nMadison: I thought you were busy...?\\nNoah: Yeah, I WAS. I quit my job. \\nMadison: No way! :o :o :o Why? I thought you liked it...?\\nNoah: Well, I used to, until my boss turned into a complete cock... Long story.', 'summary': 'Noah wants to meet, he quit his job, because his boss was a dick.'}\n",
      "{'id': '13809912', 'dialogue': \"Matt: Do you want to go for date?\\nAgnes: Wow! You caught me out with this question Matt.\\nMatt: Why?\\nAgnes: I simply didn't expect this from you.\\nMatt: Well, expect the unexpected.\\nAgnes: Can I think about it?\\nMatt: What is there to think about?\\nAgnes: Well, I don't really know you.\\nMatt: This is the perfect time to get to know eachother\\nAgnes: Well that's true.\\nMatt: So let's go to the Georgian restaurant in Kazimierz.\\nAgnes: Now your convincing me.\\nMatt: Cool, saturday at 6pm?\\nAgnes: That's fine.\\nMatt: I can pick you up on the way to the restaurant.\\nAgnes: That's really kind of you.\\nMatt: No problem.\\nAgnes: See you on saturday.\\nMatt: Yes, looking forward to it.\\nAgnes: Me too.\", 'summary': \"Matt invites Agnes for a date to get to know each other better. They'll go to the Georgian restaurant in Kazimierz on Saturday at 6 pm, and he'll pick her up on the way to the place.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 2438.32 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 1254.78 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 1333.29 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 1376.30 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 937.29 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 923.06 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-Preprocessing Sample Datasets\n",
      "Classification Samples:\n",
      "{'input_ids': [4501, 4921, 6493, 10, 7387, 126, 2829, 2865, 45, 8, 21555, 3173, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2841, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': [4501, 4921, 6493, 10, 2579, 150, 3, 7820, 3, 6, 163, 5347, 15, 26, 23856, 7, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2841, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': [4501, 4921, 6493, 10, 24, 5682, 165, 2850, 11, 4521, 7, 424, 1066, 786, 81, 936, 1405, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [1465, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': [4501, 4921, 6493, 10, 3048, 3, 5108, 120, 7035, 12, 2367, 8, 337, 1019, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2841, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': [4501, 4921, 6493, 10, 30, 8, 6025, 27831, 18, 858, 18, 532, 18, 687, 26, 7, 27319, 7, 8, 21075, 7, 228, 3, 26, 52, 9390, 95, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2841, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': [4501, 4921, 6493, 10, 24, 3, 31, 7, 623, 396, 17414, 12, 7565, 224, 25307, 1058, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2841, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': [4501, 4921, 6493, 10, 3, 21275, 24, 8, 2090, 13, 224, 3, 107, 30322, 2463, 3465, 4849, 38, 6234, 12884, 1031, 54, 341, 919, 91, 3, 9, 422, 3, 6, 525, 814, 28, 46, 3973, 1481, 32, 102, 3, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [1465, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': [4501, 4921, 6493, 10, 13, 392, 75, 63, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [1465, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': [4501, 4921, 6493, 10, 3, 9, 20, 8918, 17310, 18, 1201, 18, 1490, 3, 31, 7, 2629, 1294, 26, 138, 10773, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2841, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': [4501, 4921, 6493, 10, 33, 72, 7447, 816, 190, 145, 16, 167, 3, 2, 269, 18, 10337, 53, 3, 31, 4852, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [1465, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "Summarization Samples:\n",
      "{'input_ids': [12198, 1635, 1737, 8, 826, 3634, 10, 21542, 10, 27, 13635, 5081, 5, 531, 25, 241, 128, 58, 16637, 10, 10625, 55, 21542, 10, 27, 31, 195, 830, 25, 5721, 3, 10, 18, 61, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [21542, 13635, 5081, 11, 56, 830, 16637, 128, 5721, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': [12198, 1635, 1737, 8, 826, 3634, 10, 25051, 10, 2645, 33, 25, 10601, 21, 16, 48, 4356, 58, 15865, 10, 18587, 7, 38, 373, 5, 25051, 10, 1212, 396, 1603, 15865, 10, 1651, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [25051, 11, 20373, 5144, 33, 10601, 21, 10215, 7, 16, 48, 4356, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': [12198, 1635, 1737, 8, 826, 3634, 10, 4485, 10, 2018, 6, 125, 31, 7, 95, 58, 6777, 10, 3862, 6526, 3, 17, 115, 107, 6, 27, 47, 352, 12, 103, 1995, 13, 2005, 68, 3492, 95, 813, 2935, 7, 17, 77, 1014, 4485, 10, 363, 410, 25, 515, 30, 692, 58, 6777, 10, 3359, 25, 214, 6, 73, 23, 2005, 11, 73, 89, 4636, 53, 82, 562, 6777, 10, 3836, 5721, 27, 31, 195, 888, 82, 38, 7, 11, 103, 762, 6777, 10, 101, 130, 352, 12, 20, 6155, 7, 17, 3, 9, 11416, 78, 1446, 13, 2309, 27, 31, 195, 3, 1544, 128, 20, 6155, 6265, 17984, 4485, 10, 242, 692, 2005, 27, 1568, 1908, 51, 7512, 32, 3317, 213, 3, 76, 169, 9797, 21, 692, 15030, 7, 4485, 10, 94, 310, 1691, 6777, 10, 2049, 6, 2087, 27, 31, 195, 103, 24, 4485, 10, 27, 92, 114, 338, 442, 18, 7085, 16, 3, 1258, 3478, 869, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [6777, 164, 653, 8, 3, 17043, 7512, 32, 3317, 3024, 57, 4485, 12, 129, 72, 2005, 612, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': [12198, 1635, 1737, 8, 826, 3634, 10, 8200, 10, 15868, 6, 27, 317, 27, 31, 51, 16, 3, 32, 162, 28, 5377, 9, 5, 5, 3, 52, 9, 8738, 10, 1008, 17, 497, 959, 1307, 5, 5, 8200, 10, 363, 103, 25, 1243, 8546, 3, 52, 9, 8738, 10, 2384, 39, 7683, 19844, 53, 1365, 5, 5, 27, 31, 51, 1067, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [8200, 317, 7, 3, 88, 19, 16, 333, 28, 5377, 9, 5, 15868, 2746, 8200, 12, 539, 112, 1365, 5, 15868, 19, 1067, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': [12198, 1635, 1737, 8, 826, 3634, 10, 3084, 10, 3, 13133, 147, 88, 986, 3, 5206, 497, 424, 3084, 10, 3, 23, 278, 31, 17, 214, 125, 12, 103, 3, 10, 18, 87, 31195, 10, 125, 410, 3, 88, 497, 8546, 3084, 10, 3, 88, 47, 2508, 30, 8, 951, 28, 841, 3084, 10, 3, 23, 278, 31, 17, 214, 113, 3084, 10, 11, 3, 88, 47, 5188, 135, 24, 3, 88, 2088, 31, 17, 182, 1095, 270, 31195, 10, 17227, 3158, 3084, 10, 3, 88, 47, 2145, 3, 88, 744, 31, 17, 114, 271, 82, 562, 5058, 31195, 10, 2275, 210, 6, 149, 103, 25, 473, 81, 34, 58, 3084, 10, 3, 23, 816, 3, 23, 47, 3, 9, 207, 3, 3522, 5058, 3084, 10, 11, 24, 62, 43, 3, 9, 1245, 286, 31195, 10, 24, 31, 7, 1176, 388, 3158, 31195, 10, 3, 23, 261, 12, 333, 840, 28, 25, 274, 3, 23, 2301, 16, 28, 140, 18124, 31195, 10, 3, 23, 278, 31, 17, 214, 572, 3, 88, 31, 7, 2145, 24, 3084, 10, 125, 225, 3, 23, 103, 8665, 31195, 10, 12855, 3, 99, 34, 31, 7, 13965, 53, 25, 24, 231, 25, 225, 1350, 12, 376, 31195, 10, 217, 125, 31, 7, 352, 30, 3084, 10, 3, 23, 278, 31, 17, 241, 12, 129, 16, 136, 773, 13, 25704, 713, 3084, 10, 2087, 3, 23, 31, 195, 131, 752, 34, 281, 3084, 10, 11, 217, 149, 34, 1550, 16, 8, 647, 31195, 10, 34, 31, 7, 39, 1160, 3, 7, 265, 31195, 10, 3, 99, 3, 23, 130, 25, 3, 23, 133, 131, 1350, 12, 376, 11, 964, 8, 799, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [3084, 19, 11319, 6, 250, 3, 88, 147, 88, 986, 11066, 24184, 81, 376, 38, 3, 9, 562, 5058, 5, 31195, 317, 7, 3084, 225, 1350, 12, 11066, 5, 3084, 19, 59, 417, 125, 12, 103, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': [12198, 1635, 1737, 8, 826, 3634, 10, 1484, 1420, 10, 2018, 132, 6, 405, 1321, 1423, 125, 833, 27, 530, 4464, 30, 58, 1008, 10, 1521, 25, 2261, 58, 1484, 1420, 10, 9651, 2261, 5, 101, 31, 60, 30, 4257, 6, 11, 7138, 9, 31, 7, 11454, 44, 140, 81, 424, 5, 27, 43, 3, 9, 6765, 23373, 24, 48, 429, 43, 424, 12, 103, 28, 69, 1683, 7685, 6, 68, 27, 43, 16715, 12, 691, 5, 11314, 144, 17, 10, 15280, 30, 6, 27, 31, 195, 987, 82, 2512, 5, 1008, 10, 1626, 1024, 6, 841, 31, 7, 16, 3, 9, 418, 13, 3169, 3, 10, 308, 11314, 144, 17, 10, 1600, 11030, 27, 897, 25, 1423, 8, 215, 3, 117, 61, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [11314, 144, 17, 5607, 7, 1484, 1420, 112, 1683, 7685, 19, 30, 8, 1003, 189, 13, 1600, 5, 1484, 1420, 31, 7, 2512, 19, 13423, 11, 34, 429, 36, 250, 1484, 1420, 15687, 81, 70, 7685, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': [12198, 1635, 1737, 8, 826, 3634, 10, 1079, 10, 8945, 5, 2751, 132, 136, 11920, 21, 5721, 58, 1336, 7, 7, 152, 3515, 10, 21820, 3, 10, 308, 1129, 503, 6, 38, 373, 3, 10, 308, 1079, 10, 363, 1776, 58, 1336, 7, 7, 152, 3515, 10, 27, 31, 51, 59, 417, 78, 27, 31, 195, 691, 34, 21, 25, 16, 460, 6890, 7, 5, 1079, 10, 7823, 6, 2049, 5, 11342, 27, 2654, 31, 17, 36, 132, 6, 68, 27, 47, 3164, 38, 3, 89, 4636, 233, 2258, 13721, 7930, 38, 373, 47, 1119, 12, 2816, 7, 7, 140, 326, 1336, 7, 7, 152, 3515, 10, 465, 682, 6, 125, 410, 3, 88, 103, 48, 97, 58, 1079, 10, 9428, 534, 6, 131, 8, 337, 38, 373, 6, 10902, 178, 114, 502, 6, 4106, 53, 12, 103, 48, 11, 24, 233, 1336, 7, 7, 152, 3515, 10, 8032, 12, 1616, 24, 5, 68, 572, 278, 31, 17, 25, 131, 281, 12, 39, 5752, 11, 817, 376, 762, 58, 1079, 10, 27, 133, 6, 68, 27, 278, 31, 17, 43, 136, 380, 45, 717, 6, 79, 33, 114, 8581, 7812, 29, 4353, 4995, 7, 11, 17582, 24, 762, 31, 7, 1399, 233, 196, 31, 51, 59, 3, 13366, 2210, 762, 21, 921, 1336, 7, 7, 152, 3515, 10, 27, 734, 233, 13099, 6, 131, 653, 12, 9751, 376, 5, 27, 214, 34, 429, 1345, 15708, 38, 3, 89, 4636, 6, 68, 1664, 132, 31, 7, 1327, 72, 25, 54, 103, 5, 1079, 10, 17945, 27, 214, 233, 25746, 128, 6061, 48, 471, 58, 1336, 7, 7, 152, 3515, 10, 10625, 6, 68, 27, 530, 128, 97, 227, 2287, 163, 233, 8048, 471, 19, 3, 13366, 36, 3164, 1079, 10, 150, 682, 6, 27, 54, 1262, 25, 234, 11, 62, 54, 281, 12, 128, 1207, 42, 2891, 5, 1336, 7, 7, 152, 3515, 10, 1633, 5, 3, 1825, 6, 27, 530, 48, 11920, 5, 34, 31, 7, 543, 627, 1215, 5, 204, 11, 6180, 27, 92, 1380, 8, 717, 12, 810, 430, 5800, 6, 902, 8, 19067, 45, 8, 182, 166, 1688, 5, 1142, 608, 34, 5, 1079, 10, 281, 7, 107, 233, 196, 278, 31, 17, 214, 3, 99, 27, 31, 51, 2592, 631, 12, 103, 34, 3, 10, 31, 308, 1336, 7, 7, 152, 3515, 10, 25, 33, 6, 278, 31, 17, 3516, 3, 10, 345, 1142, 8196, 66, 8, 1234, 25, 278, 31, 17, 214, 11, 62, 31, 195, 916, 30, 2089, 5, 1079, 10, 3, 1825, 233, 189, 35, 27, 31, 195, 653, 82, 200, 3, 10, 308, 1336, 7, 7, 152, 3515, 10, 417, 6, 3, 99, 25, 56, 43, 136, 746, 131, 893, 1499, 42, 580, 140, 11, 27, 31, 195, 199, 25, 5, 1079, 10, 27, 897, 27, 751, 31, 17, 43, 12, 2670, 39, 97, 3, 226, 308, 1336, 7, 7, 152, 3515, 10, 25, 31, 60, 59, 3, 26281, 82, 97, 6, 27, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1079, 737, 31, 17, 504, 95, 21, 853, 788, 12, 128, 161, 807, 28, 112, 7930, 5, 1336, 7, 7, 152, 3515, 6, 112, 3145, 1219, 376, 84, 8998, 12, 103, 6, 11, 84, 5800, 12, 810, 5, 328, 33, 352, 12, 942, 95, 21, 3, 9, 6061, 17078, 48, 471, 227, 853, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': [12198, 1635, 1737, 8, 826, 3634, 10, 8077, 10, 27, 435, 3, 9, 2324, 30, 25, 9863, 11, 27, 317, 25, 31, 195, 114, 34, 2549, 10, 363, 2324, 58, 8077, 10, 3, 2, 11966, 834, 9269, 3155, 2549, 10, 3359, 5, 27, 214, 34, 55, 2549, 10, 27, 1943, 34, 274, 16, 128, 23436, 8077, 10, 27, 54, 31, 17, 1190, 1556, 34, 147, 11, 147, 2549, 10, 466, 31, 7, 1776, 149, 27, 214, 13475, 12, 66, 13, 8, 3605, 30, 82, 23424, 3, 10, 308, 8077, 10, 1626, 1024, 5, 465, 13475, 270, 713, 5, 13507, 138, 3, 117, 308, 2549, 10, 13507, 138, 3605, 33, 315, 773, 13, 723, 5, 2549, 10, 299, 25, 43, 12, 1423, 24, 8, 1756, 25, 103, 116, 25, 3011, 12, 48, 2324, 2549, 10, 27, 7, 8, 1810, 208, 485, 39, 2241, 56, 1979, 12, 8, 2324, 2549, 10, 275, 334, 715, 25, 577, 48, 2324, 44, 234, 2549, 10, 148, 31, 195, 36, 1631, 13, 39, 161, 8077, 10, 11475, 6, 27, 214, 24, 5, 466, 31, 7, 572, 62, 1664, 497, 3, 18, 27, 261, 12, 114, 24, 2324, 6, 68, 230, 34, 131, 5607, 7, 140, 13, 1282, 5655, 2549, 10, 6214, 102, 5, 2181, 715, 25, 483, 39, 2397, 6, 25, 43, 12, 129, 5413, 13, 39, 1305, 723, 3, 10, 308, 8077, 10, 1626, 1024, 1024, 5, 10998, 6, 1176, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [8077, 1299, 7, 2549, 46, 15205, 2324, 3, 88, 429, 114, 5, 2549, 4054, 8, 2324, 5, 37, 2241, 1979, 7, 8, 3605, 12, 8, 2625, 79, 130, 1944, 16, 11, 3200, 12, 809, 8, 1968, 5655, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': [12198, 1635, 1737, 8, 826, 3634, 10, 24384, 10, 366, 11, 213, 33, 62, 1338, 58, 3, 10, 61, 16168, 10, 27, 816, 25, 130, 3164, 233, 58, 24384, 10, 11475, 6, 27, 7896, 134, 5, 27, 10399, 82, 613, 5, 16168, 10, 465, 194, 55, 3, 10, 32, 3, 10, 32, 3, 10, 32, 1615, 58, 27, 816, 25, 6528, 34, 233, 58, 24384, 10, 1548, 6, 27, 261, 12, 6, 552, 82, 7930, 2120, 139, 3, 9, 743, 3, 12105, 233, 3230, 733, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [24384, 2746, 12, 942, 6, 3, 88, 10399, 112, 613, 6, 250, 112, 7930, 47, 3, 9, 3, 26, 3142, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': [12198, 1635, 1737, 8, 826, 3634, 10, 5199, 10, 531, 25, 241, 12, 281, 21, 833, 58, 4821, 1496, 10, 9758, 55, 148, 4682, 140, 91, 28, 48, 822, 5199, 5, 5199, 10, 1615, 58, 4821, 1496, 10, 27, 914, 737, 31, 17, 1672, 48, 45, 25, 5, 5199, 10, 1548, 6, 1672, 8, 7544, 5, 4821, 1496, 10, 1072, 27, 317, 81, 34, 58, 5199, 10, 363, 19, 132, 12, 317, 81, 58, 4821, 1496, 10, 1548, 6, 27, 278, 31, 17, 310, 214, 25, 5, 5199, 10, 100, 19, 8, 626, 97, 12, 129, 12, 214, 284, 9269, 4821, 1496, 10, 1548, 24, 31, 7, 1176, 5, 5199, 10, 264, 752, 31, 7, 281, 12, 8, 5664, 29, 2062, 16, 26094, 4133, 49, 172, 5, 4821, 1496, 10, 852, 39, 20863, 140, 5, 5199, 10, 7823, 6, 3, 7, 6010, 1135, 44, 431, 2028, 58, 4821, 1496, 10, 466, 31, 7, 1399, 5, 5199, 10, 27, 54, 1432, 25, 95, 30, 8, 194, 12, 8, 2062, 5, 4821, 1496, 10, 466, 31, 7, 310, 773, 13, 25, 5, 5199, 10, 465, 682, 5, 4821, 1496, 10, 1610, 25, 30, 3, 7, 6010, 1135, 5, 5199, 10, 2163, 6, 479, 1039, 12, 34, 5, 4821, 1496, 10, 1212, 396, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [5199, 5484, 7, 4821, 1496, 21, 3, 9, 833, 12, 129, 12, 214, 284, 119, 394, 5, 328, 31, 195, 281, 12, 8, 5664, 29, 2062, 16, 26094, 4133, 49, 172, 30, 1856, 44, 431, 6366, 6, 11, 3, 88, 31, 195, 1432, 160, 95, 30, 8, 194, 12, 8, 286, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "Preprocessing complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocessing for Classification\n",
    "def preprocess_classification(examples):\n",
    "    # Create input sentences with the required prefix\n",
    "    inputs = [f\"Classify sentiment: {text}\" for text in examples[\"sentence\"]]\n",
    "    \n",
    "    # Define max length for inputs\n",
    "    max_input_len = MAX_POS - NUM_VIRTUAL_TOKENS\n",
    "    \n",
    "    # Tokenize inputs with truncation and padding\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_len, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Convert labels from numerical to text\n",
    "    labels_text = [\"negative\" if label == 0 else \"positive\" for label in examples[\"label\"]]\n",
    "    \n",
    "    # Tokenize labels similar to inputs\n",
    "    labels = tokenizer(text_target=labels_text, max_length=10, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Add tokenized labels to model inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Preprocessing for Summarization\n",
    "def preprocess_summarization(examples):\n",
    "    # Create input dialogues with the required prefix\n",
    "    inputs = [f\"Summarize the following conversation:\\n{dialogue}\" for dialogue in examples[\"dialogue\"]]\n",
    "    \n",
    "    # Define max length for inputs\n",
    "    max_input_len = MAX_POS - NUM_VIRTUAL_TOKENS\n",
    "    \n",
    "    # Tokenize inputs with truncation and padding\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_len, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Define max length for summaries\n",
    "    max_label_len = 128 - NUM_VIRTUAL_TOKENS\n",
    "    \n",
    "    # Tokenize summaries with truncation and padding\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=max_label_len, truncation=True, padding=\"max_length\").input_ids\n",
    "    \n",
    "    # Add tokenized summaries to model inputs\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Print 10 samples from each train dataset before preprocessing\n",
    "print(\"Original Sample Datasets\")\n",
    "\n",
    "print(\"Classification Train Samples (Before Preprocessing):\")\n",
    "for i in range(10):\n",
    "    print(classification_dataset[\"train\"][i])\n",
    "\n",
    "print(\"\\nSummarization Train Samples (Before Preprocessing):\")\n",
    "for i in range(10):\n",
    "    print(summarization_dataset[\"train\"][i])\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_classification = classification_dataset.map(preprocess_classification, batched=True, remove_columns=classification_dataset[\"train\"].column_names)\n",
    "tokenized_summarization = summarization_dataset.map(preprocess_summarization, batched=True, remove_columns=summarization_dataset[\"train\"].column_names)\n",
    "\n",
    "# Printing 10 samples from each post preprocessing\n",
    "print(\"Post-Preprocessing Sample Datasets\")\n",
    "\n",
    "print(\"Classification Samples:\")\n",
    "for i in range(10):\n",
    "    print(tokenized_classification[\"train\"][i])\n",
    "\n",
    "print(\"\\nSummarization Samples:\")\n",
    "for i in range(10):\n",
    "    print(tokenized_summarization[\"train\"][i])\n",
    "\n",
    "print(\"\\nPreprocessing complete\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9c51916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_classification_metrics(eval_pred):\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        \n",
    "        # Handling prediction tensors\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        if len(predictions.shape) == 3:\n",
    "            predictions = np.argmax(predictions, axis=-1)\n",
    "        \n",
    "        # Replace -100 in labels with pad_token_id\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "        # Validate predictions and labels for negative values\n",
    "        if np.any(predictions < 0) or np.any(labels < 0):\n",
    "            raise ValueError(\"Found negative values in predictions or labels\")\n",
    "        \n",
    "        # Decode the predictions and labels\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        # Normalize the decoded texts\n",
    "        decoded_preds = [p.strip().lower() for p in decoded_preds]\n",
    "        decoded_labels = [l.strip().lower() for l in decoded_labels]\n",
    "        \n",
    "        # Convert text predictions/labels to binary\n",
    "        pred_binary = [1 if 'positive' in p else 0 for p in decoded_preds]\n",
    "        label_binary = [1 if 'positive' in l else 0 for l in decoded_labels]\n",
    "        \n",
    "        # Compute metrics\n",
    "        acc = accuracy_metric.compute(predictions=pred_binary, references=label_binary)\n",
    "        f1 = f1_metric.compute(predictions=pred_binary, references=label_binary, average=\"weighted\")\n",
    "        \n",
    "        return {\"accuracy\": acc[\"accuracy\"], \"f1\": f1[\"f1\"]}\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Metrics error: {e}\")\n",
    "        return {\"accuracy\": 0.0, \"f1\": 0.0}\n",
    "\n",
    "def compute_summarization_metrics(eval_pred):\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        \n",
    "        # Handling prediction tensors\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        if len(predictions.shape) == 3:\n",
    "            predictions = np.argmax(predictions, axis=-1)\n",
    "        \n",
    "        # Replace -100 in predictions/labels with pad_token_id\n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "        # Validate predictions and labels for negative values\n",
    "        if np.any(predictions < 0) or np.any(labels < 0):\n",
    "            raise ValueError(\"Found negative values in predictions or labels\")\n",
    "        \n",
    "        # Decode the predictions and labels\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        # Normalize the decoded texts\n",
    "        decoded_preds = [p.strip() if p.strip() else \"empty\" for p in decoded_preds]\n",
    "        decoded_labels = [l.strip() if l.strip() else \"empty\" for l in decoded_labels]\n",
    "        \n",
    "        # Compute ROUGE scores\n",
    "        result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        \n",
    "        return {\n",
    "            \"rouge1\": result[\"rouge1\"],\n",
    "            \"rouge2\": result[\"rouge2\"],\n",
    "            \"rougeL\": result[\"rougeL\"],\n",
    "            \"rougeLsum\": result[\"rougeLsum\"]\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Metrics error: {e}\")\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88478538",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes=None, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, cbar=False, xticklabels=classes, yticklabels=classes)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def compute_and_plot_confusion_matrix_classification(decoded_labels, decoded_preds):\n",
    "    # Convert text labels to binary 0/1\n",
    "    label_binary = [1 if 'positive' in l else 0 for l in decoded_labels]\n",
    "    pred_binary = [1 if 'positive' in p else 0 for p in decoded_preds]\n",
    "    plot_confusion_matrix(label_binary, pred_binary, classes=['negative', 'positive'], title='Classification Confusion Matrix')\n",
    "\n",
    "def compute_and_plot_confusion_matrix_summarization(decoded_labels, decoded_preds, tokenizer):\n",
    "    # For summarization, generate token-level confusion matrix based on token matches\n",
    "    label_tokens = [tokenizer.tokenize(l) for l in decoded_labels]\n",
    "    pred_tokens = [tokenizer.tokenize(p) for p in decoded_preds]\n",
    "\n",
    "    true_tokens = []\n",
    "    pred_tokens_flat = []\n",
    "    for lt, pt in zip(label_tokens, pred_tokens):\n",
    "        min_len = min(len(lt), len(pt))\n",
    "        true_tokens.extend(lt[:min_len])\n",
    "        pred_tokens_flat.extend(pt[:min_len])\n",
    "\n",
    "    # Limit to top 10 tokens for visualization\n",
    "    all_tokens = list(set(true_tokens + pred_tokens_flat))\n",
    "    if len(all_tokens) > 10:\n",
    "        all_tokens = all_tokens[:10]\n",
    "\n",
    "    plot_confusion_matrix(true_tokens, pred_tokens_flat, classes=all_tokens, title='Summarization Token-level Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1da2cd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING ARGS \n",
    "def get_training_args(method_name, task_name):\n",
    "    is_peft = method_name in [\"lora\", \"prefix\", \"prompt\"] or \"_ablated_\" in method_name\n",
    "    lr = 1e-3 if is_peft else 5e-5\n",
    "    \n",
    "    if DATASET_SIZE == 'full':\n",
    "        epochs, batch, eval_steps = 3, 8, 500\n",
    "    elif DATASET_SIZE <= 500:\n",
    "        epochs, batch, eval_steps = 5, 4, 50\n",
    "    else:\n",
    "        epochs, batch, eval_steps = 3, 8, 100\n",
    "\n",
    "    ## TOD: Check Kaggle\n",
    "    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    use_fp16 = False  # Disabled to avoid NaN losses\n",
    "    load_best = method_name == \"full_ft\" or \"lora\" in method_name\n",
    "    \n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"./results/{task_name}/{method_name}\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch,\n",
    "        per_device_eval_batch_size=batch * 2,\n",
    "        learning_rate=lr,\n",
    "        warmup_steps=min(100, DATASET_SIZE // 10) if DATASET_SIZE != 'full' else 500,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"steps\" if DATASET_SIZE != 'full' else \"epoch\",\n",
    "        eval_steps=eval_steps if DATASET_SIZE != 'full' else None,\n",
    "        save_strategy=\"steps\" if DATASET_SIZE != 'full' else \"epoch\",\n",
    "        save_steps=eval_steps if DATASET_SIZE != 'full' else None,\n",
    "        load_best_model_at_end=load_best,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        save_total_limit=2,\n",
    "        logging_steps=20 if DATASET_SIZE != 'full' else 100,\n",
    "        bf16=use_bf16,\n",
    "        fp16=use_fp16,\n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_drop_last=True, # Avoid incomplete batches for stability\n",
    "        report_to=\"none\",\n",
    "        predict_with_generate=True,\n",
    "        max_grad_norm=1.0,  # Added to prevent gradient explosions\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a23e3a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXPERIMENT: LORA on CLASSIFICATION\n",
      "============================================================\n",
      "\n",
      "trainable params: 589,824 || all params: 61,096,448 || trainable%: 0.9654\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 03:00, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.282900</td>\n",
       "      <td>0.781488</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed and saved to ./models/classification/lora\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: LORA on SUMMARIZATION\n",
      "============================================================\n",
      "\n",
      "trainable params: 589,824 || all params: 61,096,448 || trainable%: 0.9654\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 04:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.663400</td>\n",
       "      <td>0.740636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed and saved to ./models/summarization/lora\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: PREFIX on CLASSIFICATION\n",
      "============================================================\n",
      "\n",
      "trainable params: 6,576,640 || all params: 67,083,264 || trainable%: 9.8037\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.127400</td>\n",
       "      <td>0.051097</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best checkpoint manually: ./results/classification/prefix\\checkpoint-50\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed and saved to ./models/classification/prefix\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: PREFIX on SUMMARIZATION\n",
      "============================================================\n",
      "\n",
      "trainable params: 6,576,640 || all params: 67,083,264 || trainable%: 9.8037\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.855800</td>\n",
       "      <td>0.518996</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best checkpoint manually: ./results/summarization/prefix\\checkpoint-50\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed and saved to ./models/summarization/prefix\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: PROMPT on CLASSIFICATION\n",
      "============================================================\n",
      "\n",
      "trainable params: 20,480 || all params: 60,527,104 || trainable%: 0.0338\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>9.915700</td>\n",
       "      <td>8.546119</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best checkpoint manually: ./results/classification/prompt\\checkpoint-50\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed and saved to ./models/classification/prompt\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: PROMPT on SUMMARIZATION\n",
      "============================================================\n",
      "\n",
      "trainable params: 20,480 || all params: 60,527,104 || trainable%: 0.0338\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>10.766100</td>\n",
       "      <td>9.797164</td>\n",
       "      <td>0.374461</td>\n",
       "      <td>0.130998</td>\n",
       "      <td>0.338430</td>\n",
       "      <td>0.335196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best checkpoint manually: ./results/summarization/prompt\\checkpoint-50\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed and saved to ./models/summarization/prompt\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: FULL_FT on CLASSIFICATION\n",
      "============================================================\n",
      "\n",
      "trainable params: 60,506,624 || all params: 60,506,624 || trainable%: 100.00\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.959100</td>\n",
       "      <td>2.185961</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed and saved to ./models/classification/full_ft\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: FULL_FT on SUMMARIZATION\n",
      "============================================================\n",
      "\n",
      "trainable params: 60,506,624 || all params: 60,506,624 || trainable%: 100.00\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 02:09, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.143500</td>\n",
       "      <td>1.980092</td>\n",
       "      <td>0.056548</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed and saved to ./models/summarization/full_ft\n",
      "\n",
      "\n",
      "============================================================\n",
      "ALL EXPERIMENTS COMPLETED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# MAIN TRAINING LOOP \n",
    "base_methods = [\"lora\", \"prefix\", \"prompt\", \"full_ft\"]\n",
    "ablation_methods = [\"lora_ablated_alpha0\", \"prefix_ablated_no_proj\", \"prompt_ablated_short\"]\n",
    "methods_to_run = base_methods + (ablation_methods if RUN_ABLATIONS else [])\n",
    "tasks = {\n",
    "    \"classification\": (tokenized_classification, compute_classification_metrics),\n",
    "    \"summarization\": (tokenized_summarization, compute_summarization_metrics)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "os.makedirs(\"./plots\", exist_ok=True) \n",
    "\n",
    "for method_name in methods_to_run:\n",
    "    for task_name, (dataset, compute_metrics) in tasks.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EXPERIMENT: {method_name.upper()} on {task_name.upper()}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        try:\n",
    "            config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "            use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                config=config,\n",
    "                dtype=torch.bfloat16 if use_bf16 else torch.float32,\n",
    "            )\n",
    "\n",
    "            model.to(device)\n",
    "            \n",
    "            # Note: t5-small has correct dims (num_heads=8, head_dim=64); PEFT handles DynamicCache natively.\n",
    "            # Create PEFT configs dynamically from model.config\n",
    "            if method_name != \"full_ft\":\n",
    "                d_model = model.config.d_model\n",
    "                num_heads = model.config.num_heads\n",
    "                total_layers = model.config.num_layers + model.config.num_decoder_layers\n",
    "                peft_configs_local = {\n",
    "                    \"lora\": LoraConfig(\n",
    "                        r=16,\n",
    "                        lora_alpha=32,\n",
    "                        target_modules=[\"q\", \"v\"],\n",
    "                        lora_dropout=0.05,\n",
    "                        bias=\"none\",\n",
    "                        task_type=TaskType.SEQ_2_SEQ_LM\n",
    "                    ),\n",
    "                    \"lora_ablated_alpha0\": LoraConfig(\n",
    "                        r=16,\n",
    "                        lora_alpha=0,  # Ablation: zero scaling, no effect from adapter\n",
    "                        target_modules=[\"q\", \"v\"],\n",
    "                        lora_dropout=0.05,\n",
    "                        bias=\"none\",\n",
    "                        task_type=TaskType.SEQ_2_SEQ_LM\n",
    "                    ),\n",
    "                    \"prefix\": PrefixTuningConfig(\n",
    "                        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "                        inference_mode=False,\n",
    "                        num_virtual_tokens=NUM_VIRTUAL_TOKENS,\n",
    "                        token_dim=d_model,\n",
    "                        num_transformer_submodules=2,\n",
    "                        num_attention_heads=num_heads,\n",
    "                        num_layers=total_layers,\n",
    "                        encoder_hidden_size=d_model,\n",
    "                        prefix_projection=True  # Baseline with projection\n",
    "                    ),\n",
    "                    \"prefix_ablated_no_proj\": PrefixTuningConfig(  # Ablation: Remove projection layer\n",
    "                        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "                        inference_mode=False,\n",
    "                        num_virtual_tokens=NUM_VIRTUAL_TOKENS,\n",
    "                        token_dim=d_model,\n",
    "                        num_transformer_submodules=2,\n",
    "                        num_attention_heads=num_heads,\n",
    "                        num_layers=total_layers,\n",
    "                        encoder_hidden_size=d_model,\n",
    "                        prefix_projection=False  # Ablated\n",
    "                    ),\n",
    "                    \"prompt\": PromptTuningConfig(\n",
    "                        num_virtual_tokens=NUM_VIRTUAL_TOKENS,\n",
    "                        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "                        prompt_tuning_init=\"RANDOM\"\n",
    "                    ),\n",
    "                    \"prompt_ablated_short\": PromptTuningConfig(  # Ablation: Fewer tokens (e.g., half)\n",
    "                        num_virtual_tokens=NUM_VIRTUAL_TOKENS // 2,\n",
    "                        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "                        prompt_tuning_init=\"RANDOM\"\n",
    "                    )\n",
    "                }\n",
    "                model = get_peft_model(model, peft_configs_local[method_name])\n",
    "                model.print_trainable_parameters()\n",
    "            else:\n",
    "                trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                total = sum(p.numel() for p in model.parameters())\n",
    "                print(f\"trainable params: {trainable:,} || all params: {total:,} || trainable%: 100.00\")\n",
    "            \n",
    "            training_args = get_training_args(method_name, task_name)\n",
    "            data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=dataset[\"train\"],\n",
    "                eval_dataset=dataset[\"validation\"],\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_metrics,\n",
    "                tokenizer=tokenizer\n",
    "            )\n",
    "\n",
    "            print(\"Training...\")\n",
    "            train_result = trainer.train()\n",
    "            \n",
    "            # Manual load best model for prefix/prompt methods\n",
    "            if not training_args.load_best_model_at_end and trainer.state.best_model_checkpoint:\n",
    "                print(f\"Loading best checkpoint manually: {trainer.state.best_model_checkpoint}\")\n",
    "                base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    MODEL_NAME,\n",
    "                    config=config,\n",
    "                    dtype=torch.bfloat16 if use_bf16 else torch.float32,\n",
    "                )\n",
    "                base_model.to(device)\n",
    "                model = PeftModel.from_pretrained(base_model, trainer.state.best_model_checkpoint)\n",
    "                trainer.model = model\n",
    "                model.to(device)  # Ensure the full PEFT model is on device\n",
    "            \n",
    "            print(\"Evaluating...\")\n",
    "            test_dataset = dataset.get(\"test\", dataset[\"validation\"])\n",
    "            gen_kwargs = {\n",
    "                \"max_length\": 128 if task_name == \"summarization\" else 10,\n",
    "                \"num_beams\": 4,\n",
    "                \"early_stopping\": True,\n",
    "            }\n",
    "            \n",
    "            training_args.generation_max_length = gen_kwargs[\"max_length\"]\n",
    "            training_args.generation_num_beams = gen_kwargs[\"num_beams\"]\n",
    "            test_metrics = trainer.evaluate(test_dataset)\n",
    "            exp_name = f\"{method_name}_{task_name}\"\n",
    "            trainable = model.num_parameters(only_trainable=True) if hasattr(model, 'num_parameters') else sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            total = model.num_parameters() if hasattr(model, 'num_parameters') else sum(p.numel() for p in model.parameters())\n",
    "            \n",
    "            results[exp_name] = {\n",
    "                \"train_metrics\": train_result.metrics,\n",
    "                \"test_metrics\": test_metrics,\n",
    "                \"trainable_params\": trainable,\n",
    "                \"total_params\": total,\n",
    "                \"log_history\": trainer.state.log_history # Collect for plotting\n",
    "            }\n",
    "            \n",
    "            save_path = f\"./models/{task_name}/{method_name}\"\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            trainer.save_model(save_path)\n",
    "            print(f\"Completed and saved to {save_path}\\n\")\n",
    "            del model, trainer\n",
    "            safe_cleanup()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"ERROR in {method_name}_{task_name}: {e}\")\n",
    "            import traceback\n",
    "            logger.error(traceback.format_exc())\n",
    "            try:\n",
    "                del model, trainer\n",
    "            except:\n",
    "                pass\n",
    "            safe_cleanup()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL EXPERIMENTS COMPLETED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93ce6bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTS SUMMARY:\n",
      "============================================================\n",
      "\n",
      "LORA - Classification:\n",
      " Trainable: 0.97%\n",
      " Accuracy: 0.0000\n",
      " F1: 0.0000\n",
      "\n",
      "LORA - Summarization:\n",
      " Trainable: 0.97%\n",
      " ROUGE-1: 0.0000\n",
      " ROUGE-L: 0.0000\n",
      "\n",
      "PREFIX - Classification:\n",
      " Trainable: 0.00%\n",
      " Accuracy: 0.0000\n",
      " F1: 0.0000\n",
      "\n",
      "PREFIX - Summarization:\n",
      " Trainable: 0.00%\n",
      " ROUGE-1: 0.1227\n",
      " ROUGE-L: 0.1238\n",
      "\n",
      "PROMPT - Classification:\n",
      " Trainable: 0.00%\n",
      " Accuracy: 0.0000\n",
      " F1: 0.0000\n",
      "\n",
      "PROMPT - Summarization:\n",
      " Trainable: 0.00%\n",
      " ROUGE-1: 0.2549\n",
      " ROUGE-L: 0.1876\n",
      "\n",
      "FULL - Ft_classification:\n",
      " Trainable: 100.00%\n",
      " ROUGE-1: 0.0000\n",
      " ROUGE-L: 0.0000\n",
      "\n",
      "FULL - Ft_summarization:\n",
      " Trainable: 100.00%\n",
      " ROUGE-1: 0.1798\n",
      " ROUGE-L: 0.1362\n",
      "\n",
      "Generating learning curves...\n",
      "✓ Learning curves saved to ./plots/lora_classification_curves.png\n",
      "✓ Learning curves saved to ./plots/lora_summarization_curves.png\n",
      "✓ Learning curves saved to ./plots/prefix_classification_curves.png\n",
      "✓ Learning curves saved to ./plots/prefix_summarization_curves.png\n",
      "✓ Learning curves saved to ./plots/prompt_classification_curves.png\n",
      "✓ Learning curves saved to ./plots/prompt_summarization_curves.png\n",
      "✓ Learning curves saved to ./plots/full_ft_classification_curves.png\n",
      "✓ Learning curves saved to ./plots/full_ft_summarization_curves.png\n",
      "\n",
      "Results saved to 'peft_results.csv'\n",
      "Report saved to 'final_report.md' (includes plot links)\n",
      "\n",
      "OUTCOME INSIGHTS:\n",
      "- For Classification, PREFIX has the lowest trainable params (0.00%).\n",
      "- LORA achieves the highest ACCURACY score (0.0000) on Classification.\n",
      "- For Summarization, PREFIX has the lowest trainable params (0.00%).\n",
      "- PROMPT achieves the highest ROUGEL score (0.1876) on Summarization.\n",
      "View plots in ./plots/ for detailed curves (loss/metric vs step) and comparisons.\n"
     ]
    }
   ],
   "source": [
    "# RESULTS \n",
    "if results:\n",
    "    print(\"\\nRESULTS SUMMARY:\")\n",
    "    print(\"=\"*60)\n",
    "    for exp_name, exp_data in results.items():\n",
    "        method, task = exp_name.split(\"_\", 1)\n",
    "        metrics = exp_data[\"test_metrics\"]\n",
    "        pct = 100 * exp_data[\"trainable_params\"] / exp_data[\"total_params\"]\n",
    "        print(f\"\\n{method.upper()} - {task.capitalize()}:\")\n",
    "        print(f\" Trainable: {pct:.2f}%\")\n",
    "        if task == \"classification\":\n",
    "            print(f\" Accuracy: {metrics.get('eval_accuracy', 0):.4f}\")\n",
    "            print(f\" F1: {metrics.get('eval_f1', 0):.4f}\")\n",
    "        else:\n",
    "            print(f\" ROUGE-1: {metrics.get('eval_rouge1', 0):.4f}\")\n",
    "            print(f\" ROUGE-L: {metrics.get('eval_rougeL', 0):.4f}\")\n",
    "\n",
    "    # Ablation deltas if enabled\n",
    "    if RUN_ABLATIONS:\n",
    "        print(\"\\nABLATION DELTAS:\")\n",
    "        for exp_name, exp_data in results.items():\n",
    "            method, task = exp_name.split(\"_\", 1)\n",
    "            if \"_ablated_\" in method:\n",
    "                base_method = method.split(\"_ablated_\")[0] + \"_\" + task\n",
    "                if base_method in results:\n",
    "                    base_metrics = results[base_method][\"test_metrics\"]\n",
    "                    delta = {k: exp_data[\"test_metrics\"].get(k, 0) - base_metrics.get(k, 0) for k in base_metrics if \"eval_\" in k}\n",
    "                    print(f\"Delta for {method.upper()} - {task.capitalize()}: {delta}\")\n",
    "\n",
    "    # Plot learning curves for each experiment\n",
    "    print(\"\\nGenerating learning curves...\")\n",
    "    plot_paths = {}\n",
    "    for exp_name, exp_data in results.items():\n",
    "        task_name = exp_name.split(\"_\", 1)[1]\n",
    "        plot_path = plot_learning_curves(exp_data[\"log_history\"], exp_name, task_name)\n",
    "        plot_paths[exp_name] = plot_path\n",
    "    \n",
    "    # Graphical ablation comparisons per task\n",
    "    if RUN_ABLATIONS:\n",
    "        print(\"\\nGenerating ablation comparison plots...\")\n",
    "        ablation_plot_paths = {}\n",
    "        for task_name in tasks.keys():\n",
    "            task_results = {k: v for k, v in results.items() if k.endswith(f\"_{task_name}\")}\n",
    "            if task_results:\n",
    "                ablation_plot_path = plot_ablation_comparisons(task_results, task_name)\n",
    "                if ablation_plot_path:\n",
    "                    ablation_plot_paths[task_name] = ablation_plot_path\n",
    "    results_df = []\n",
    "    for exp_name, exp_data in results.items():\n",
    "        method, task = exp_name.split(\"_\", 1)\n",
    "        results_df.append({\n",
    "            \"Method\": method.upper(),\n",
    "            \"Task\": task.capitalize(),\n",
    "            \"Trainable %\": 100 * exp_data[\"trainable_params\"] / exp_data[\"total_params\"],\n",
    "            **{k: v for k, v in exp_data[\"test_metrics\"].items() if isinstance(v, (int, float))}\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results_df)\n",
    "    cols = [\"Method\", \"Task\", \"Trainable %\"]\n",
    "    metric_cols = [c for c in df.columns if c.startswith(\"eval_\")]\n",
    "    cols.extend(sorted(metric_cols))\n",
    "    df = df[cols]\n",
    "    df.to_csv(\"peft_results.csv\", index=False)\n",
    "    print(f\"\\nResults saved to 'peft_results.csv'\")\n",
    "    \n",
    "    with open(\"final_report.md\", \"w\") as f:\n",
    "        f.write(f\"# PEFT Comparison Results - T5-small\\n\\n\")\n",
    "        f.write(f\"## Configuration\\n\")\n",
    "        f.write(f\"- Model: {MODEL_NAME} (switched from flan-t5-small to fix config dim bug)\\n\")\n",
    "        f.write(f\"- Dataset Size: {DATASET_SIZE}\\n\")\n",
    "        f.write(f\"- Methods: LoRA, Prefix-Tuning, Prompt-Tuning, Full Fine-Tuning\\n\")\n",
    "        if RUN_ABLATIONS:\n",
    "            f.write(f\"- Ablations: Enabled (including ablated variants); LoRA ablation uses lora_alpha=0 for no adaptation effect\\n\")\n",
    "        f.write(f\"- Special: Native DynamicCache support; correct dims (num_heads=8, head_dim=64)\\n\\n\")\n",
    "        f.write(f\"## Summary Table\\n\\n\")\n",
    "        f.write(df.to_markdown(index=False))\n",
    "        f.write(\"\\n\\n## Learning Curves\\n\")\n",
    "        for exp_name, plot_path in plot_paths.items():\n",
    "            f.write(f\"- [{exp_name}]({plot_path})\\n\")\n",
    "        if RUN_ABLATIONS and ablation_plot_paths:\n",
    "            f.write(\"\\n## Ablation Comparisons\\n\")\n",
    "            for task_name, plot_path in ablation_plot_paths.items():\n",
    "                f.write(f\"- [{task_name.capitalize()} Ablation Comparison]({plot_path})\\n\")\n",
    "    \n",
    "    print(\"Report saved to 'final_report.md' (includes plot links)\")\n",
    "\n",
    "    # Generate dynamic outcome insights based on results\n",
    "    print(\"\\nOUTCOME INSIGHTS:\")\n",
    "    if results:\n",
    "        # General insights from trainable params and metrics\n",
    "        for task in tasks.keys():\n",
    "            task_exps = {k: v for k, v in results.items() if k.endswith(task)}\n",
    "            if task_exps:\n",
    "                # Find method with lowest trainable %\n",
    "                min_trainable_method = min(task_exps, key=lambda k: 100 * task_exps[k][\"trainable_params\"] / task_exps[k][\"total_params\"])\n",
    "                min_pct = 100 * task_exps[min_trainable_method][\"trainable_params\"] / task_exps[min_trainable_method][\"total_params\"]\n",
    "                print(f\"- For {task.capitalize()}, {min_trainable_method.split('_')[0].upper()} has the lowest trainable params ({min_pct:.2f}%).\")\n",
    "                \n",
    "                # Find best performing method (use key metric)\n",
    "                key_metric = 'eval_accuracy' if task == 'classification' else 'eval_rougeL'\n",
    "                best_method = max(task_exps, key=lambda k: task_exps[k][\"test_metrics\"].get(key_metric, 0))\n",
    "                best_score = task_exps[best_method][\"test_metrics\"].get(key_metric, 0)\n",
    "                print(f\"- {best_method.split('_')[0].upper()} achieves the highest {key_metric.replace('eval_', '').upper()} score ({best_score:.4f}) on {task.capitalize()}.\")\n",
    "        \n",
    "        # Ablation-specific insights\n",
    "        if RUN_ABLATIONS:\n",
    "            for exp_name, exp_data in results.items():\n",
    "                method, task = exp_name.split(\"_\", 1)\n",
    "                if \"_ablated_\" in method:\n",
    "                    base_method = method.split(\"_ablated_\")[0] + \"_\" + task\n",
    "                    if base_method in results:\n",
    "                        base_metrics = results[base_method][\"test_metrics\"]\n",
    "                        delta = {k: exp_data[\"test_metrics\"].get(k, 0) - base_metrics.get(k, 0) for k in base_metrics if \"eval_\" in k}\n",
    "                        key_delta = delta.get('eval_accuracy' if task == 'classification' else 'eval_rougeL', 0)\n",
    "                        impact = \"degradation\" if key_delta < 0 else \"improvement\" if key_delta > 0 else \"no change\"\n",
    "                        print(f\"- Ablation in {method.upper()} on {task.capitalize()} leads to {impact} in performance (delta: {key_delta:.4f}).\")\n",
    "        \n",
    "        print(f\"View plots in ./plots/ for detailed curves (loss/metric vs step) and comparisons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "098a5345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUCCESS - All 4 PEFT methods completed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUCCESS - All 4 PEFT methods completed!\" + (\" With ablations!\" if RUN_ABLATIONS else \"\"))\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
