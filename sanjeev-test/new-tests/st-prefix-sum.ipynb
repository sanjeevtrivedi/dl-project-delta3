{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43022682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Prefix Tuning - Summarization\n",
    "# ============================================================================\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# If running on Kaggle, install packages quietly (keeps behavior from original)\n",
    "if os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\") is not None:\n",
    "    os.system(\"pip install -q --upgrade evaluate transformers peft protobuf==4.25.3\")\n",
    "# else:\n",
    "#     os.system(\"pip install -r /home/requirements.txt\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "from peft import PrefixTuningConfig, get_peft_model, PeftModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============================================================================\n",
    "# ENVIRONMENT DETECTION (SAFE: disable MPS for prefix tuning on Mac)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ENVIRONMENT DETECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "IS_MPS_AVAILABLE = torch.backends.mps.is_available()\n",
    "IS_CUDA = torch.cuda.is_available()\n",
    "IS_KAGGLE = os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\", None) is not None\n",
    "IS_MAC_MACHINE = sys.platform == \"darwin\"\n",
    "\n",
    "# IMPORTANT: For Prefix Tuning we avoid MPS due to cache/device bugs.\n",
    "# On a Mac we force CPU (rather than MPS). On Kaggle (if CUDA) we use CUDA.\n",
    "if IS_CUDA:\n",
    "    DEVICE = \"cuda\"\n",
    "    DEVICE_NAME = f\"CUDA - {torch.cuda.get_device_name(0)}\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    if IS_MAC_MACHINE and IS_MPS_AVAILABLE:\n",
    "        DEVICE_NAME = \"CPU Only (MPS available but DISABLED for Prefix Tuning)\"\n",
    "    else:\n",
    "        DEVICE_NAME = \"CPU Only\"\n",
    "\n",
    "print(f\" PyTorch Version: {torch.__version__}\")\n",
    "print(f\" MPS Available: {IS_MPS_AVAILABLE}\")\n",
    "print(f\" CUDA Available: {IS_CUDA}\")\n",
    "print(f\" Kaggle Environment: {IS_KAGGLE}\")\n",
    "print(f\" Using Device: {DEVICE_NAME}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ENVIRONMENT CONFIGURATION\n",
    "# ============================================================================\n",
    "if IS_CUDA and not IS_MAC_MACHINE:\n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# ============================================================================\n",
    "# MEMORY MANAGEMENT HELPERS\n",
    "# ============================================================================\n",
    "def clear_gpu_memory():\n",
    "    # We only attempt CUDA empty cache; MPS empty cache is not used now.\n",
    "    try:\n",
    "        if IS_CUDA and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    except Exception:\n",
    "        pass\n",
    "    gc.collect()\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    if IS_CUDA:\n",
    "        return float(torch.cuda.memory_allocated() / 1024**3)\n",
    "    return 0.0\n",
    "\n",
    "def print_gpu_status():\n",
    "    if IS_CUDA:\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        used = get_gpu_memory_usage()\n",
    "        print(f\" GPU Memory Used: {used:.2f} GB / {total:.2f} GB\")\n",
    "        print(f\" GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\" Using CPU - no GPU available\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION (you can change model/task here)\n",
    "# ============================================================================\n",
    "model_name = \"google/flan-t5-small\"\n",
    "task = \"summarization\"  # or \"classification\"\n",
    "dataset_size = 100 if IS_MAC_MACHINE else 'full'  # reduce for Mac\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\" Model: {model_name}\")\n",
    "print(f\" Task: {task}\")\n",
    "print(f\" Dataset Size: {dataset_size} (auto-reduced on Mac)\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD TOKENIZER\n",
    "# ============================================================================\n",
    "print(\"\\n Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\" Tokenizer loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# PREPROCESSING\n",
    "# ============================================================================\n",
    "def preprocess_summarization(examples):\n",
    "    inputs = [f\"summarize: {text}\" for text in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=512, truncation=True, padding=False\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"summary\"], max_length=128, truncation=True, padding=False\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_classification(examples):\n",
    "    label_map = {0: \"negative\", 1: \"positive\"}\n",
    "    inputs = [\n",
    "        f\"classify sentiment: {text}\" for text in examples[\"text\"]\n",
    "    ]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=512, truncation=True, padding=False\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        [label_map[label] for label in examples[\"label\"]],\n",
    "        max_length=8,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATASET\n",
    "# ============================================================================\n",
    "print(f\"\\n Loading {task} dataset...\")\n",
    "clear_gpu_memory()\n",
    "try:\n",
    "    if task == \"summarization\":\n",
    "        dataset = load_dataset(\"knkarthick/samsum\")\n",
    "        if \"val\" in dataset:\n",
    "            dataset[\"validation\"] = dataset.pop(\"val\")\n",
    "        dataset = dataset.rename_column(\"dialogue\", \"text\")\n",
    "        preprocess_function = preprocess_summarization\n",
    "        metric = evaluate.load(\"rouge\")\n",
    "        splits = [\"train\", \"validation\", \"test\"]\n",
    "    elif task == \"classification\":\n",
    "        dataset = load_dataset(\"imdb\")\n",
    "        if \"unsupervised\" in dataset:\n",
    "            del dataset[\"unsupervised\"]\n",
    "        train_val = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "        dataset[\"train\"] = train_val[\"train\"]\n",
    "        dataset[\"validation\"] = train_val[\"test\"]\n",
    "        preprocess_function = preprocess_classification\n",
    "        metric = evaluate.load(\"accuracy\")\n",
    "        splits = [\"train\", \"validation\", \"test\"]\n",
    "    else:\n",
    "        raise ValueError(\"Task must be 'summarization' or 'classification'\")\n",
    "    print(f\" Dataset loaded. Splits: {list(dataset.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\" Error loading dataset: {e}\")\n",
    "    print(\" Please check your internet connection\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# SUBSAMPLE DATASET (if requested)\n",
    "# ============================================================================\n",
    "print(f\"\\n Subsampling dataset (size={dataset_size})...\")\n",
    "if dataset_size != \"full\":\n",
    "    for split in splits:\n",
    "        if split in dataset:\n",
    "            available = len(dataset[split])\n",
    "            num_samples = min(\n",
    "                dataset_size if split == \"train\" else max(50, dataset_size // 2),\n",
    "                available,\n",
    "            )\n",
    "            dataset[split] = dataset[split].shuffle(seed=42).select(\n",
    "                range(num_samples)\n",
    "            )\n",
    "            print(f\" {split}: {num_samples} samples\")\n",
    "\n",
    "# ============================================================================\n",
    "# TOKENIZE DATASETS\n",
    "# ============================================================================\n",
    "print(\"\\n Tokenizing datasets...\")\n",
    "clear_gpu_memory()\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=32,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing\",\n",
    ")\n",
    "print(\" Datasets tokenized\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADING (base model), print config JSON, then lock PrefixTuningConfig\n",
    "# ============================================================================\n",
    "print(f\"\\n Loading model {model_name} with Prefix Tuning...\")\n",
    "clear_gpu_memory()\n",
    "try:\n",
    "    # Load base model into CPU first (safe)\n",
    "    print(\" Loading base model on CPU (device_map='cpu')...\")\n",
    "    base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"cpu\",\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    print(\" Base model loaded on CPU\")\n",
    "\n",
    "    # Print model.config JSON (so you have a record)\n",
    "    try:\n",
    "        cfg_dict = base_model.config.to_dict()\n",
    "        print(\"\\n--- MODEL CONFIG (start) ---\")\n",
    "        print(json.dumps(cfg_dict, indent=2))\n",
    "        print(\"--- MODEL CONFIG (end) ---\\n\")\n",
    "    except Exception:\n",
    "        print(\"\\nmodel.config:\", base_model.config)\n",
    "\n",
    "    # --- Derive safe, exact PrefixTuning constants from base_model.config ---\n",
    "    cfg = base_model.config\n",
    "\n",
    "    # num_layers: prefer decoder layers\n",
    "    num_layers = None\n",
    "    for attr in (\"num_decoder_layers\", \"decoder_layers\", \"num_layers\", \"num_hidden_layers\"):\n",
    "        if hasattr(cfg, attr) and getattr(cfg, attr) is not None:\n",
    "            num_layers = int(getattr(cfg, attr))\n",
    "            break\n",
    "    if num_layers is None:\n",
    "        num_layers = int(getattr(cfg, \"num_layers\", 12))\n",
    "\n",
    "    # num_attention_heads\n",
    "    num_attention_heads = None\n",
    "    for attr in (\"num_heads\", \"num_attention_heads\"):\n",
    "        if hasattr(cfg, attr) and getattr(cfg, attr) is not None:\n",
    "            num_attention_heads = int(getattr(cfg, attr))\n",
    "            break\n",
    "    if num_attention_heads is None:\n",
    "        num_attention_heads = 8\n",
    "\n",
    "    # token_dim: prefer d_kv (T5) else fallback to d_model\n",
    "    if hasattr(cfg, \"d_kv\") and cfg.d_kv is not None:\n",
    "        raw_d_kv = int(cfg.d_kv)\n",
    "    elif hasattr(cfg, \"d_model\") and cfg.d_model is not None:\n",
    "        raw_d_kv = int(cfg.d_model) // num_attention_heads\n",
    "    else:\n",
    "        raw_d_kv = 64\n",
    "\n",
    "    # For PEFT, token_dim is head_dim * num_attention_heads\n",
    "    token_dim = raw_d_kv * num_attention_heads\n",
    "\n",
    "    # choose num_virtual_tokens scaled for Mac\n",
    "    num_virtual_tokens = 10 if IS_MAC_MACHINE else 20\n",
    "\n",
    "    # Lock and create exact PrefixTuningConfig\n",
    "    prefix_config = PrefixTuningConfig(\n",
    "        task_type=\"SEQ_2_SEQ_LM\",\n",
    "        num_virtual_tokens=num_virtual_tokens,\n",
    "        prefix_projection=True,\n",
    "        num_layers=num_layers,\n",
    "        num_attention_heads=num_attention_heads,\n",
    "        token_dim=token_dim,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\" Applying PrefixTuningConfig -> num_virtual_tokens={num_virtual_tokens},\"\n",
    "        f\" num_layers={num_layers}, num_attention_heads={num_attention_heads}, token_dim={token_dim}\"\n",
    "    )\n",
    "\n",
    "    # Wrap base model with PEFT prefix adapter (wrapped on CPU initially)\n",
    "    model = get_peft_model(base_model, prefix_config)\n",
    "    print(\" PEFT wrapper created (currently on CPU)\")\n",
    "\n",
    "    # Move PEFT-wrapped model to device (single move)\n",
    "    print(f\" Moving PEFT-wrapped model to device: {DEVICE} ...\")\n",
    "    model = model.to(DEVICE)\n",
    "    print(\" Model moved to device\")\n",
    "\n",
    "    # Defensive: disable use_cache during training\n",
    "    try:\n",
    "        model.config.use_cache = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Optional check: print trainable params\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\" Prefix Tuning Applied | Trainable Params: {trainable} / {total}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Error loading/applying prefix tuning: {e}\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# DATA COLLATOR\n",
    "# ============================================================================\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer, model=model, padding=True\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# METRICS\n",
    "# ============================================================================\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, torch.Tensor):\n",
    "        predictions = predictions.cpu().numpy()\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.cpu().numpy()\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    # Replace -100 with pad_token_id (ignore_index)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    try:\n",
    "        decoded_preds = tokenizer.batch_decode(\n",
    "            predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "    except OverflowError:\n",
    "        max_id = tokenizer.vocab_size - 1\n",
    "        predictions = np.clip(predictions, 0, max_id)\n",
    "        decoded_preds = tokenizer.batch_decode(\n",
    "            predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "    decoded_labels = tokenizer.batch_decode(\n",
    "        labels, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    if task == \"summarization\":\n",
    "        try:\n",
    "            result = metric.compute(\n",
    "                predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "            )\n",
    "            return {\n",
    "                \"rouge1\": round(result[\"rouge1\"], 4),\n",
    "                \"rouge2\": round(result[\"rouge2\"], 4),\n",
    "                \"rougeL\": round(result[\"rougeL\"], 4),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\" Error computing ROUGE: {e}\")\n",
    "            return {\"rouge1\": 0.0}\n",
    "    else:\n",
    "        preds = [p.strip() for p in decoded_preds]\n",
    "        refs = [r.strip() for r in decoded_labels]\n",
    "        label_map_rev = {\"negative\": 0, \"positive\": 1}\n",
    "        pred_ids = [label_map_rev.get(p, -1) for p in preds]\n",
    "        ref_ids = [label_map_rev.get(r, -1) for r in refs]\n",
    "        pairs = [(p, r) for p, r in zip(pred_ids, ref_ids) if p != -1 and r != -1]\n",
    "        if not pairs:\n",
    "            return {\"accuracy\": 0.0}\n",
    "        pred_ids, ref_ids = zip(*pairs)\n",
    "        result = metric.compute(predictions=pred_ids, references=ref_ids)\n",
    "        return result\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING ARGUMENTS\n",
    "# ============================================================================\n",
    "print(\"\\n Configuring training arguments...\")\n",
    "num_epochs = 2 if IS_MAC_MACHINE else 5\n",
    "grad_accum = 2 if IS_MAC_MACHINE else 4\n",
    "fp16_enabled = IS_CUDA and not IS_MAC_MACHINE\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"./{task}_flan_t5_small_prefix_{dataset_size}_{'mac' if IS_MAC_MACHINE else 'gpu'}\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=grad_accum,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=fp16_enabled,\n",
    "    bf16=False,\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=5e-5,\n",
    "    seed=42,\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=1,\n",
    ")\n",
    "print(\" Training arguments configured\")\n",
    "\n",
    "# ============================================================================\n",
    "# INITIALIZE TRAINER\n",
    "# ============================================================================\n",
    "print(\"\\n Initializing Trainer...\")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets.get(\"validation\", tokenized_datasets[\"train\"]),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\" Trainer initialized\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAIN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "print_gpu_status()\n",
    "print()\n",
    "clear_gpu_memory()\n",
    "try:\n",
    "    model.train()\n",
    "    trainer.train()\n",
    "    print(\"\\n Training completed successfully!\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n Training interrupted by user\")\n",
    "except RuntimeError as e:\n",
    "    print(\"\\n RuntimeError during training:\", e)\n",
    "    if \"does not require grad\" in str(e):\n",
    "        print(\" Check that PEFT parameters are trainable and gradient_checkpointing is False.\")\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\" Out of memory. Try reducing dataset_size or lowering num_virtual_tokens.\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# PLOTTING METRICS (unchanged)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PLOTTING METRICS\")\n",
    "print(\"=\" * 80)\n",
    "log_history = trainer.state.log_history\n",
    "# Extract data\n",
    "steps = []\n",
    "train_losses = []\n",
    "epochs = []\n",
    "eval_losses = []\n",
    "eval_metrics = {}\n",
    "for log in log_history:\n",
    "    if 'loss' in log:\n",
    "        steps.append(log['step'])\n",
    "        train_losses.append(log['loss'])\n",
    "    if 'eval_loss' in log:\n",
    "        epochs.append(log['epoch'])\n",
    "        eval_losses.append(log['eval_loss'])\n",
    "        for k in log:\n",
    "            if k.startswith('eval_') and k != 'eval_loss' and k != 'eval_runtime' and k != 'eval_samples_per_second' and k != 'eval_steps_per_second':\n",
    "                if k not in eval_metrics:\n",
    "                    eval_metrics[k] = []\n",
    "                eval_metrics[k].append(log[k])\n",
    "\n",
    "if train_losses:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(steps, train_losses, label='Training Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Steps')\n",
    "    plt.legend()\n",
    "    plt.savefig('training_loss.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "if eval_losses:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(epochs, eval_losses, label='Evaluation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Evaluation Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.savefig('eval_loss.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "for metric_name, values in eval_metrics.items():\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(epochs, values, label=metric_name)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric_name.split('eval_')[1])\n",
    "    plt.title(f'{metric_name} over Epochs')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{metric_name}.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATING\")\n",
    "print(\"=\" * 80)\n",
    "clear_gpu_memory()\n",
    "try:\n",
    "    test_results = trainer.evaluate(\n",
    "        tokenized_datasets.get(\"test\", tokenized_datasets[\"validation\"])\n",
    "    )\n",
    "    print(\"\\n Test Results:\")\n",
    "    for k, v in test_results.items():\n",
    "        print(f\" {k}: {v}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n Evaluation error: {e}\")\n",
    "    test_results = {}\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE PEFT ADAPTER & TOKENIZER (deterministic)\n",
    "# ============================================================================\n",
    "print(\"\\n Saving PEFT adapter and tokenizer...\")\n",
    "peft_save_dir = f\"./peft_prefix_flan_t5_small_{dataset_size}_{'mac' if IS_MAC_MACHINE else 'gpu'}\"\n",
    "os.makedirs(peft_save_dir, exist_ok=True)\n",
    "try:\n",
    "    # `model` is the PEFT-wrapped model\n",
    "    model.save_pretrained(peft_save_dir)\n",
    "    tokenizer.save_pretrained(peft_save_dir)\n",
    "    print(f\" Saved PEFT adapter + tokenizer to: {peft_save_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\" Warning when saving: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: example of loading the PEFT adapter back for inference\n",
    "# ============================================================================\n",
    "print(\"\\n Demonstrating reload of base model + PEFT adapter for inference...\")\n",
    "try:\n",
    "    base_for_load = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"cpu\",\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    loaded_peft = PeftModel.from_pretrained(base_for_load, peft_save_dir)\n",
    "    loaded_peft = loaded_peft.to(DEVICE)\n",
    "    loaded_tokenizer = AutoTokenizer.from_pretrained(peft_save_dir)\n",
    "    print(\" Reloaded PEFT model and tokenizer successfully.\")\n",
    "except Exception as e:\n",
    "    print(\" Warning: could not reload PEFT adapter automatically:\", e)\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE METRICS & SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n Saving metrics & summary...\")\n",
    "metrics_file = f\"{task}_prefix_metrics_{dataset_size}_{'mac' if IS_MAC_MACHINE else 'gpu'}.json\"\n",
    "metrics_dict = {\n",
    "    \"model\": model_name,\n",
    "    \"task\": task,\n",
    "    \"platform\": \"Mac (CPU)\" if IS_MAC_MACHINE else \"GPU (CUDA)\" if IS_CUDA else \"CPU\",\n",
    "    \"dataset_size\": dataset_size,\n",
    "    \"test_results\": test_results,\n",
    "    \"training_config\": {\n",
    "        \"epochs\": training_args.num_train_epochs,\n",
    "        \"batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"gradient_accumulation\": training_args.gradient_accumulation_steps,\n",
    "        \"fp16\": fp16_enabled,\n",
    "        \"optimizer\": training_args.optim,\n",
    "    },\n",
    "}\n",
    "with open(metrics_file, \"w\") as f:\n",
    "    json.dump(metrics_dict, f, indent=4)\n",
    "print(f\" Metrics saved to {metrics_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\" Platform: {'Mac (CPU)' if IS_MAC_MACHINE else 'GPU (CUDA)' if IS_CUDA else 'CPU'}\")\n",
    "print(f\" Task: {task}\")\n",
    "print(f\" Model: {model_name}\")\n",
    "print(f\" Dataset Size: {dataset_size}\")\n",
    "print(f\" Training Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\" Gradient Accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\" Test Results: {test_results}\")\n",
    "print(f\" PEFT saved to: {peft_save_dir}\")\n",
    "print_gpu_status()\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n Script complete.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
