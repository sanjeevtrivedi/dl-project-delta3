{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc71207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ENVIRONMENT DETECTION\n",
      "================================================================================\n",
      " PyTorch Version: 2.9.0\n",
      " MPS Available: True\n",
      " CUDA Available: False\n",
      " Kaggle Environment: False\n",
      " Using Device: Apple Metal Performance Shaders (MPS)\n",
      "\n",
      "================================================================================\n",
      "CONFIGURATION\n",
      "================================================================================\n",
      " Model: google/flan-t5-small\n",
      " Task: classification\n",
      " Dataset Size: 100 (auto-reduced on Mac)\n",
      "\n",
      " Loading tokenizer...\n",
      " Tokenizer loaded\n",
      "\n",
      " Loading classification dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'financial_phrasebank' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'financial_phrasebank' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'knkarthick/dialogsum' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'knkarthick/dialogsum' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Could not load financial_phrasebank: Dataset scripts are no longer supported, but found financial_phrasebank.py\n",
      "  Falling back to summarization...\n",
      " Dataset loaded. Splits: ['train', 'validation', 'test']\n",
      "\n",
      " Subsampling dataset (size=100)...\n",
      "  train: 100 samples\n",
      "  validation: 50 samples\n",
      "  test: 50 samples\n",
      "\n",
      " Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 50/50 [00:00<00:00, 1615.93 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Datasets tokenized\n",
      "\n",
      " Loading model google/flan-t5-small...\n",
      "  Loading on CPU first, then moving to MPS...\n",
      "  Model moved to MPS\n",
      " Model loaded\n",
      "\n",
      " Configuring LoRA...\n",
      "trainable params: 172,032 || all params: 77,133,184 || trainable%: 0.2230\n",
      " LoRA configured\n",
      "\n",
      " Configuring training arguments...\n",
      " Training arguments configured\n",
      "\n",
      " Initializing Trainer...\n",
      " Trainer initialized\n",
      "\n",
      "================================================================================\n",
      "STARTING TRAINING\n",
      "================================================================================\n",
      " GPU Device: Apple MPS (auto-managed memory)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:57, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.415900</td>\n",
       "      <td>2.398353</td>\n",
       "      <td>0.136300</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.088200</td>\n",
       "      <td>2.022127</td>\n",
       "      <td>0.155600</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.139300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training completed successfully!\n",
      "\n",
      "================================================================================\n",
      "EVALUATING\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Results:\n",
      "  eval_loss: 2.1264264583587646\n",
      "  eval_rouge1: 0.1728\n",
      "  eval_rouge2: 0.0254\n",
      "  eval_rougeL: 0.1488\n",
      "  eval_runtime: 48.0531\n",
      "  eval_samples_per_second: 1.041\n",
      "  eval_steps_per_second: 1.041\n",
      "  epoch: 2.0\n",
      "\n",
      " Saving model and metrics...\n",
      " Model saved to ./summarization_flan_t5_small_lora_100_mac\n",
      " Metrics saved to summarization_metrics_100_mac.json\n",
      "\n",
      "================================================================================\n",
      "TRAINING SUMMARY\n",
      "================================================================================\n",
      " Platform: Mac (MPS)\n",
      " Task: summarization\n",
      " Model: google/flan-t5-small\n",
      " Dataset Size: 100\n",
      " Training Epochs: 2\n",
      " Gradient Accumulation: 2\n",
      " LoRA Rank: 4\n",
      " Test Results: {'eval_loss': 2.1264264583587646, 'eval_rouge1': 0.1728, 'eval_rouge2': 0.0254, 'eval_rougeL': 0.1488, 'eval_runtime': 48.0531, 'eval_samples_per_second': 1.041, 'eval_steps_per_second': 1.041, 'epoch': 2.0}\n",
      " Model Directory: ./summarization_flan_t5_small_lora_100_mac\n",
      " Metrics File: summarization_metrics_100_mac.json\n",
      " GPU Device: Apple MPS (auto-managed memory)\n",
      "================================================================================\n",
      "\n",
      " Training Complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============================================================================\n",
    "# UNIVERSAL ENVIRONMENT DETECTION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ENVIRONMENT DETECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "IS_MAC = torch.backends.mps.is_available()\n",
    "IS_CUDA = torch.cuda.is_available()\n",
    "IS_KAGGLE = os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\", None) is not None\n",
    "\n",
    "if IS_MAC:\n",
    "    DEVICE = \"mps\"\n",
    "    DEVICE_NAME = \"Apple Metal Performance Shaders (MPS)\"\n",
    "elif IS_CUDA:\n",
    "    DEVICE = \"cuda\"\n",
    "    DEVICE_NAME = f\"CUDA - {torch.cuda.get_device_name(0)}\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    DEVICE_NAME = \"CPU Only\"\n",
    "\n",
    "print(f\" PyTorch Version: {torch.__version__}\")\n",
    "print(f\" MPS Available: {IS_MAC}\")\n",
    "print(f\" CUDA Available: {IS_CUDA}\")\n",
    "print(f\" Kaggle Environment: {IS_KAGGLE}\")\n",
    "print(f\" Using Device: {DEVICE_NAME}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ENVIRONMENT CONFIGURATION\n",
    "# ============================================================================\n",
    "if IS_CUDA and not IS_MAC:\n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "KAGGLE_REQUIREMENTS_PATH = \"/kaggle/input/dependencies/requirements-kaggle-v1.0.txt\"\n",
    "if IS_KAGGLE and os.path.exists(KAGGLE_REQUIREMENTS_PATH):\n",
    "    print(\"\\n Installing Kaggle dependencies...\")\n",
    "    os.system(f\"pip install -q -r {KAGGLE_REQUIREMENTS_PATH}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UNIVERSAL MEMORY MANAGEMENT\n",
    "# ============================================================================\n",
    "def clear_gpu_memory():\n",
    "    if IS_MAC and torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    elif IS_CUDA and torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    if IS_MAC:\n",
    "        return 0.0\n",
    "    if IS_CUDA:\n",
    "        return float(torch.cuda.memory_allocated() / 1024**3)\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def print_gpu_status():\n",
    "    if IS_MAC:\n",
    "        print(\" GPU Device: Apple MPS (auto-managed memory)\")\n",
    "    elif IS_CUDA:\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        used = get_gpu_memory_usage()\n",
    "        print(f\" GPU Memory Used: {used:.2f} GB / {total:.2f} GB\")\n",
    "        print(f\" GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\" Using CPU - no GPU available\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "model_name = \"google/flan-t5-small\"\n",
    "task = \"classification\"  # or \"classification / summarization\"\n",
    "dataset_size = 100 if IS_MAC else 500\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\" Model: {model_name}\")\n",
    "print(f\" Task: {task}\")\n",
    "print(f\" Dataset Size: {dataset_size} (auto-reduced on Mac)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD TOKENIZER\n",
    "# ============================================================================\n",
    "print(\"\\n Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "print(\" Tokenizer loaded\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PREPROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "def preprocess_summarization(examples):\n",
    "    inputs = [f\"summarize: {dialogue}\" for dialogue in examples[\"dialogue\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=512, truncation=True, padding=False\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"summary\"], max_length=128, truncation=True, padding=False\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_classification(examples):\n",
    "    label_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "    inputs = [\n",
    "        f\"classify sentiment: {sentence}\" for sentence in examples[\"sentence\"]\n",
    "    ]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=512, truncation=True, padding=False\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        [label_map[label] for label in examples[\"label\"]],\n",
    "        max_length=8,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD AND PREPARE DATASET\n",
    "# ============================================================================\n",
    "print(f\"\\n Loading {task} dataset...\")\n",
    "clear_gpu_memory()\n",
    "\n",
    "import evaluate\n",
    "\n",
    "try:\n",
    "    if task == \"summarization\":\n",
    "        dataset = load_dataset(\"knkarthick/dialogsum\", trust_remote_code=True)\n",
    "        preprocess_function = preprocess_summarization\n",
    "        metric = evaluate.load(\"rouge\")\n",
    "        splits = [\"train\", \"validation\", \"test\"]\n",
    "    elif task == \"classification\":\n",
    "        try:\n",
    "            dataset = load_dataset(\n",
    "                \"financial_phrasebank\",\n",
    "                \"sentences_allagree\",\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "            dataset = dataset[\"train\"].train_test_split(\n",
    "                test_size=0.2, seed=42\n",
    "            )\n",
    "            temp_split = dataset[\"test\"].train_test_split(\n",
    "                test_size=0.5, seed=42\n",
    "            )\n",
    "            dataset[\"validation\"] = temp_split[\"test\"]\n",
    "            dataset[\"test\"] = temp_split[\"train\"]\n",
    "            preprocess_function = preprocess_classification\n",
    "            metric = evaluate.load(\"accuracy\")\n",
    "            splits = [\"train\", \"validation\", \"test\"]\n",
    "        except Exception as e:\n",
    "            print(f\"  Could not load financial_phrasebank: {e}\")\n",
    "            print(\"  Falling back to summarization...\")\n",
    "            task = \"summarization\"\n",
    "            dataset = load_dataset(\n",
    "                \"knkarthick/dialogsum\", trust_remote_code=True\n",
    "            )\n",
    "            preprocess_function = preprocess_summarization\n",
    "            metric = evaluate.load(\"rouge\")\n",
    "            splits = [\"train\", \"validation\", \"test\"]\n",
    "    else:\n",
    "        raise ValueError(\"Task must be 'summarization' or 'classification'\")\n",
    "\n",
    "    print(f\" Dataset loaded. Splits: {list(dataset.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading dataset: {e}\")\n",
    "    print(\" Please check your internet connection\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SUBSAMPLE DATASET\n",
    "# ============================================================================\n",
    "print(f\"\\n Subsampling dataset (size={dataset_size})...\")\n",
    "if dataset_size != \"full\":\n",
    "    for split in splits:\n",
    "        if split in dataset:\n",
    "            available = len(dataset[split])\n",
    "            num_samples = min(\n",
    "                dataset_size if split == \"train\" else max(50, dataset_size // 2),\n",
    "                available,\n",
    "            )\n",
    "            dataset[split] = dataset[split].shuffle(seed=42).select(\n",
    "                range(num_samples)\n",
    "            )\n",
    "            print(f\"  {split}: {num_samples} samples\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TOKENIZE DATASETS\n",
    "# ============================================================================\n",
    "print(\"\\n Tokenizing datasets...\")\n",
    "clear_gpu_memory()\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=32,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing\",\n",
    ")\n",
    "print(\" Datasets tokenized\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODEL\n",
    "# ============================================================================\n",
    "print(f\"\\n Loading model {model_name}...\")\n",
    "clear_gpu_memory()\n",
    "\n",
    "try:\n",
    "    if IS_MAC:\n",
    "        print(\"  Loading on CPU first, then moving to MPS...\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"cpu\",\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "        base_model = base_model.to(\"mps\")\n",
    "        print(\"  Model moved to MPS\")\n",
    "    else:\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\" if IS_CUDA else \"cpu\",\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "    print(\" Model loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading model: {e}\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURE LORA\n",
    "# ============================================================================\n",
    "print(\"\\n Configuring LoRA...\")\n",
    "lora_r = 4 if IS_MAC else 8\n",
    "lora_alpha = 8 if IS_MAC else 16\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\" LoRA configured\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATA COLLATOR\n",
    "# ============================================================================\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer, model=model, padding=True\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# METRICS COMPUTATION\n",
    "# ============================================================================\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    if isinstance(predictions, torch.Tensor):\n",
    "        predictions = predictions.cpu().numpy()\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    # Replace -100 with pad_token_id (ignore_index)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    try:\n",
    "        decoded_preds = tokenizer.batch_decode(\n",
    "            predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "    except OverflowError:\n",
    "        # Clip predictions to valid token IDs range if overflow error occurs\n",
    "        max_id = tokenizer.vocab_size - 1\n",
    "        predictions = np.clip(predictions, 0, max_id)\n",
    "        decoded_preds = tokenizer.batch_decode(\n",
    "            predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "\n",
    "    decoded_labels = tokenizer.batch_decode(\n",
    "        labels, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n",
    "    if task == \"summarization\":\n",
    "        try:\n",
    "            result = metric.compute(\n",
    "                predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "            )\n",
    "            return {\n",
    "                \"rouge1\": round(result[\"rouge1\"], 4),\n",
    "                \"rouge2\": round(result[\"rouge2\"], 4),\n",
    "                \"rougeL\": round(result[\"rougeL\"], 4),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"  Error computing ROUGE: {e}\")\n",
    "            return {\"rouge1\": 0.0}\n",
    "    else:\n",
    "        preds = [p.strip() for p in decoded_preds]\n",
    "        refs = [r.strip() for r in decoded_labels]\n",
    "        label_map_rev = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "        pred_ids = [label_map_rev.get(p, -1) for p in preds]\n",
    "        ref_ids = [label_map_rev.get(r, -1) for r in refs]\n",
    "        pairs = [(p, r) for p, r in zip(pred_ids, ref_ids) if p != -1 and r != -1]\n",
    "        if not pairs:\n",
    "            return {\"accuracy\": 0.0}\n",
    "        pred_ids, ref_ids = zip(*pairs)\n",
    "        result = metric.compute(predictions=pred_ids, references=ref_ids)\n",
    "        return result\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING ARGUMENTS (SAFE FOR PEFT/LORA)\n",
    "# ============================================================================\n",
    "print(\"\\n Configuring training arguments...\")\n",
    "\n",
    "num_epochs = 2 if IS_MAC else 3\n",
    "grad_accum = 2 if IS_MAC else 4\n",
    "fp16_enabled = IS_CUDA and not IS_MAC\n",
    "use_8bit_optimizer = False  # keep adamw_torch for stability\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"./{task}_flan_t5_small_lora_{dataset_size}_{'mac' if IS_MAC else 'gpu'}\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=grad_accum,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.0,  # safer for LoRA-only training\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=fp16_enabled,\n",
    "    bf16=False,\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=False,  # avoid known PEFT issues\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=1e-4,\n",
    "    seed=42,\n",
    "    dataloader_pin_memory=not IS_MAC,\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=1,\n",
    ")\n",
    "print(\" Training arguments configured\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE TRAINER\n",
    "# ============================================================================\n",
    "print(\"\\n Initializing Trainer...\")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets.get(\n",
    "        \"validation\", tokenized_datasets[\"train\"]\n",
    "    ),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\" Trainer initialized\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "print_gpu_status()\n",
    "print()\n",
    "\n",
    "clear_gpu_memory()\n",
    "\n",
    "try:\n",
    "    model.train()  # ensure LoRA params require grad\n",
    "    trainer.train()\n",
    "    print(\"\\n Training completed successfully!\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n Training interrupted by user\")\n",
    "except RuntimeError as e:\n",
    "    if \"does not require grad\" in str(e):\n",
    "        print(\n",
    "            \"\\n RuntimeError: some tensors do not require grad.\\n\"\n",
    "            \" Check that LoRA parameters are trainable and gradient_checkpointing is False.\"\n",
    "        )\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"\\n Out of memory error.\")\n",
    "        print(\" Try reducing dataset_size or lowering lora_r.\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "clear_gpu_memory()\n",
    "\n",
    "try:\n",
    "    test_results = trainer.evaluate(\n",
    "        tokenized_datasets.get(\"test\", tokenized_datasets[\"validation\"])\n",
    "    )\n",
    "    print(\"\\n Test Results:\")\n",
    "    for k, v in test_results.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n Evaluation error: {e}\")\n",
    "    test_results = {}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "print(\"\\n Saving model and metrics...\")\n",
    "\n",
    "model_dir = f\"./{task}_flan_t5_small_lora_{dataset_size}_{'mac' if IS_MAC else 'gpu'}\"\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "metrics_file = (\n",
    "    f\"{task}_metrics_{dataset_size}_{'mac' if IS_MAC else 'gpu'}.json\"\n",
    ")\n",
    "metrics_dict = {\n",
    "    \"model\": model_name,\n",
    "    \"task\": task,\n",
    "    \"platform\": \"Mac (MPS)\"\n",
    "    if IS_MAC\n",
    "    else \"GPU (CUDA)\"\n",
    "    if IS_CUDA\n",
    "    else \"CPU\",\n",
    "    \"dataset_size\": dataset_size,\n",
    "    \"test_results\": test_results,\n",
    "    \"training_config\": {\n",
    "        \"epochs\": training_args.num_train_epochs,\n",
    "        \"batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"gradient_accumulation\": training_args.gradient_accumulation_steps,\n",
    "        \"lora_r\": lora_r,\n",
    "        \"lora_alpha\": lora_alpha,\n",
    "        \"fp16\": fp16_enabled,\n",
    "        \"optimizer\": training_args.optim,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(metrics_file, \"w\") as f:\n",
    "    json.dump(metrics_dict, f, indent=4)\n",
    "\n",
    "print(f\" Model saved to {model_dir}\")\n",
    "print(f\" Metrics saved to {metrics_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\" Platform: {'Mac (MPS)' if IS_MAC else 'GPU (CUDA)' if IS_CUDA else 'CPU'}\")\n",
    "print(f\" Task: {task}\")\n",
    "print(f\" Model: {model_name}\")\n",
    "print(f\" Dataset Size: {dataset_size}\")\n",
    "print(f\" Training Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\" Gradient Accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\" LoRA Rank: {lora_r}\")\n",
    "print(f\" Test Results: {test_results}\")\n",
    "print(f\" Model Directory: {model_dir}\")\n",
    "print(f\" Metrics File: {metrics_file}\")\n",
    "print_gpu_status()\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n Training Complete!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba74c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Working on Kaggle - Classification\n",
    "# ============================================================================\n",
    "# INSTALL DEPENDENCIES (MUST BE AT TOP)\n",
    "# ============================================================================\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Install required packages if running in Kaggle\n",
    "if os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\") is not None:\n",
    "    # Install evaluate and downgrade protobuf to fix compatibility issues\n",
    "    os.system(\"pip install -q evaluate protobuf==3.20.3\")\n",
    "\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UNIVERSAL ENVIRONMENT DETECTION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ENVIRONMENT DETECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "IS_MAC = torch.backends.mps.is_available()\n",
    "IS_CUDA = torch.cuda.is_available()\n",
    "IS_KAGGLE = os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\", None) is not None\n",
    "\n",
    "if IS_MAC:\n",
    "    DEVICE = \"mps\"\n",
    "    DEVICE_NAME = \"Apple Metal Performance Shaders (MPS)\"\n",
    "elif IS_CUDA:\n",
    "    DEVICE = \"cuda\"\n",
    "    DEVICE_NAME = f\"CUDA - {torch.cuda.get_device_name(0)}\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    DEVICE_NAME = \"CPU Only\"\n",
    "\n",
    "print(f\" PyTorch Version: {torch.__version__}\")\n",
    "print(f\" MPS Available: {IS_MAC}\")\n",
    "print(f\" CUDA Available: {IS_CUDA}\")\n",
    "print(f\" Kaggle Environment: {IS_KAGGLE}\")\n",
    "print(f\" Using Device: {DEVICE_NAME}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ENVIRONMENT CONFIGURATION\n",
    "# ============================================================================\n",
    "if IS_CUDA and not IS_MAC:\n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UNIVERSAL MEMORY MANAGEMENT\n",
    "# ============================================================================\n",
    "def clear_gpu_memory():\n",
    "    if IS_MAC and torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    elif IS_CUDA and torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    if IS_MAC:\n",
    "        return 0.0\n",
    "    if IS_CUDA:\n",
    "        return float(torch.cuda.memory_allocated() / 1024**3)\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def print_gpu_status():\n",
    "    if IS_MAC:\n",
    "        print(\" GPU Device: Apple MPS (auto-managed memory)\")\n",
    "    elif IS_CUDA:\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        used = get_gpu_memory_usage()\n",
    "        print(f\" GPU Memory Used: {used:.2f} GB / {total:.2f} GB\")\n",
    "        print(f\" GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\" Using CPU - no GPU available\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "model_name = \"google/flan-t5-small\"\n",
    "task = \"classification\"  # or \"summarization\"\n",
    "dataset_size = 100 if IS_MAC else 500\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\" Model: {model_name}\")\n",
    "print(f\" Task: {task}\")\n",
    "print(f\" Dataset Size: {dataset_size} (auto-reduced on Mac)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD TOKENIZER\n",
    "# ============================================================================\n",
    "print(\"\\n Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\" Tokenizer loaded\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PREPROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "def preprocess_summarization(examples):\n",
    "    inputs = [f\"summarize: {text}\" for text in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=512, truncation=True, padding=False\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"summary\"], max_length=128, truncation=True, padding=False\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def preprocess_classification(examples):\n",
    "    label_map = {0: \"negative\", 1: \"positive\"}\n",
    "    inputs = [\n",
    "        f\"classify sentiment: {text}\" for text in examples[\"text\"]\n",
    "    ]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=512, truncation=True, padding=False\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        [label_map[label] for label in examples[\"label\"]],\n",
    "        max_length=8,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD AND PREPARE DATASET\n",
    "# ============================================================================\n",
    "print(f\"\\n Loading {task} dataset...\")\n",
    "clear_gpu_memory()\n",
    "\n",
    "import evaluate\n",
    "\n",
    "try:\n",
    "    if task == \"summarization\":\n",
    "        # Use xsum for summarization\n",
    "        dataset = load_dataset(\"EdinburghNLP/xsum\")\n",
    "        dataset = dataset.rename_column(\"document\", \"text\")\n",
    "        preprocess_function = preprocess_summarization\n",
    "        metric = evaluate.load(\"rouge\")\n",
    "        splits = [\"train\", \"validation\", \"test\"]\n",
    "    elif task == \"classification\":\n",
    "        # Use imdb for sentiment classification\n",
    "        dataset = load_dataset(\"imdb\")\n",
    "        # IMDB already has 'text' and 'label' columns, no need to rename\n",
    "        # Remove 'unsupervised' split (has label=-1, unlabeled data)\n",
    "        if \"unsupervised\" in dataset:\n",
    "            del dataset[\"unsupervised\"]\n",
    "        # Split train into train/validation since IMDB only has train/test\n",
    "        train_val = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "        dataset[\"train\"] = train_val[\"train\"]\n",
    "        dataset[\"validation\"] = train_val[\"test\"]\n",
    "        preprocess_function = preprocess_classification\n",
    "        metric = evaluate.load(\"accuracy\")\n",
    "        splits = [\"train\", \"validation\", \"test\"]\n",
    "    else:\n",
    "        raise ValueError(\"Task must be 'summarization' or 'classification'\")\n",
    "\n",
    "    print(f\" Dataset loaded. Splits: {list(dataset.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\" Error loading dataset: {e}\")\n",
    "    print(\" Please check your internet connection\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SUBSAMPLE DATASET\n",
    "# ============================================================================\n",
    "print(f\"\\n Subsampling dataset (size={dataset_size})...\")\n",
    "if dataset_size != \"full\":\n",
    "    for split in splits:\n",
    "        if split in dataset:\n",
    "            available = len(dataset[split])\n",
    "            num_samples = min(\n",
    "                dataset_size if split == \"train\" else max(50, dataset_size // 2),\n",
    "                available,\n",
    "            )\n",
    "            dataset[split] = dataset[split].shuffle(seed=42).select(\n",
    "                range(num_samples)\n",
    "            )\n",
    "            print(f\"  {split}: {num_samples} samples\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TOKENIZE DATASETS\n",
    "# ============================================================================\n",
    "print(\"\\n Tokenizing datasets...\")\n",
    "clear_gpu_memory()\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=32,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing\",\n",
    ")\n",
    "print(\" Datasets tokenized\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODEL\n",
    "# ============================================================================\n",
    "print(f\"\\n Loading model {model_name}...\")\n",
    "clear_gpu_memory()\n",
    "\n",
    "try:\n",
    "    if IS_MAC:\n",
    "        print(\"  Loading on CPU first, then moving to MPS...\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"cpu\",\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "        base_model = base_model.to(\"mps\")\n",
    "        print(\"  Model moved to MPS\")\n",
    "    else:\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\" if IS_CUDA else \"cpu\",\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "    print(\" Model loaded\")\n",
    "except Exception as e:\n",
    "    print(f\" Error loading model: {e}\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURE LORA\n",
    "# ============================================================================\n",
    "print(\"\\n Configuring LoRA...\")\n",
    "lora_r = 4 if IS_MAC else 8\n",
    "lora_alpha = 8 if IS_MAC else 16\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\" LoRA configured\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATA COLLATOR\n",
    "# ============================================================================\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer, model=model, padding=True\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# METRICS COMPUTATION\n",
    "# ============================================================================\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    if isinstance(predictions, torch.Tensor):\n",
    "        predictions = predictions.cpu().numpy()\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    # Replace -100 with pad_token_id (ignore_index)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    try:\n",
    "        decoded_preds = tokenizer.batch_decode(\n",
    "            predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "    except OverflowError:\n",
    "        # Clip predictions to valid token IDs range if overflow error occurs\n",
    "        max_id = tokenizer.vocab_size - 1\n",
    "        predictions = np.clip(predictions, 0, max_id)\n",
    "        decoded_preds = tokenizer.batch_decode(\n",
    "            predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "\n",
    "    decoded_labels = tokenizer.batch_decode(\n",
    "        labels, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n",
    "    if task == \"summarization\":\n",
    "        try:\n",
    "            result = metric.compute(\n",
    "                predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "            )\n",
    "            return {\n",
    "                \"rouge1\": round(result[\"rouge1\"], 4),\n",
    "                \"rouge2\": round(result[\"rouge2\"], 4),\n",
    "                \"rougeL\": round(result[\"rougeL\"], 4),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"  Error computing ROUGE: {e}\")\n",
    "            return {\"rouge1\": 0.0}\n",
    "    else:\n",
    "        preds = [p.strip() for p in decoded_preds]\n",
    "        refs = [r.strip() for r in decoded_labels]\n",
    "        # Update label map for IMDB\n",
    "        label_map_rev = {\"negative\": 0, \"positive\": 1}\n",
    "        pred_ids = [label_map_rev.get(p, -1) for p in preds]\n",
    "        ref_ids = [label_map_rev.get(r, -1) for r in refs]\n",
    "        pairs = [(p, r) for p, r in zip(pred_ids, ref_ids) if p != -1 and r != -1]\n",
    "        if not pairs:\n",
    "            return {\"accuracy\": 0.0}\n",
    "        pred_ids, ref_ids = zip(*pairs)\n",
    "        result = metric.compute(predictions=pred_ids, references=ref_ids)\n",
    "        return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING ARGUMENTS (SAFE FOR PEFT/LORA)\n",
    "# ============================================================================\n",
    "print(\"\\n Configuring training arguments...\")\n",
    "\n",
    "num_epochs = 2 if IS_MAC else 3\n",
    "grad_accum = 2 if IS_MAC else 4\n",
    "fp16_enabled = IS_CUDA and not IS_MAC\n",
    "use_8bit_optimizer = False  # keep adamw_torch for stability\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"./{task}_flan_t5_small_lora_{dataset_size}_{'mac' if IS_MAC else 'gpu'}\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=grad_accum,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.0,  # safer for LoRA-only training\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=fp16_enabled,\n",
    "    bf16=False,\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=False,  # avoid known PEFT issues\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=1e-4,\n",
    "    seed=42,\n",
    "    dataloader_pin_memory=not IS_MAC,\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=1,\n",
    ")\n",
    "print(\" Training arguments configured\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE TRAINER\n",
    "# ============================================================================\n",
    "print(\"\\n Initializing Trainer...\")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets.get(\n",
    "        \"validation\", tokenized_datasets[\"train\"]\n",
    "    ),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\" Trainer initialized\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "print_gpu_status()\n",
    "print()\n",
    "\n",
    "clear_gpu_memory()\n",
    "\n",
    "try:\n",
    "    model.train()  # ensure LoRA params require grad\n",
    "    trainer.train()\n",
    "    print(\"\\n Training completed successfully!\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n Training interrupted by user\")\n",
    "except RuntimeError as e:\n",
    "    if \"does not require grad\" in str(e):\n",
    "        print(\n",
    "            \"\\n RuntimeError: some tensors do not require grad.\\n\"\n",
    "            \" Check that LoRA parameters are trainable and gradient_checkpointing is False.\"\n",
    "        )\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"\\n Out of memory error.\")\n",
    "        print(\" Try reducing dataset_size or lowering lora_r.\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "clear_gpu_memory()\n",
    "\n",
    "try:\n",
    "    test_results = trainer.evaluate(\n",
    "        tokenized_datasets.get(\"test\", tokenized_datasets[\"validation\"])\n",
    "    )\n",
    "    print(\"\\n Test Results:\")\n",
    "    for k, v in test_results.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n Evaluation error: {e}\")\n",
    "    test_results = {}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "print(\"\\n Saving model and metrics...\")\n",
    "\n",
    "model_dir = f\"./{task}_flan_t5_small_lora_{dataset_size}_{'mac' if IS_MAC else 'gpu'}\"\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "metrics_file = (\n",
    "    f\"{task}_metrics_{dataset_size}_{'mac' if IS_MAC else 'gpu'}.json\"\n",
    ")\n",
    "metrics_dict = {\n",
    "    \"model\": model_name,\n",
    "    \"task\": task,\n",
    "    \"platform\": \"Mac (MPS)\"\n",
    "    if IS_MAC\n",
    "    else \"GPU (CUDA)\"\n",
    "    if IS_CUDA\n",
    "    else \"CPU\",\n",
    "    \"dataset_size\": dataset_size,\n",
    "    \"test_results\": test_results,\n",
    "    \"training_config\": {\n",
    "        \"epochs\": training_args.num_train_epochs,\n",
    "        \"batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"gradient_accumulation\": training_args.gradient_accumulation_steps,\n",
    "        \"lora_r\": lora_r,\n",
    "        \"lora_alpha\": lora_alpha,\n",
    "        \"fp16\": fp16_enabled,\n",
    "        \"optimizer\": training_args.optim,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(metrics_file, \"w\") as f:\n",
    "    json.dump(metrics_dict, f, indent=4)\n",
    "\n",
    "print(f\" Model saved to {model_dir}\")\n",
    "print(f\" Metrics saved to {metrics_file}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\" Platform: {'Mac (MPS)' if IS_MAC else 'GPU (CUDA)' if IS_CUDA else 'CPU'}\")\n",
    "print(f\" Task: {task}\")\n",
    "print(f\" Model: {model_name}\")\n",
    "print(f\" Dataset Size: {dataset_size}\")\n",
    "print(f\" Training Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\" Gradient Accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\" LoRA Rank: {lora_r}\")\n",
    "print(f\" Test Results: {test_results}\")\n",
    "print(f\" Model Directory: {model_dir}\")\n",
    "print(f\" Metrics File: {metrics_file}\")\n",
    "print_gpu_status()\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n Training Complete!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0a6391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Working on Kaggle - Summarization\n",
    "# ============================================================================\n",
    "# INSTALL DEPENDENCIES\n",
    "# ============================================================================\n",
    "import os\n",
    "import sys\n",
    "# Install required packages if running in Kaggle\n",
    "if os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\") is not None:\n",
    "    # Install evaluate and downgrade protobuf to fix compatibility issues\n",
    "    os.system(\"pip install -q evaluate protobuf==3.20.3\")\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# ============================================================================\n",
    "# UNIVERSAL ENVIRONMENT DETECTION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ENVIRONMENT DETECTION\")\n",
    "print(\"=\" * 80)\n",
    "IS_MAC = torch.backends.mps.is_available()\n",
    "IS_CUDA = torch.cuda.is_available()\n",
    "IS_KAGGLE = os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\", None) is not None\n",
    "if IS_MAC:\n",
    "    DEVICE = \"mps\"\n",
    "    DEVICE_NAME = \"Apple Metal Performance Shaders (MPS)\"\n",
    "elif IS_CUDA:\n",
    "    DEVICE = \"cuda\"\n",
    "    DEVICE_NAME = f\"CUDA - {torch.cuda.get_device_name(0)}\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    DEVICE_NAME = \"CPU Only\"\n",
    "print(f\" PyTorch Version: {torch.__version__}\")\n",
    "print(f\" MPS Available: {IS_MAC}\")\n",
    "print(f\" CUDA Available: {IS_CUDA}\")\n",
    "print(f\" Kaggle Environment: {IS_KAGGLE}\")\n",
    "print(f\" Using Device: {DEVICE_NAME}\")\n",
    "# ============================================================================\n",
    "# ENVIRONMENT CONFIGURATION\n",
    "# ============================================================================\n",
    "if IS_CUDA and not IS_MAC:\n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# ============================================================================\n",
    "# UNIVERSAL MEMORY MANAGEMENT\n",
    "# ============================================================================\n",
    "def clear_gpu_memory():\n",
    "    if IS_MAC and torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    elif IS_CUDA and torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "def get_gpu_memory_usage():\n",
    "    if IS_MAC:\n",
    "        return 0.0\n",
    "    if IS_CUDA:\n",
    "        return float(torch.cuda.memory_allocated() / 1024**3)\n",
    "    return 0.0\n",
    "def print_gpu_status():\n",
    "    if IS_MAC:\n",
    "        print(\" GPU Device: Apple MPS (auto-managed memory)\")\n",
    "    elif IS_CUDA:\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        used = get_gpu_memory_usage()\n",
    "        print(f\" GPU Memory Used: {used:.2f} GB / {total:.2f} GB\")\n",
    "        print(f\" GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\" Using CPU - no GPU available\")\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "model_name = \"google/flan-t5-small\"\n",
    "task = \"summarization\" # or \"classification\"\n",
    "dataset_size = 100 if IS_MAC else 500\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\" Model: {model_name}\")\n",
    "print(f\" Task: {task}\")\n",
    "print(f\" Dataset Size: {dataset_size} (auto-reduced on Mac)\")\n",
    "# ============================================================================\n",
    "# LOAD TOKENIZER\n",
    "# ============================================================================\n",
    "print(\"\\n Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\" Tokenizer loaded\")\n",
    "# ============================================================================\n",
    "# PREPROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "def preprocess_summarization(examples):\n",
    "    inputs = [f\"summarize: {text}\" for text in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=512, truncation=True, padding=False\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"summary\"], max_length=128, truncation=True, padding=False\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "def preprocess_classification(examples):\n",
    "    label_map = {0: \"negative\", 1: \"positive\"}\n",
    "    inputs = [\n",
    "        f\"classify sentiment: {text}\" for text in examples[\"text\"]\n",
    "    ]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=512, truncation=True, padding=False\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        [label_map[label] for label in examples[\"label\"]],\n",
    "        max_length=8,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "# ============================================================================\n",
    "# LOAD AND PREPARE DATASET\n",
    "# ============================================================================\n",
    "print(f\"\\n Loading {task} dataset...\")\n",
    "clear_gpu_memory()\n",
    "import evaluate\n",
    "try:\n",
    "    if task == \"summarization\":\n",
    "        # Use knkarthick/samsum for summarization\n",
    "        dataset = load_dataset(\"knkarthick/samsum\")\n",
    "        # Rename 'val' to 'validation' if necessary\n",
    "        if \"val\" in dataset:\n",
    "            dataset[\"validation\"] = dataset.pop(\"val\")\n",
    "        dataset = dataset.rename_column(\"dialogue\", \"text\")\n",
    "        preprocess_function = preprocess_summarization\n",
    "        metric = evaluate.load(\"rouge\")\n",
    "        splits = [\"train\", \"validation\", \"test\"]\n",
    "    elif task == \"classification\":\n",
    "        # Use imdb for sentiment classification\n",
    "        dataset = load_dataset(\"imdb\")\n",
    "        # IMDB already has 'text' and 'label' columns, no need to rename\n",
    "        # Remove 'unsupervised' split (has label=-1, unlabeled data)\n",
    "        if \"unsupervised\" in dataset:\n",
    "            del dataset[\"unsupervised\"]\n",
    "        # Split train into train/validation since IMDB only has train/test\n",
    "        train_val = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "        dataset[\"train\"] = train_val[\"train\"]\n",
    "        dataset[\"validation\"] = train_val[\"test\"]\n",
    "        preprocess_function = preprocess_classification\n",
    "        metric = evaluate.load(\"accuracy\")\n",
    "        splits = [\"train\", \"validation\", \"test\"]\n",
    "    else:\n",
    "        raise ValueError(\"Task must be 'summarization' or 'classification'\")\n",
    "    print(f\" Dataset loaded. Splits: {list(dataset.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\" Error loading dataset: {e}\")\n",
    "    print(\" Please check your internet connection\")\n",
    "    raise SystemExit(1)\n",
    "# ============================================================================\n",
    "# SUBSAMPLE DATASET\n",
    "# ============================================================================\n",
    "print(f\"\\n Subsampling dataset (size={dataset_size})...\")\n",
    "if dataset_size != \"full\":\n",
    "    for split in splits:\n",
    "        if split in dataset:\n",
    "            available = len(dataset[split])\n",
    "            num_samples = min(\n",
    "                dataset_size if split == \"train\" else max(50, dataset_size // 2),\n",
    "                available,\n",
    "            )\n",
    "            dataset[split] = dataset[split].shuffle(seed=42).select(\n",
    "                range(num_samples)\n",
    "            )\n",
    "            print(f\" {split}: {num_samples} samples\")\n",
    "# ============================================================================\n",
    "# TOKENIZE DATASETS\n",
    "# ============================================================================\n",
    "print(\"\\n Tokenizing datasets...\")\n",
    "clear_gpu_memory()\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=32,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing\",\n",
    ")\n",
    "print(\" Datasets tokenized\")\n",
    "# ============================================================================\n",
    "# LOAD MODEL\n",
    "# ============================================================================\n",
    "print(f\"\\n Loading model {model_name}...\")\n",
    "clear_gpu_memory()\n",
    "try:\n",
    "    if IS_MAC:\n",
    "        print(\" Loading on CPU first, then moving to MPS...\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"cpu\",\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "        base_model = base_model.to(\"mps\")\n",
    "        print(\" Model moved to MPS\")\n",
    "    else:\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\" if IS_CUDA else \"cpu\",\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "    print(\" Model loaded\")\n",
    "except Exception as e:\n",
    "    print(f\" Error loading model: {e}\")\n",
    "    raise SystemExit(1)\n",
    "# ============================================================================\n",
    "# CONFIGURE LORA\n",
    "# ============================================================================\n",
    "print(\"\\n Configuring LoRA...\")\n",
    "lora_r = 4 if IS_MAC else 8\n",
    "lora_alpha = 8 if IS_MAC else 16\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\" LoRA configured\")\n",
    "# ============================================================================\n",
    "# DATA COLLATOR\n",
    "# ============================================================================\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer, model=model, padding=True\n",
    ")\n",
    "# ============================================================================\n",
    "# METRICS COMPUTATION\n",
    "# ============================================================================\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, torch.Tensor):\n",
    "        predictions = predictions.cpu().numpy()\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.cpu().numpy()\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    # Replace -100 with pad_token_id (ignore_index)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    try:\n",
    "        decoded_preds = tokenizer.batch_decode(\n",
    "            predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "    except OverflowError:\n",
    "        # Clip predictions to valid token IDs range if overflow error occurs\n",
    "        max_id = tokenizer.vocab_size - 1\n",
    "        predictions = np.clip(predictions, 0, max_id)\n",
    "        decoded_preds = tokenizer.batch_decode(\n",
    "            predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "    decoded_labels = tokenizer.batch_decode(\n",
    "        labels, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    if task == \"summarization\":\n",
    "        try:\n",
    "            result = metric.compute(\n",
    "                predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "            )\n",
    "            return {\n",
    "                \"rouge1\": round(result[\"rouge1\"], 4),\n",
    "                \"rouge2\": round(result[\"rouge2\"], 4),\n",
    "                \"rougeL\": round(result[\"rougeL\"], 4),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\" Error computing ROUGE: {e}\")\n",
    "            return {\"rouge1\": 0.0}\n",
    "    else:\n",
    "        preds = [p.strip() for p in decoded_preds]\n",
    "        refs = [r.strip() for r in decoded_labels]\n",
    "        # Update label map for IMDB\n",
    "        label_map_rev = {\"negative\": 0, \"positive\": 1}\n",
    "        pred_ids = [label_map_rev.get(p, -1) for p in preds]\n",
    "        ref_ids = [label_map_rev.get(r, -1) for r in refs]\n",
    "        pairs = [(p, r) for p, r in zip(pred_ids, ref_ids) if p != -1 and r != -1]\n",
    "        if not pairs:\n",
    "            return {\"accuracy\": 0.0}\n",
    "        pred_ids, ref_ids = zip(*pairs)\n",
    "        result = metric.compute(predictions=pred_ids, references=ref_ids)\n",
    "        return result\n",
    "# ============================================================================\n",
    "# TRAINING ARGUMENTS (SAFE FOR PEFT/LORA)\n",
    "# ============================================================================\n",
    "print(\"\\n Configuring training arguments...\")\n",
    "num_epochs = 2 if IS_MAC else 3\n",
    "grad_accum = 2 if IS_MAC else 4\n",
    "fp16_enabled = IS_CUDA and not IS_MAC\n",
    "use_8bit_optimizer = False # keep adamw_torch for stability\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"./{task}_flan_t5_small_lora_{dataset_size}_{'mac' if IS_MAC else 'gpu'}\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=grad_accum,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.0, # safer for LoRA-only training\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=fp16_enabled,\n",
    "    bf16=False,\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=False, # avoid known PEFT issues\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=1e-4,\n",
    "    seed=42,\n",
    "    dataloader_pin_memory=not IS_MAC,\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=1,\n",
    ")\n",
    "print(\" Training arguments configured\")\n",
    "# ============================================================================\n",
    "# CREATE TRAINER\n",
    "# ============================================================================\n",
    "print(\"\\n Initializing Trainer...\")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets.get(\n",
    "        \"validation\", tokenized_datasets[\"train\"]\n",
    "    ),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\" Trainer initialized\")\n",
    "# ============================================================================\n",
    "# TRAINING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "print_gpu_status()\n",
    "print()\n",
    "clear_gpu_memory()\n",
    "try:\n",
    "    model.train() # ensure LoRA params require grad\n",
    "    trainer.train()\n",
    "    print(\"\\n Training completed successfully!\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n Training interrupted by user\")\n",
    "except RuntimeError as e:\n",
    "    if \"does not require grad\" in str(e):\n",
    "        print(\n",
    "            \"\\n RuntimeError: some tensors do not require grad.\\n\"\n",
    "            \" Check that LoRA parameters are trainable and gradient_checkpointing is False.\"\n",
    "        )\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"\\n Out of memory error.\")\n",
    "        print(\" Try reducing dataset_size or lowering lora_r.\")\n",
    "    raise\n",
    "# ============================================================================\n",
    "# PLOTTING METRICS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PLOTTING METRICS\")\n",
    "print(\"=\" * 80)\n",
    "log_history = trainer.state.log_history\n",
    "# Extract data\n",
    "steps = []\n",
    "train_losses = []\n",
    "epochs = []\n",
    "eval_losses = []\n",
    "eval_metrics = {}  # for different metrics\n",
    "for log in log_history:\n",
    "    if 'loss' in log:\n",
    "        steps.append(log['step'])\n",
    "        train_losses.append(log['loss'])\n",
    "    if 'eval_loss' in log:\n",
    "        epochs.append(log['epoch'])\n",
    "        eval_losses.append(log['eval_loss'])\n",
    "        # Add other metrics\n",
    "        for k in log:\n",
    "            if k.startswith('eval_') and k != 'eval_loss' and k != 'eval_runtime' and k != 'eval_samples_per_second' and k != 'eval_steps_per_second':\n",
    "                if k not in eval_metrics:\n",
    "                    eval_metrics[k] = []\n",
    "                eval_metrics[k].append(log[k])\n",
    "# Plot training loss\n",
    "if train_losses:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(steps, train_losses, label='Training Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Steps')\n",
    "    plt.legend()\n",
    "    plt.savefig('training_loss.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "# Plot eval loss\n",
    "if eval_losses:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(epochs, eval_losses, label='Evaluation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Evaluation Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.savefig('eval_loss.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "# Plot eval metrics\n",
    "for metric_name, values in eval_metrics.items():\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(epochs, values, label=metric_name)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric_name.split('eval_')[1])\n",
    "    plt.title(f'{metric_name} over Epochs')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{metric_name}.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "# ============================================================================\n",
    "# EVALUATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATING\")\n",
    "print(\"=\" * 80)\n",
    "clear_gpu_memory()\n",
    "try:\n",
    "    test_results = trainer.evaluate(\n",
    "        tokenized_datasets.get(\"test\", tokenized_datasets[\"validation\"])\n",
    "    )\n",
    "    print(\"\\n Test Results:\")\n",
    "    for k, v in test_results.items():\n",
    "        print(f\" {k}: {v}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n Evaluation error: {e}\")\n",
    "    test_results = {}\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "print(\"\\n Saving model and metrics...\")\n",
    "model_dir = f\"./{task}_flan_t5_small_lora_{dataset_size}_{'mac' if IS_MAC else 'gpu'}\"\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "metrics_file = (\n",
    "    f\"{task}_metrics_{dataset_size}_{'mac' if IS_MAC else 'gpu'}.json\"\n",
    ")\n",
    "metrics_dict = {\n",
    "    \"model\": model_name,\n",
    "    \"task\": task,\n",
    "    \"platform\": \"Mac (MPS)\"\n",
    "    if IS_MAC\n",
    "    else \"GPU (CUDA)\"\n",
    "    if IS_CUDA\n",
    "    else \"CPU\",\n",
    "    \"dataset_size\": dataset_size,\n",
    "    \"test_results\": test_results,\n",
    "    \"training_config\": {\n",
    "        \"epochs\": training_args.num_train_epochs,\n",
    "        \"batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"gradient_accumulation\": training_args.gradient_accumulation_steps,\n",
    "        \"lora_r\": lora_r,\n",
    "        \"lora_alpha\": lora_alpha,\n",
    "        \"fp16\": fp16_enabled,\n",
    "        \"optimizer\": training_args.optim,\n",
    "    },\n",
    "}\n",
    "with open(metrics_file, \"w\") as f:\n",
    "    json.dump(metrics_dict, f, indent=4)\n",
    "print(f\" Model saved to {model_dir}\")\n",
    "print(f\" Metrics saved to {metrics_file}\")\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\" Platform: {'Mac (MPS)' if IS_MAC else 'GPU (CUDA)' if IS_CUDA else 'CPU'}\")\n",
    "print(f\" Task: {task}\")\n",
    "print(f\" Model: {model_name}\")\n",
    "print(f\" Dataset Size: {dataset_size}\")\n",
    "print(f\" Training Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\" Gradient Accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\" LoRA Rank: {lora_r}\")\n",
    "print(f\" Test Results: {test_results}\")\n",
    "print(f\" Model Directory: {model_dir}\")\n",
    "print(f\" Metrics File: {metrics_file}\")\n",
    "print_gpu_status()\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n Training Complete!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bcf9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Working on Kaggle - Summarization & Classification\n",
    "# ============================================================================\n",
    "# INSTALL DEPENDENCIES \n",
    "# ============================================================================\n",
    "import os\n",
    "import sys\n",
    "# Install required packages if running in Kaggle\n",
    "if os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\") is not None:\n",
    "    # Install evaluate, transformers, and peft; upgrade to latest to fix deprecation\n",
    "    os.system(\"pip install -q --upgrade evaluate transformers peft protobuf==4.25.3\")\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# ============================================================================\n",
    "# UNIVERSAL ENVIRONMENT DETECTION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ENVIRONMENT DETECTION\")\n",
    "print(\"=\" * 80)\n",
    "IS_MAC = torch.backends.mps.is_available()\n",
    "IS_CUDA = torch.cuda.is_available()\n",
    "IS_KAGGLE = os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\", None) is not None\n",
    "if IS_MAC:\n",
    "    DEVICE = \"mps\"\n",
    "    DEVICE_NAME = \"Apple Metal Performance Shaders (MPS)\"\n",
    "elif IS_CUDA:\n",
    "    DEVICE = \"cuda\"\n",
    "    DEVICE_NAME = f\"CUDA - {torch.cuda.get_device_name(0)}\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    DEVICE_NAME = \"CPU Only\"\n",
    "print(f\" PyTorch Version: {torch.__version__}\")\n",
    "print(f\" MPS Available: {IS_MAC}\")\n",
    "print(f\" CUDA Available: {IS_CUDA}\")\n",
    "print(f\" Kaggle Environment: {IS_KAGGLE}\")\n",
    "print(f\" Using Device: {DEVICE_NAME}\")\n",
    "# ============================================================================\n",
    "# ENVIRONMENT CONFIGURATION\n",
    "# ============================================================================\n",
    "if IS_CUDA and not IS_MAC:\n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# ============================================================================\n",
    "# UNIVERSAL MEMORY MANAGEMENT\n",
    "# ============================================================================\n",
    "def clear_gpu_memory():\n",
    "    if IS_MAC and torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    elif IS_CUDA and torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "def get_gpu_memory_usage():\n",
    "    if IS_MAC:\n",
    "        return 0.0\n",
    "    if IS_CUDA:\n",
    "        return float(torch.cuda.memory_allocated() / 1024**3)\n",
    "    return 0.0\n",
    "def print_gpu_status():\n",
    "    if IS_MAC:\n",
    "        print(\" GPU Device: Apple MPS (auto-managed memory)\")\n",
    "    elif IS_CUDA:\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        used = get_gpu_memory_usage()\n",
    "        print(f\" GPU Memory Used: {used:.2f} GB / {total:.2f} GB\")\n",
    "        print(f\" GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\" Using CPU - no GPU available\")\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "model_name = \"google/flan-t5-small\"\n",
    "task = \"classification\" # or \"classification / summarization\"\n",
    "dataset_size = 100 if IS_MAC else 500\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\" Model: {model_name}\")\n",
    "print(f\" Task: {task}\")\n",
    "print(f\" Dataset Size: {dataset_size} (auto-reduced on Mac)\")\n",
    "# ============================================================================\n",
    "# LOAD TOKENIZER\n",
    "# ============================================================================\n",
    "print(\"\\n Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\" Tokenizer loaded\")\n",
    "# ============================================================================\n",
    "# PREPROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "def preprocess_summarization(examples):\n",
    "    inputs = [f\"summarize: {text}\" for text in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=512, truncation=True, padding=False\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"summary\"], max_length=128, truncation=True, padding=False\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "def preprocess_classification(examples):\n",
    "    label_map = {0: \"negative\", 1: \"positive\"}\n",
    "    inputs = [\n",
    "        f\"classify sentiment: {text}\" for text in examples[\"text\"]\n",
    "    ]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=512, truncation=True, padding=False\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        [label_map[label] for label in examples[\"label\"]],\n",
    "        max_length=8,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "# ============================================================================\n",
    "# LOAD AND PREPARE DATASET\n",
    "# ============================================================================\n",
    "print(f\"\\n Loading {task} dataset...\")\n",
    "clear_gpu_memory()\n",
    "import evaluate\n",
    "try:\n",
    "    if task == \"summarization\":\n",
    "        # Use knkarthick/samsum for summarization\n",
    "        dataset = load_dataset(\"knkarthick/samsum\")\n",
    "        # Rename 'val' to 'validation' if necessary\n",
    "        if \"val\" in dataset:\n",
    "            dataset[\"validation\"] = dataset.pop(\"val\")\n",
    "        dataset = dataset.rename_column(\"dialogue\", \"text\")\n",
    "        preprocess_function = preprocess_summarization\n",
    "        metric = evaluate.load(\"rouge\")\n",
    "        splits = [\"train\", \"validation\", \"test\"]\n",
    "    elif task == \"classification\":\n",
    "        # Use imdb for sentiment classification\n",
    "        dataset = load_dataset(\"imdb\")\n",
    "        # IMDB already has 'text' and 'label' columns, no need to rename\n",
    "        # Remove 'unsupervised' split (has label=-1, unlabeled data)\n",
    "        if \"unsupervised\" in dataset:\n",
    "            del dataset[\"unsupervised\"]\n",
    "        # Split train into train/validation since IMDB only has train/test\n",
    "        train_val = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "        dataset[\"train\"] = train_val[\"train\"]\n",
    "        dataset[\"validation\"] = train_val[\"test\"]\n",
    "        preprocess_function = preprocess_classification\n",
    "        metric = evaluate.load(\"accuracy\")\n",
    "        splits = [\"train\", \"validation\", \"test\"]\n",
    "    else:\n",
    "        raise ValueError(\"Task must be 'summarization' or 'classification'\")\n",
    "    print(f\" Dataset loaded. Splits: {list(dataset.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\" Error loading dataset: {e}\")\n",
    "    print(\" Please check your internet connection\")\n",
    "    raise SystemExit(1)\n",
    "# ============================================================================\n",
    "# SUBSAMPLE DATASET\n",
    "# ============================================================================\n",
    "print(f\"\\n Subsampling dataset (size={dataset_size})...\")\n",
    "if dataset_size != \"full\":\n",
    "    for split in splits:\n",
    "        if split in dataset:\n",
    "            available = len(dataset[split])\n",
    "            num_samples = min(\n",
    "                dataset_size if split == \"train\" else max(50, dataset_size // 2),\n",
    "                available,\n",
    "            )\n",
    "            dataset[split] = dataset[split].shuffle(seed=42).select(\n",
    "                range(num_samples)\n",
    "            )\n",
    "            print(f\" {split}: {num_samples} samples\")\n",
    "# ============================================================================\n",
    "# TOKENIZE DATASETS\n",
    "# ============================================================================\n",
    "print(\"\\n Tokenizing datasets...\")\n",
    "clear_gpu_memory()\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=32,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing\",\n",
    ")\n",
    "print(\" Datasets tokenized\")\n",
    "# ============================================================================\n",
    "# LOAD MODEL\n",
    "# ============================================================================\n",
    "print(f\"\\n Loading model {model_name}...\")\n",
    "clear_gpu_memory()\n",
    "try:\n",
    "    if IS_MAC:\n",
    "        print(\" Loading on CPU first, then moving to MPS...\")\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"cpu\",\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "        base_model = base_model.to(\"mps\")\n",
    "        print(\" Model moved to MPS\")\n",
    "    else:\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\" if IS_CUDA else \"cpu\",\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "    print(\" Model loaded\")\n",
    "except Exception as e:\n",
    "    print(f\" Error loading model: {e}\")\n",
    "    raise SystemExit(1)\n",
    "# ============================================================================\n",
    "# CONFIGURE LORA\n",
    "# ============================================================================\n",
    "print(\"\\n Configuring LoRA...\")\n",
    "lora_r = 4 if IS_MAC else 8\n",
    "lora_alpha = 8 if IS_MAC else 16\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\" LoRA configured\")\n",
    "# ============================================================================\n",
    "# DATA COLLATOR\n",
    "# ============================================================================\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer, model=model, padding=True\n",
    ")\n",
    "# ============================================================================\n",
    "# METRICS COMPUTATION\n",
    "# ============================================================================\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, torch.Tensor):\n",
    "        predictions = predictions.cpu().numpy()\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.cpu().numpy()\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    # Replace -100 with pad_token_id (ignore_index)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    try:\n",
    "        decoded_preds = tokenizer.batch_decode(\n",
    "            predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "    except OverflowError:\n",
    "        # Clip predictions to valid token IDs range if overflow error occurs\n",
    "        max_id = tokenizer.vocab_size - 1\n",
    "        predictions = np.clip(predictions, 0, max_id)\n",
    "        decoded_preds = tokenizer.batch_decode(\n",
    "            predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "    decoded_labels = tokenizer.batch_decode(\n",
    "        labels, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    if task == \"summarization\":\n",
    "        try:\n",
    "            result = metric.compute(\n",
    "                predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "            )\n",
    "            return {\n",
    "                \"rouge1\": round(result[\"rouge1\"], 4),\n",
    "                \"rouge2\": round(result[\"rouge2\"], 4),\n",
    "                \"rougeL\": round(result[\"rougeL\"], 4),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\" Error computing ROUGE: {e}\")\n",
    "            return {\"rouge1\": 0.0}\n",
    "    else:\n",
    "        preds = [p.strip() for p in decoded_preds]\n",
    "        refs = [r.strip() for r in decoded_labels]\n",
    "        # Update label map for IMDB\n",
    "        label_map_rev = {\"negative\": 0, \"positive\": 1}\n",
    "        pred_ids = [label_map_rev.get(p, -1) for p in preds]\n",
    "        ref_ids = [label_map_rev.get(r, -1) for r in refs]\n",
    "        pairs = [(p, r) for p, r in zip(pred_ids, ref_ids) if p != -1 and r != -1]\n",
    "        if not pairs:\n",
    "            return {\"accuracy\": 0.0}\n",
    "        pred_ids, ref_ids = zip(*pairs)\n",
    "        result = metric.compute(predictions=pred_ids, references=ref_ids)\n",
    "        return result\n",
    "# ============================================================================\n",
    "# TRAINING ARGUMENTS (SAFE FOR PEFT/LORA)\n",
    "# ============================================================================\n",
    "print(\"\\n Configuring training arguments...\")\n",
    "num_epochs = 2 if IS_MAC else 3\n",
    "grad_accum = 2 if IS_MAC else 4\n",
    "fp16_enabled = False\n",
    "use_8bit_optimizer = False # keep adamw_torch for stability\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"./{task}_flan_t5_small_lora_{dataset_size}_{'mac' if IS_MAC else 'gpu'}\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=grad_accum,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01, # safer for LoRA-only training\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=fp16_enabled,\n",
    "    bf16=False,\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=False, # avoid known PEFT issues\n",
    "    optim=\"adamw_torch\",\n",
    "    learning_rate=5e-5,\n",
    "    seed=42,\n",
    "    dataloader_pin_memory=not IS_MAC,\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=1,\n",
    ")\n",
    "print(\" Training arguments configured\")\n",
    "# ============================================================================\n",
    "# CREATE TRAINER\n",
    "# ============================================================================\n",
    "print(\"\\n Initializing Trainer...\")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets.get(\n",
    "        \"validation\", tokenized_datasets[\"train\"]\n",
    "    ),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\" Trainer initialized\")\n",
    "# ============================================================================\n",
    "# TRAINING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "print_gpu_status()\n",
    "print()\n",
    "clear_gpu_memory()\n",
    "try:\n",
    "    model.train() # ensure LoRA params require grad\n",
    "    trainer.train()\n",
    "    print(\"\\n Training completed successfully!\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n Training interrupted by user\")\n",
    "except RuntimeError as e:\n",
    "    if \"does not require grad\" in str(e):\n",
    "        print(\n",
    "            \"\\n RuntimeError: some tensors do not require grad.\\n\"\n",
    "            \" Check that LoRA parameters are trainable and gradient_checkpointing is False.\"\n",
    "        )\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"\\n Out of memory error.\")\n",
    "        print(\" Try reducing dataset_size or lowering lora_r.\")\n",
    "    raise\n",
    "# ============================================================================\n",
    "# PLOTTING METRICS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PLOTTING METRICS\")\n",
    "print(\"=\" * 80)\n",
    "log_history = trainer.state.log_history\n",
    "# Extract data\n",
    "steps = []\n",
    "train_losses = []\n",
    "epochs = []\n",
    "eval_losses = []\n",
    "eval_metrics = {}  # for different metrics\n",
    "for log in log_history:\n",
    "    if 'loss' in log:\n",
    "        steps.append(log['step'])\n",
    "        train_losses.append(log['loss'])\n",
    "    if 'eval_loss' in log:\n",
    "        epochs.append(log['epoch'])\n",
    "        eval_losses.append(log['eval_loss'])\n",
    "        # Add other metrics\n",
    "        for k in log:\n",
    "            if k.startswith('eval_') and k != 'eval_loss' and k != 'eval_runtime' and k != 'eval_samples_per_second' and k != 'eval_steps_per_second':\n",
    "                if k not in eval_metrics:\n",
    "                    eval_metrics[k] = []\n",
    "                eval_metrics[k].append(log[k])\n",
    "# Plot training loss\n",
    "if train_losses:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(steps, train_losses, label='Training Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Steps')\n",
    "    plt.legend()\n",
    "    plt.savefig('training_loss.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "# Plot eval loss\n",
    "if eval_losses:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(epochs, eval_losses, label='Evaluation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Evaluation Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.savefig('eval_loss.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "# Plot eval metrics\n",
    "for metric_name, values in eval_metrics.items():\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(epochs, values, label=metric_name)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric_name.split('eval_')[1])\n",
    "    plt.title(f'{metric_name} over Epochs')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{metric_name}.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "# ============================================================================\n",
    "# EVALUATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATING\")\n",
    "print(\"=\" * 80)\n",
    "clear_gpu_memory()\n",
    "try:\n",
    "    test_results = trainer.evaluate(\n",
    "        tokenized_datasets.get(\"test\", tokenized_datasets[\"validation\"])\n",
    "    )\n",
    "    print(\"\\n Test Results:\")\n",
    "    for k, v in test_results.items():\n",
    "        print(f\" {k}: {v}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n Evaluation error: {e}\")\n",
    "    test_results = {}\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "print(\"\\n Saving model and metrics...\")\n",
    "model_dir = f\"./{task}_flan_t5_small_lora_{dataset_size}_{'mac' if IS_MAC else 'gpu'}\"\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "metrics_file = (\n",
    "    f\"{task}_metrics_{dataset_size}_{'mac' if IS_MAC else 'gpu'}.json\"\n",
    ")\n",
    "metrics_dict = {\n",
    "    \"model\": model_name,\n",
    "    \"task\": task,\n",
    "    \"platform\": \"Mac (MPS)\"\n",
    "    if IS_MAC\n",
    "    else \"GPU (CUDA)\"\n",
    "    if IS_CUDA\n",
    "    else \"CPU\",\n",
    "    \"dataset_size\": dataset_size,\n",
    "    \"test_results\": test_results,\n",
    "    \"training_config\": {\n",
    "        \"epochs\": training_args.num_train_epochs,\n",
    "        \"batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"gradient_accumulation\": training_args.gradient_accumulation_steps,\n",
    "        \"lora_r\": lora_r,\n",
    "        \"lora_alpha\": lora_alpha,\n",
    "        \"fp16\": fp16_enabled,\n",
    "        \"optimizer\": training_args.optim,\n",
    "    },\n",
    "}\n",
    "with open(metrics_file, \"w\") as f:\n",
    "    json.dump(metrics_dict, f, indent=4)\n",
    "print(f\" Model saved to {model_dir}\")\n",
    "print(f\" Metrics saved to {metrics_file}\")\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\" Platform: {'Mac (MPS)' if IS_MAC else 'GPU (CUDA)' if IS_CUDA else 'CPU'}\")\n",
    "print(f\" Task: {task}\")\n",
    "print(f\" Model: {model_name}\")\n",
    "print(f\" Dataset Size: {dataset_size}\")\n",
    "print(f\" Training Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\" Gradient Accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\" LoRA Rank: {lora_r}\")\n",
    "print(f\" Test Results: {test_results}\")\n",
    "print(f\" Model Directory: {model_dir}\")\n",
    "print(f\" Metrics File: {metrics_file}\")\n",
    "print_gpu_status()\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n Training Complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b8da7",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/sanjeevtrivedi/st-lora-sum-ipynb\n",
    "https://www.kaggle.com/code/sanjeevtrivedi/st-lora-clf-ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
