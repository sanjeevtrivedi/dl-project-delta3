{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62641d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    PrefixTuningConfig,\n",
    "    PromptTuningConfig,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "import logging\n",
    "import warnings\n",
    "import json # For saving log_history if needed\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# KAGGLE TOGGLE \n",
    "IS_KAGGLE = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', None) is not None  # Detect if running on Kaggle\n",
    "if IS_KAGGLE:\n",
    "    os.system('pip install transformers==4.57.1 peft==0.17.1 datasets==4.3.0 torch==2.9.0 evaluate==0.4.6 rouge-score==0.1.2 scikit-learn==1.7.2 accelerate==1.11.0 matplotlib==3.10.7 seaborn==0.13.2 wandb==0.22.3 tabulate==0.9.0 --no-deps')\n",
    "\n",
    "# DEVICE DETECTION \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if IS_KAGGLE else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# CONFIGURATION \n",
    "MODEL_NAME = \"t5-small\" # Switched from flan-t5-small to avoid config dim bug (num_heads=6 mismatch)\n",
    "DATASET_SIZE = 500 # or 'full'\n",
    "RUN_ABLATIONS = True  # Toggle to enable/disable ablation study (modular flag)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "NUM_VIRTUAL_TOKENS = 20 # For truncation safety\n",
    "MAX_POS = 512\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PEFT COMPARISON - T5-small\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset size: {DATASET_SIZE}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(\"Methods: LoRA, Prefix-Tuning, Prompt-Tuning, Full FT\")\n",
    "if RUN_ABLATIONS:\n",
    "    print(\"Ablations Enabled: Including ablated variants for study\")\n",
    "    print(\"Note: For LoRA ablation, using lora_alpha=0 to nullify adapter effect\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "# UTILITIES \n",
    "def limit_dataset_size(dataset, size):\n",
    "    if size == 'full':\n",
    "        return dataset\n",
    "    if isinstance(size, int) and size > 0:\n",
    "        return dataset.select(range(min(size, len(dataset))))\n",
    "    raise ValueError(f\"Invalid size: {size}\")\n",
    "def setup_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "def safe_cleanup():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    elif device.type == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "def plot_learning_curves(log_history, exp_name, task_name, save_dir=\"./plots\"):\n",
    "    \"\"\"Plot train/eval loss and task-specific metrics vs step.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "   \n",
    "    # Extract data\n",
    "    steps = [log['step'] for log in log_history if 'step' in log]\n",
    "    train_losses = [log['train_loss'] for log in log_history if 'train_loss' in log]\n",
    "    eval_losses = [log['eval_loss'] for log in log_history if 'eval_loss' in log]\n",
    "   \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "   \n",
    "    # Loss curve\n",
    "    axes[0].plot(steps[:len(train_losses)], train_losses, label='Train Loss', marker='o')\n",
    "    if eval_losses:\n",
    "        axes[0].plot(steps[:len(eval_losses)], eval_losses, label='Eval Loss', marker='s')\n",
    "    axes[0].set_xlabel('Step')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title(f'{exp_name} - Loss Curve')\n",
    "    axes[0].legend()\n",
    "   \n",
    "    # Task-specific metric\n",
    "    if task_name == \"classification\":\n",
    "        eval_accs = [log['eval_accuracy'] for log in log_history if 'eval_accuracy' in log]\n",
    "        if eval_accs:\n",
    "            axes[1].plot(steps[:len(eval_accs)], eval_accs, label='Eval Accuracy', marker='o', color='green')\n",
    "            axes[1].set_ylabel('Accuracy')\n",
    "    else: # summarization\n",
    "        eval_rouge_ls = [log['eval_rougeL'] for log in log_history if 'eval_rougeL' in log]\n",
    "        if eval_rouge_ls:\n",
    "            axes[1].plot(steps[:len(eval_rouge_ls)], eval_rouge_ls, label='Eval ROUGE-L', marker='o', color='green')\n",
    "            axes[1].set_ylabel('ROUGE-L')\n",
    "   \n",
    "    axes[1].set_xlabel('Step')\n",
    "    axes[1].set_title(f'{exp_name} - {task_name.capitalize()} Metric')\n",
    "    axes[1].legend()\n",
    "   \n",
    "    plt.tight_layout()\n",
    "    plot_path = f\"{save_dir}/{exp_name}_curves.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"✓ Learning curves saved to {plot_path}\")\n",
    "    return plot_path\n",
    "\n",
    "\n",
    "def plot_ablation_comparisons(results, task_name, save_dir=\"./plots\"):\n",
    "    \"\"\"Graphical analysis: Compare baselines vs ablations for a task.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    methods = list(results.keys())\n",
    "    baselines = [m for m in methods if \"_ablated_\" not in m]\n",
    "    ablations = [m for m in methods if \"_ablated_\" in m]\n",
    "    \n",
    "    if not ablations:\n",
    "        return None\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Trainable params comparison\n",
    "    trainable_pcts = [100 * results[m][\"trainable_params\"] / results[m][\"total_params\"] for m in methods]\n",
    "    sns.barplot(x=methods, y=trainable_pcts, ax=axes[0])\n",
    "    axes[0].set_ylabel('Trainable %')\n",
    "    axes[0].set_title(f'Trainable Params Comparison - {task_name.capitalize()}')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Metric comparison (use key metric)\n",
    "    if task_name == \"classification\":\n",
    "        metrics = [results[m][\"test_metrics\"].get(\"eval_accuracy\", 0) for m in methods]\n",
    "        metric_label = 'Accuracy'\n",
    "    else:\n",
    "        metrics = [results[m][\"test_metrics\"].get(\"eval_rougeL\", 0) for m in methods]\n",
    "        metric_label = 'ROUGE-L'\n",
    "    \n",
    "    sns.barplot(x=methods, y=metrics, ax=axes[1])\n",
    "    axes[1].set_ylabel(metric_label)\n",
    "    axes[1].set_title(f'Performance Comparison - {task_name.capitalize()}')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = f\"{save_dir}/ablation_comparison_{task_name}.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"✓ Ablation comparison plot saved to {plot_path}\")\n",
    "    return plot_path\n",
    "\n",
    "\n",
    "# LOAD DATASETS \n",
    "print(\"Loading datasets...\")\n",
    "classification_dataset = load_dataset(\"glue\", \"sst2\")\n",
    "summarization_dataset = load_dataset(\"knkarthick/samsum\")\n",
    "tokenizer = setup_tokenizer(MODEL_NAME)\n",
    "if DATASET_SIZE != 'full':\n",
    "    classification_dataset['train'] = limit_dataset_size(classification_dataset['train'], DATASET_SIZE)\n",
    "    classification_dataset['validation'] = limit_dataset_size(classification_dataset['validation'], DATASET_SIZE // 4)\n",
    "    classification_dataset['test'] = limit_dataset_size(classification_dataset.get('test', classification_dataset['validation']), DATASET_SIZE // 4)\n",
    "    summarization_dataset['train'] = limit_dataset_size(summarization_dataset['train'], DATASET_SIZE)\n",
    "    summarization_dataset['validation'] = limit_dataset_size(summarization_dataset['validation'], DATASET_SIZE // 4)\n",
    "    summarization_dataset['test'] = limit_dataset_size(summarization_dataset['test'], DATASET_SIZE // 4)\n",
    "print(\"✓ Datasets loaded\\n\")\n",
    "# PREPROCESSING \n",
    "def preprocess_classification(examples):\n",
    "    inputs = [f\"Classify sentiment: {text}\" for text in examples[\"sentence\"]]\n",
    "    max_input_len = MAX_POS - NUM_VIRTUAL_TOKENS\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_len, truncation=True, padding=\"max_length\")\n",
    "    labels_text = [\"negative\" if label == 0 else \"positive\" for label in examples[\"label\"]]\n",
    "    labels = tokenizer(text_target=labels_text, max_length=10, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "def preprocess_summarization(examples):\n",
    "    inputs = [f\"Summarize the following conversation:\\n{dialogue}\" for dialogue in examples[\"dialogue\"]]\n",
    "    max_input_len = MAX_POS - NUM_VIRTUAL_TOKENS\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_len, truncation=True, padding=\"max_length\")\n",
    "    max_label_len = 128 - NUM_VIRTUAL_TOKENS\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=max_label_len, truncation=True, padding=\"max_length\").input_ids\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "tokenized_classification = classification_dataset.map(preprocess_classification, batched=True, remove_columns=classification_dataset[\"train\"].column_names)\n",
    "tokenized_summarization = summarization_dataset.map(preprocess_summarization, batched=True, remove_columns=summarization_dataset[\"train\"].column_names)\n",
    "print(\"✓ Preprocessing complete\\n\")\n",
    "# METRICS \n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "def compute_classification_metrics(eval_pred):\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        if len(predictions.shape) == 3:\n",
    "            predictions = np.argmax(predictions, axis=-1)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        decoded_preds = [p.strip().lower() for p in decoded_preds]\n",
    "        decoded_labels = [l.strip().lower() for l in decoded_labels]\n",
    "        pred_binary = [1 if 'positive' in p else 0 for p in decoded_preds]\n",
    "        label_binary = [1 if 'positive' in l else 0 for l in decoded_labels]\n",
    "        acc = accuracy_metric.compute(predictions=pred_binary, references=label_binary)\n",
    "        f1 = f1_metric.compute(predictions=pred_binary, references=label_binary, average=\"weighted\")\n",
    "        return {\"accuracy\": acc[\"accuracy\"], \"f1\": f1[\"f1\"]}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Metrics error: {e}\")\n",
    "        return {\"accuracy\": 0.0, \"f1\": 0.0}\n",
    "def compute_summarization_metrics(eval_pred):\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        if len(predictions.shape) == 3:\n",
    "            predictions = np.argmax(predictions, axis=-1)\n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        decoded_preds = [p.strip() if p.strip() else \"empty\" for p in decoded_preds]\n",
    "        decoded_labels = [l.strip() if l.strip() else \"empty\" for l in decoded_labels]\n",
    "        result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        return {\n",
    "            \"rouge1\": result[\"rouge1\"],\n",
    "            \"rouge2\": result[\"rouge2\"],\n",
    "            \"rougeL\": result[\"rougeL\"],\n",
    "            \"rougeLsum\": result[\"rougeLsum\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Metrics error: {e}\")\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0}\n",
    "# TRAINING ARGS \n",
    "def get_training_args(method_name, task_name):\n",
    "    is_peft = method_name in [\"lora\", \"prefix\", \"prompt\"] or \"_ablated_\" in method_name\n",
    "    lr = 1e-3 if is_peft else 5e-5\n",
    "    if DATASET_SIZE == 'full':\n",
    "        epochs, batch, eval_steps = 3, 8, 500\n",
    "    elif DATASET_SIZE <= 500:\n",
    "        epochs, batch, eval_steps = 5, 4, 50\n",
    "    else:\n",
    "        epochs, batch, eval_steps = 3, 8, 100\n",
    "    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    use_fp16 = False  # Disabled to avoid NaN losses\n",
    "    load_best = method_name == \"full_ft\" or \"lora\" in method_name\n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"./results/{task_name}/{method_name}\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch,\n",
    "        per_device_eval_batch_size=batch * 2,\n",
    "        learning_rate=lr,\n",
    "        warmup_steps=min(100, DATASET_SIZE // 10) if DATASET_SIZE != 'full' else 500,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"steps\" if DATASET_SIZE != 'full' else \"epoch\",\n",
    "        eval_steps=eval_steps if DATASET_SIZE != 'full' else None,\n",
    "        save_strategy=\"steps\" if DATASET_SIZE != 'full' else \"epoch\",\n",
    "        save_steps=eval_steps if DATASET_SIZE != 'full' else None,\n",
    "        load_best_model_at_end=load_best,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        save_total_limit=2,\n",
    "        logging_steps=20 if DATASET_SIZE != 'full' else 100,\n",
    "        bf16=use_bf16,\n",
    "        fp16=use_fp16,\n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_drop_last=True, # Avoid incomplete batches for stability\n",
    "        report_to=\"none\",\n",
    "        predict_with_generate=True,\n",
    "        max_grad_norm=1.0,  # Added to prevent gradient explosions\n",
    "    )\n",
    "# MAIN TRAINING LOOP \n",
    "base_methods = [\"lora\", \"prefix\", \"prompt\", \"full_ft\"]\n",
    "ablation_methods = [\"lora_ablated_alpha0\", \"prefix_ablated_no_proj\", \"prompt_ablated_short\"]\n",
    "methods_to_run = base_methods + (ablation_methods if RUN_ABLATIONS else [])\n",
    "tasks = {\n",
    "    \"classification\": (tokenized_classification, compute_classification_metrics),\n",
    "    \"summarization\": (tokenized_summarization, compute_summarization_metrics)\n",
    "}\n",
    "results = {}\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "os.makedirs(\"./plots\", exist_ok=True) # For curves\n",
    "for method_name in methods_to_run:\n",
    "    for task_name, (dataset, compute_metrics) in tasks.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EXPERIMENT: {method_name.upper()} on {task_name.upper()}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        try:\n",
    "            config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "            use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                config=config,\n",
    "                dtype=torch.bfloat16 if use_bf16 else torch.float32,\n",
    "            )\n",
    "            model.to(device)\n",
    "            # Note: t5-small has correct dims (num_heads=8, head_dim=64); PEFT handles DynamicCache natively.\n",
    "            # Create PEFT configs dynamically from model.config\n",
    "            if method_name != \"full_ft\":\n",
    "                d_model = model.config.d_model\n",
    "                num_heads = model.config.num_heads\n",
    "                total_layers = model.config.num_layers + model.config.num_decoder_layers\n",
    "                peft_configs_local = {\n",
    "                    \"lora\": LoraConfig(\n",
    "                        r=16,\n",
    "                        lora_alpha=32,\n",
    "                        target_modules=[\"q\", \"v\"],\n",
    "                        lora_dropout=0.05,\n",
    "                        bias=\"none\",\n",
    "                        task_type=TaskType.SEQ_2_SEQ_LM\n",
    "                    ),\n",
    "                    \"lora_ablated_alpha0\": LoraConfig(\n",
    "                        r=16,\n",
    "                        lora_alpha=0,  # Ablation: zero scaling, no effect from adapter\n",
    "                        target_modules=[\"q\", \"v\"],\n",
    "                        lora_dropout=0.05,\n",
    "                        bias=\"none\",\n",
    "                        task_type=TaskType.SEQ_2_SEQ_LM\n",
    "                    ),\n",
    "                    \"prefix\": PrefixTuningConfig(\n",
    "                        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "                        inference_mode=False,\n",
    "                        num_virtual_tokens=NUM_VIRTUAL_TOKENS,\n",
    "                        token_dim=d_model,\n",
    "                        num_transformer_submodules=2,\n",
    "                        num_attention_heads=num_heads,\n",
    "                        num_layers=total_layers,\n",
    "                        encoder_hidden_size=d_model,\n",
    "                        prefix_projection=True  # Baseline with projection\n",
    "                    ),\n",
    "                    \"prefix_ablated_no_proj\": PrefixTuningConfig(  # Ablation: Remove projection layer\n",
    "                        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "                        inference_mode=False,\n",
    "                        num_virtual_tokens=NUM_VIRTUAL_TOKENS,\n",
    "                        token_dim=d_model,\n",
    "                        num_transformer_submodules=2,\n",
    "                        num_attention_heads=num_heads,\n",
    "                        num_layers=total_layers,\n",
    "                        encoder_hidden_size=d_model,\n",
    "                        prefix_projection=False  # Ablated\n",
    "                    ),\n",
    "                    \"prompt\": PromptTuningConfig(\n",
    "                        num_virtual_tokens=NUM_VIRTUAL_TOKENS,\n",
    "                        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "                        prompt_tuning_init=\"RANDOM\"\n",
    "                    ),\n",
    "                    \"prompt_ablated_short\": PromptTuningConfig(  # Ablation: Fewer tokens (e.g., half)\n",
    "                        num_virtual_tokens=NUM_VIRTUAL_TOKENS // 2,\n",
    "                        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "                        prompt_tuning_init=\"RANDOM\"\n",
    "                    )\n",
    "                }\n",
    "                model = get_peft_model(model, peft_configs_local[method_name])\n",
    "                model.print_trainable_parameters()\n",
    "            else:\n",
    "                trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                total = sum(p.numel() for p in model.parameters())\n",
    "                print(f\"trainable params: {trainable:,} || all params: {total:,} || trainable%: 100.00\")\n",
    "            training_args = get_training_args(method_name, task_name)\n",
    "            data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=dataset[\"train\"],\n",
    "                eval_dataset=dataset[\"validation\"],\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_metrics,\n",
    "                tokenizer=tokenizer\n",
    "            )\n",
    "            print(\"Training...\")\n",
    "            train_result = trainer.train()\n",
    "            # Manual load best model for prefix/prompt methods\n",
    "            if not training_args.load_best_model_at_end and trainer.state.best_model_checkpoint:\n",
    "                print(f\"Loading best checkpoint manually: {trainer.state.best_model_checkpoint}\")\n",
    "                base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    MODEL_NAME,\n",
    "                    config=config,\n",
    "                    dtype=torch.bfloat16 if use_bf16 else torch.float32,\n",
    "                )\n",
    "                base_model.to(device)\n",
    "                model = PeftModel.from_pretrained(base_model, trainer.state.best_model_checkpoint)\n",
    "                trainer.model = model\n",
    "                model.to(device)  # Ensure the full PEFT model is on device\n",
    "            print(\"Evaluating...\")\n",
    "            test_dataset = dataset.get(\"test\", dataset[\"validation\"])\n",
    "            gen_kwargs = {\n",
    "                \"max_length\": 128 if task_name == \"summarization\" else 10,\n",
    "                \"num_beams\": 4,\n",
    "                \"early_stopping\": True,\n",
    "            }\n",
    "            training_args.generation_max_length = gen_kwargs[\"max_length\"]\n",
    "            training_args.generation_num_beams = gen_kwargs[\"num_beams\"]\n",
    "            test_metrics = trainer.evaluate(test_dataset)\n",
    "            exp_name = f\"{method_name}_{task_name}\"\n",
    "            trainable = model.num_parameters(only_trainable=True) if hasattr(model, 'num_parameters') else sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            total = model.num_parameters() if hasattr(model, 'num_parameters') else sum(p.numel() for p in model.parameters())\n",
    "            results[exp_name] = {\n",
    "                \"train_metrics\": train_result.metrics,\n",
    "                \"test_metrics\": test_metrics,\n",
    "                \"trainable_params\": trainable,\n",
    "                \"total_params\": total,\n",
    "                \"log_history\": trainer.state.log_history # Collect for plotting\n",
    "            }\n",
    "            save_path = f\"./models/{task_name}/{method_name}\"\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            trainer.save_model(save_path)\n",
    "            print(f\"✓ Completed and saved to {save_path}\\n\")\n",
    "            del model, trainer\n",
    "            safe_cleanup()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"ERROR in {method_name}_{task_name}: {e}\")\n",
    "            import traceback\n",
    "            logger.error(traceback.format_exc())\n",
    "            try:\n",
    "                del model, trainer\n",
    "            except:\n",
    "                pass\n",
    "            safe_cleanup()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL EXPERIMENTS COMPLETED\")\n",
    "print(\"=\"*60)\n",
    "# RESULTS \n",
    "if results:\n",
    "    print(\"\\nRESULTS SUMMARY:\")\n",
    "    print(\"=\"*60)\n",
    "    for exp_name, exp_data in results.items():\n",
    "        method, task = exp_name.split(\"_\", 1)\n",
    "        metrics = exp_data[\"test_metrics\"]\n",
    "        pct = 100 * exp_data[\"trainable_params\"] / exp_data[\"total_params\"]\n",
    "        print(f\"\\n{method.upper()} - {task.capitalize()}:\")\n",
    "        print(f\" Trainable: {pct:.2f}%\")\n",
    "        if task == \"classification\":\n",
    "            print(f\" Accuracy: {metrics.get('eval_accuracy', 0):.4f}\")\n",
    "            print(f\" F1: {metrics.get('eval_f1', 0):.4f}\")\n",
    "        else:\n",
    "            print(f\" ROUGE-1: {metrics.get('eval_rouge1', 0):.4f}\")\n",
    "            print(f\" ROUGE-L: {metrics.get('eval_rougeL', 0):.4f}\")\n",
    "    # Ablation deltas if enabled\n",
    "    if RUN_ABLATIONS:\n",
    "        print(\"\\nABLATION DELTAS:\")\n",
    "        for exp_name, exp_data in results.items():\n",
    "            method, task = exp_name.split(\"_\", 1)\n",
    "            if \"_ablated_\" in method:\n",
    "                base_method = method.split(\"_ablated_\")[0] + \"_\" + task\n",
    "                if base_method in results:\n",
    "                    base_metrics = results[base_method][\"test_metrics\"]\n",
    "                    delta = {k: exp_data[\"test_metrics\"].get(k, 0) - base_metrics.get(k, 0) for k in base_metrics if \"eval_\" in k}\n",
    "                    print(f\"Delta for {method.upper()} - {task.capitalize()}: {delta}\")\n",
    "    # Plot learning curves for each experiment\n",
    "    print(\"\\nGenerating learning curves...\")\n",
    "    plot_paths = {}\n",
    "    for exp_name, exp_data in results.items():\n",
    "        task_name = exp_name.split(\"_\", 1)[1]\n",
    "        plot_path = plot_learning_curves(exp_data[\"log_history\"], exp_name, task_name)\n",
    "        plot_paths[exp_name] = plot_path\n",
    "    # Graphical ablation comparisons per task\n",
    "    if RUN_ABLATIONS:\n",
    "        print(\"\\nGenerating ablation comparison plots...\")\n",
    "        ablation_plot_paths = {}\n",
    "        for task_name in tasks.keys():\n",
    "            task_results = {k: v for k, v in results.items() if k.endswith(f\"_{task_name}\")}\n",
    "            if task_results:\n",
    "                ablation_plot_path = plot_ablation_comparisons(task_results, task_name)\n",
    "                if ablation_plot_path:\n",
    "                    ablation_plot_paths[task_name] = ablation_plot_path\n",
    "    results_df = []\n",
    "    for exp_name, exp_data in results.items():\n",
    "        method, task = exp_name.split(\"_\", 1)\n",
    "        results_df.append({\n",
    "            \"Method\": method.upper(),\n",
    "            \"Task\": task.capitalize(),\n",
    "            \"Trainable %\": 100 * exp_data[\"trainable_params\"] / exp_data[\"total_params\"],\n",
    "            **{k: v for k, v in exp_data[\"test_metrics\"].items() if isinstance(v, (int, float))}\n",
    "        })\n",
    "    df = pd.DataFrame(results_df)\n",
    "    cols = [\"Method\", \"Task\", \"Trainable %\"]\n",
    "    metric_cols = [c for c in df.columns if c.startswith(\"eval_\")]\n",
    "    cols.extend(sorted(metric_cols))\n",
    "    df = df[cols]\n",
    "    df.to_csv(\"peft_results.csv\", index=False)\n",
    "    print(f\"\\n✓ Results saved to 'peft_results.csv'\")\n",
    "    with open(\"final_report.md\", \"w\") as f:\n",
    "        f.write(f\"# PEFT Comparison Results - T5-small\\n\\n\")\n",
    "        f.write(f\"## Configuration\\n\")\n",
    "        f.write(f\"- Model: {MODEL_NAME} (switched from flan-t5-small to fix config dim bug)\\n\")\n",
    "        f.write(f\"- Dataset Size: {DATASET_SIZE}\\n\")\n",
    "        f.write(f\"- Methods: LoRA, Prefix-Tuning, Prompt-Tuning, Full Fine-Tuning\\n\")\n",
    "        if RUN_ABLATIONS:\n",
    "            f.write(f\"- Ablations: Enabled (including ablated variants); LoRA ablation uses lora_alpha=0 for no adaptation effect\\n\")\n",
    "        f.write(f\"- Special: Native DynamicCache support; correct dims (num_heads=8, head_dim=64)\\n\\n\")\n",
    "        f.write(f\"## Summary Table\\n\\n\")\n",
    "        f.write(df.to_markdown(index=False))\n",
    "        f.write(\"\\n\\n## Learning Curves\\n\")\n",
    "        for exp_name, plot_path in plot_paths.items():\n",
    "            f.write(f\"- [{exp_name}]({plot_path})\\n\")\n",
    "        if RUN_ABLATIONS and ablation_plot_paths:\n",
    "            f.write(\"\\n## Ablation Comparisons\\n\")\n",
    "            for task_name, plot_path in ablation_plot_paths.items():\n",
    "                f.write(f\"- [{task_name.capitalize()} Ablation Comparison]({plot_path})\\n\")\n",
    "    print(\"✓ Report saved to 'final_report.md' (includes plot links)\")\n",
    "    # Generate dynamic outcome insights based on results\n",
    "    print(\"\\nOUTCOME INSIGHTS:\")\n",
    "    if results:\n",
    "        # General insights from trainable params and metrics\n",
    "        for task in tasks.keys():\n",
    "            task_exps = {k: v for k, v in results.items() if k.endswith(task)}\n",
    "            if task_exps:\n",
    "                # Find method with lowest trainable %\n",
    "                min_trainable_method = min(task_exps, key=lambda k: 100 * task_exps[k][\"trainable_params\"] / task_exps[k][\"total_params\"])\n",
    "                min_pct = 100 * task_exps[min_trainable_method][\"trainable_params\"] / task_exps[min_trainable_method][\"total_params\"]\n",
    "                print(f\"- For {task.capitalize()}, {min_trainable_method.split('_')[0].upper()} has the lowest trainable params ({min_pct:.2f}%).\")\n",
    "                \n",
    "                # Find best performing method (use key metric)\n",
    "                key_metric = 'eval_accuracy' if task == 'classification' else 'eval_rougeL'\n",
    "                best_method = max(task_exps, key=lambda k: task_exps[k][\"test_metrics\"].get(key_metric, 0))\n",
    "                best_score = task_exps[best_method][\"test_metrics\"].get(key_metric, 0)\n",
    "                print(f\"- {best_method.split('_')[0].upper()} achieves the highest {key_metric.replace('eval_', '').upper()} score ({best_score:.4f}) on {task.capitalize()}.\")\n",
    "        \n",
    "        # Ablation-specific insights\n",
    "        if RUN_ABLATIONS:\n",
    "            for exp_name, exp_data in results.items():\n",
    "                method, task = exp_name.split(\"_\", 1)\n",
    "                if \"_ablated_\" in method:\n",
    "                    base_method = method.split(\"_ablated_\")[0] + \"_\" + task\n",
    "                    if base_method in results:\n",
    "                        base_metrics = results[base_method][\"test_metrics\"]\n",
    "                        delta = {k: exp_data[\"test_metrics\"].get(k, 0) - base_metrics.get(k, 0) for k in base_metrics if \"eval_\" in k}\n",
    "                        key_delta = delta.get('eval_accuracy' if task == 'classification' else 'eval_rougeL', 0)\n",
    "                        impact = \"degradation\" if key_delta < 0 else \"improvement\" if key_delta > 0 else \"no change\"\n",
    "                        print(f\"- Ablation in {method.upper()} on {task.capitalize()} leads to {impact} in performance (delta: {key_delta:.4f}).\")\n",
    "        \n",
    "        print(f\"View plots in ./plots/ for detailed curves (loss/metric vs step) and comparisons.\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUCCESS - All 4 PEFT methods completed!\" + (\" With ablations!\" if RUN_ABLATIONS else \"\"))\n",
    "print(\"=\"*60)\n",
    "print(\"\\nKey Features:\")\n",
    "print(\"LoRA: Most efficient and reliable\")\n",
    "print(\"Prefix-Tuning: Fully compatible with correct T5 config (no reshape errors)\")\n",
    "print(\"Prompt-Tuning: Ultra parameter-efficient\")\n",
    "print(\"Full Fine-Tuning: Baseline comparison\")\n",
    "if RUN_ABLATIONS:\n",
    "    print(\"Ablation Study: Modular, toggle with RUN_ABLATIONS flag; includes deltas and comparison plots\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
