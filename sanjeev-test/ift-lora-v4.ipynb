{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad1dec4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Kaggle: False\n",
      "Using device: mps\n",
      "============================================================\n",
      "LoRA and Full Fine-Tuning COMPARISON - flan-t5-small\n",
      "============================================================\n",
      "Dataset size: 1000\n",
      "Model: google/flan-t5-small\n",
      "Methods: LoRA, Full Fine-Tuning\n",
      "============================================================\n",
      "\n",
      "Loading datasets\n",
      "Limiting dataset size to 1000 for train.\n",
      "Datasets loaded\n",
      "\n",
      "\n",
      "Applying preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 3657.93 examples/s]\n",
      "Map: 100%|██████████| 250/250 [00:00<00:00, 5221.16 examples/s]\n",
      "Map: 100%|██████████| 250/250 [00:00<00:00, 5783.72 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2464.95 examples/s]\n",
      "Map: 100%|██████████| 250/250 [00:00<00:00, 2397.69 examples/s]\n",
      "Map: 100%|██████████| 250/250 [00:00<00:00, 2403.79 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing complete\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: LORA on CLASSIFICATION\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/flan-t5-small and are newly initialized because the shapes did not match:\n",
      "- decoder.block.0.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: found shape torch.Size([32, 6]) in the checkpoint and torch.Size([32, 8]) in the model instantiated\n",
      "- decoder.block.0.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.0.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.0.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.0.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: found shape torch.Size([32, 6]) in the checkpoint and torch.Size([32, 8]) in the model instantiated\n",
      "- encoder.block.0.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.1.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.1.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.1.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.1.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.2.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.2.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.2.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.2.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.3.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.3.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.3.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.3.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.4.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.4.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.4.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.4.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.5.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.5.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.5.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.5.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.6.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.6.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.6.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.6.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.7.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.7.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.7.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.7.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 84,825,600 || trainable%: 1.8542\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 03:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>25.137300</td>\n",
       "      <td>28.537214</td>\n",
       "      <td>0.491667</td>\n",
       "      <td>0.324115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sample pred: ship ship ship ship ship ship ship ship ship ship, label: positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sample pred: ship ship, label: positive\n",
      "INFO:__main__:Sample pred: ship ship, label: positive\n",
      "INFO:__main__:Sample generations: ['ship ship', 'ship ship', 'ship ship', 'ship ship', 'ship ship']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed and saved to ./outputs/ift-lora-v4/models/classification/lora\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: LORA on SUMMARIZATION\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/flan-t5-small and are newly initialized because the shapes did not match:\n",
      "- decoder.block.0.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: found shape torch.Size([32, 6]) in the checkpoint and torch.Size([32, 8]) in the model instantiated\n",
      "- decoder.block.0.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.0.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.0.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.0.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: found shape torch.Size([32, 6]) in the checkpoint and torch.Size([32, 8]) in the model instantiated\n",
      "- encoder.block.0.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.1.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.1.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.1.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.1.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.2.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.2.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.2.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.2.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.3.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.3.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.3.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.3.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.4.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.4.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.4.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.4.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.5.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.5.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.5.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.5.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.6.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.6.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.6.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.6.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.7.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.7.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.7.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.7.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 84,825,600 || trainable%: 1.8542\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 04:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>21.106400</td>\n",
       "      <td>24.095316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sample pred: ship ship ship ship ship ship ship ship ship ship, label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sample pred: ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship , label: Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship , label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample generations: ['ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ', 'ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ', 'ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ', 'ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ', 'ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ship ']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed and saved to ./outputs/ift-lora-v4/models/summarization/lora\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: FULL_FT on CLASSIFICATION\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/flan-t5-small and are newly initialized because the shapes did not match:\n",
      "- decoder.block.0.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: found shape torch.Size([32, 6]) in the checkpoint and torch.Size([32, 8]) in the model instantiated\n",
      "- decoder.block.0.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.0.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.0.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.0.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: found shape torch.Size([32, 6]) in the checkpoint and torch.Size([32, 8]) in the model instantiated\n",
      "- encoder.block.0.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.1.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.1.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.1.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.1.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.2.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.2.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.2.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.2.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.3.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.3.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.3.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.3.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.4.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.4.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.4.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.4.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.5.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.5.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.5.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.5.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.6.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.6.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.6.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.6.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.7.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.7.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.7.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.7.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 83,252,736 || all params: 83,252,736 || trainable%: 100.00\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 03:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>24.741400</td>\n",
       "      <td>28.007496</td>\n",
       "      <td>0.491667</td>\n",
       "      <td>0.324115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sample pred: ship ship ship ship ship ship ship ship ship ship, label: positive\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sample pred: ship ship, label: positive\n",
      "INFO:__main__:Sample pred: ship ship, label: positive\n",
      "INFO:__main__:Sample generations: ['ship ship', 'ship ship', 'ship ship', 'ship ship', 'ship ship']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed and saved to ./outputs/ift-lora-v4/models/classification/full_ft\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: FULL_FT on SUMMARIZATION\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/flan-t5-small and are newly initialized because the shapes did not match:\n",
      "- decoder.block.0.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: found shape torch.Size([32, 6]) in the checkpoint and torch.Size([32, 8]) in the model instantiated\n",
      "- decoder.block.0.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.0.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.1.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.2.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.3.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.4.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.5.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.6.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.1.EncDecAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.1.EncDecAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.1.EncDecAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- decoder.block.7.layer.1.EncDecAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.0.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.0.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.0.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: found shape torch.Size([32, 6]) in the checkpoint and torch.Size([32, 8]) in the model instantiated\n",
      "- encoder.block.0.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.1.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.1.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.1.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.1.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.2.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.2.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.2.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.2.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.3.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.3.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.3.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.3.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.4.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.4.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.4.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.4.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.5.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.5.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.5.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.5.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.6.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.6.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.6.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.6.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.7.layer.0.SelfAttention.k.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.7.layer.0.SelfAttention.o.weight: found shape torch.Size([512, 384]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.7.layer.0.SelfAttention.q.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "- encoder.block.7.layer.0.SelfAttention.v.weight: found shape torch.Size([384, 512]) in the checkpoint and torch.Size([512, 512]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 83,252,736 || all params: 83,252,736 || trainable%: 100.00\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/96 05:27 < 01:07, 0.24 it/s, Epoch 2.48/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>20.930900</td>\n",
       "      <td>23.555183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sample pred: ship ship ship ship ship ship ship ship ship ship, label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 384\u001b[39m\n\u001b[32m    373\u001b[39m trainer = Seq2SeqTrainer(\n\u001b[32m    374\u001b[39m     model=model,\n\u001b[32m    375\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    380\u001b[39m     tokenizer=tokenizer\n\u001b[32m    381\u001b[39m )\n\u001b[32m    383\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m training_args.load_best_model_at_end \u001b[38;5;129;01mand\u001b[39;00m trainer.state.best_model_checkpoint:\n\u001b[32m    387\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading best checkpoint manually: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.state.best_model_checkpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal/IITB-src/dl-project-delta3/.venv/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal/IITB-src/dl-project-delta3/.venv/lib/python3.12/site-packages/transformers/trainer.py:2618\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2616\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2617\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2618\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[32m   2620\u001b[39m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[32m   2621\u001b[39m \u001b[38;5;28mself\u001b[39m.current_gradient_accumulation_steps = \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal/IITB-src/dl-project-delta3/.venv/lib/python3.12/site-packages/transformers/trainer.py:5654\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5652\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5653\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5654\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5655\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5656\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal/IITB-src/dl-project-delta3/.venv/lib/python3.12/site-packages/accelerate/data_loader.py:577\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    575\u001b[39m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m         current_batch = \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    578\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_state_dict()\n\u001b[32m    579\u001b[39m     next_batch = \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal/IITB-src/dl-project-delta3/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py:154\u001b[39m, in \u001b[36msend_to_device\u001b[39m\u001b[34m(tensor, device, non_blocking, skip_keys)\u001b[39m\n\u001b[32m    152\u001b[39m     device = \u001b[33m\"\u001b[39m\u001b[33mnpu:0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/personal/IITB-src/dl-project-delta3/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:838\u001b[39m, in \u001b[36mBatchEncoding.to\u001b[39m\u001b[34m(self, device, non_blocking)\u001b[39m\n\u001b[32m    833\u001b[39m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[32m    835\u001b[39m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    837\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = {\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m         k: \u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(v, \u001b[33m\"\u001b[39m\u001b[33mto\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(v.to) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[32m    839\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data.items()\n\u001b[32m    840\u001b[39m     }\n\u001b[32m    841\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    842\u001b[39m     logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. This is not supported.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "#Project: Compare low-resource adaptation techniques: \n",
    "# (LoRA & Full Fine-Tuning) on two downstream tasks \n",
    "# (classification & summarization). \n",
    "# Report parameter-efficiency vs performance curves.\n",
    "#####################################################################\n",
    "\n",
    "############## Local Working Version. Use Python 3.12.10 ##############\n",
    "#File Name: lora_fullft.ipynb\n",
    "#Create a venv using python3.12 -m venv .venv\n",
    "#Activate the venv using source .venv/bin/activate\n",
    "#Install dependencies using pip install -r requirements.txt\n",
    "#################################################################\n",
    "\n",
    "import os   \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "KAGGLE_REQUIREMENTS_PATH = '/kaggle/input/dependencies/requirements-kaggle-v1.0.txt'\n",
    "\n",
    "# KAGGLE TOGGLE \n",
    "IS_KAGGLE = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', None) is not None  # Detect if running on Kaggle\n",
    "print(f\"Running on Kaggle: {IS_KAGGLE}\")\n",
    "if IS_KAGGLE:\n",
    "    if os.path.exists(KAGGLE_REQUIREMENTS_PATH):\n",
    "        print(f\"Installing dependencies from {KAGGLE_REQUIREMENTS_PATH}...\")\n",
    "        os.system(f'pip install -r {KAGGLE_REQUIREMENTS_PATH}')\n",
    "    else:\n",
    "        print(f\"WARNING: Could not find {KAGGLE_REQUIREMENTS_PATH}.\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "import logging\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# DEVICE DETECTION \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# CONFIGURATION \n",
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "SUMMARIZATION_DATASET = \"knkarthick/samsum\"\n",
    "BENCHMARK_GLUE = \"glue\"\n",
    "GLUE_DATASET_TASK_SC = \"sst2\"\n",
    "\n",
    "DATASET_SIZE = 1000  # Use 'full' for full dataset\n",
    "RUN_ABLATIONS = False\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "NUM_VIRTUAL_TOKENS = 50\n",
    "MAX_POS = 512\n",
    "\n",
    "OUTPUT_DIR = './outputs/ift-lora-v4' if not IS_KAGGLE else '/kaggle/working/outputs/ift-lora-v4'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LoRA and Full Fine-Tuning COMPARISON - flan-t5-small\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset size: {DATASET_SIZE}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(\"Methods: LoRA, Full Fine-Tuning\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# UTILITIES \n",
    "def limit_dataset_size(dataset, size):\n",
    "    if size == 'full':\n",
    "        return dataset\n",
    "    if isinstance(size, int) and size > 0:\n",
    "        return dataset.select(range(min(size, len(dataset))))\n",
    "    raise ValueError(f\"Invalid size: {size}\")\n",
    "\n",
    "def setup_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "def safe_cleanup():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    elif device.type == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "# PLOTS \n",
    "def plot_learning_curves(log_history, exp_name, task_name, save_dir=\"./plots\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    steps = [log['step'] for log in log_history if 'step' in log and 'eval_loss' not in log]\n",
    "    eval_steps = [log['step'] for log in log_history if 'eval_loss' in log]\n",
    "    train_losses = [log['loss'] for log in log_history if 'loss' in log]\n",
    "    eval_losses = [log['eval_loss'] for log in log_history if 'eval_loss' in log]\n",
    "   \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "   \n",
    "    train_steps_for_loss = [log['step'] for log in log_history if 'loss' in log]\n",
    "    axes[0].plot(train_steps_for_loss, train_losses, label='Train Loss', marker='o', alpha=0.7)\n",
    "    if eval_losses:\n",
    "        axes[0].plot(eval_steps, eval_losses, label='Eval Loss', marker='s')\n",
    "    axes[0].set_xlabel('Step')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title(f'{exp_name} - Loss Curve')\n",
    "    axes[0].legend()\n",
    "   \n",
    "    if task_name == \"classification\":\n",
    "        eval_accs = [log['eval_accuracy'] for log in log_history if 'eval_accuracy' in log]\n",
    "        if eval_accs:\n",
    "            axes[1].plot(eval_steps, eval_accs, label='Eval Accuracy', marker='o', color='green')\n",
    "            axes[1].set_ylabel('Accuracy')\n",
    "    else:\n",
    "        eval_rouge_ls = [log['eval_rougeL'] for log in log_history if 'eval_rougeL' in log]\n",
    "        if eval_rouge_ls:\n",
    "            axes[1].plot(eval_steps, eval_rouge_ls, label='Eval ROUGE-L', marker='o', color='green')\n",
    "            axes[1].set_ylabel('ROUGE-L')\n",
    "   \n",
    "    axes[1].set_xlabel('Step')\n",
    "    axes[1].set_title(f'{exp_name} - {task_name.capitalize()} Metric')\n",
    "    axes[1].legend()\n",
    "   \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(save_dir, f\"{exp_name}_curves.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"✓ Learning curves saved to {plot_path}\")\n",
    "    return plot_path\n",
    "\n",
    "# LOAD DATASETS \n",
    "print(\"Loading datasets\")\n",
    "classification_dataset = load_dataset(BENCHMARK_GLUE, GLUE_DATASET_TASK_SC)\n",
    "summarization_dataset = load_dataset(SUMMARIZATION_DATASET)\n",
    "\n",
    "tokenizer = setup_tokenizer(MODEL_NAME)\n",
    "\n",
    "if DATASET_SIZE != 'full':\n",
    "    print(f\"Limiting dataset size to {DATASET_SIZE} for train.\")\n",
    "    classification_dataset['train'] = limit_dataset_size(classification_dataset['train'], DATASET_SIZE)\n",
    "    classification_dataset['validation'] = limit_dataset_size(classification_dataset['validation'], DATASET_SIZE // 4)\n",
    "    classification_dataset['test'] = limit_dataset_size(classification_dataset.get('test', classification_dataset['validation']), DATASET_SIZE // 4)\n",
    "    \n",
    "    summarization_dataset['train'] = limit_dataset_size(summarization_dataset['train'], DATASET_SIZE)\n",
    "    summarization_dataset['validation'] = limit_dataset_size(summarization_dataset['validation'], DATASET_SIZE // 4)\n",
    "    summarization_dataset['test'] = limit_dataset_size(summarization_dataset['test'], DATASET_SIZE // 4)\n",
    "\n",
    "print(\"Datasets loaded\\n\")\n",
    "\n",
    "# Preprocessing functions (same as before)\n",
    "def preprocess_classification(examples):\n",
    "    inputs = [f\"Classify sentiment: {text}\" for text in examples[\"sentence\"]]\n",
    "    max_input_len = MAX_POS - NUM_VIRTUAL_TOKENS\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_len, truncation=True, padding=\"max_length\")\n",
    "    labels_text = [\"negative\" if label == 0 else \"positive\" for label in examples[\"label\"]]\n",
    "    labels = tokenizer(text_target=labels_text, max_length=10, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_summarization(examples):\n",
    "    inputs = [f\"Summarize the following conversation:\\n{dialogue}\" for dialogue in examples[\"dialogue\"]]\n",
    "    max_input_len = MAX_POS - NUM_VIRTUAL_TOKENS\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_len, truncation=True, padding=\"max_length\")\n",
    "    max_label_len = 128 - NUM_VIRTUAL_TOKENS\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=max_label_len, truncation=True, padding=\"max_length\").input_ids\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "print(\"\\nApplying preprocessing...\")\n",
    "tokenized_classification = classification_dataset.map(preprocess_classification, batched=True, remove_columns=classification_dataset[\"train\"].column_names)\n",
    "tokenized_summarization = summarization_dataset.map(preprocess_summarization, batched=True, remove_columns=summarization_dataset[\"train\"].column_names)\n",
    "\n",
    "print(\"\\nPreprocessing complete\\n\")\n",
    "\n",
    "# Metrics (same as before)\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_classification_metrics(eval_pred):\n",
    "    # ... (same as your original code)\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        if len(predictions.shape) == 3:\n",
    "            predictions = np.argmax(predictions, axis=-1)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "        if np.any(predictions < 0) or np.any(labels < 0):\n",
    "            predictions = np.clip(predictions, 0, None)\n",
    "            labels = np.clip(labels, 0, None)\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        logger.info(f\"Sample pred: {decoded_preds[0]}, label: {decoded_labels[0]}\")\n",
    "        decoded_preds = [p.strip().lower() for p in decoded_preds]\n",
    "        decoded_labels = [l.strip().lower() for l in decoded_labels]\n",
    "        pred_binary = [1 if p == 'positive' else 0 for p in decoded_preds]\n",
    "        label_binary = [1 if l == 'positive' else 0 for l in decoded_labels]\n",
    "        acc = accuracy_metric.compute(predictions=pred_binary, references=label_binary)\n",
    "        f1 = f1_metric.compute(predictions=pred_binary, references=label_binary, average=\"weighted\")\n",
    "        return {\"accuracy\": acc.get(\"accuracy\", 0.0), \"f1\": f1.get(\"f1\", 0.0)}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Classification metrics error: {e}. Returning defaults.\")\n",
    "        return {\"accuracy\": 0.0, \"f1\": 0.0}\n",
    "\n",
    "def compute_summarization_metrics(eval_pred):\n",
    "    # ... (same as your original code)\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        if len(predictions.shape) == 3:\n",
    "            predictions = np.argmax(predictions, axis=-1)\n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        if np.any(predictions < 0) or np.any(labels < 0):\n",
    "            predictions = np.clip(predictions, 0, None)\n",
    "            labels = np.clip(labels, 0, None)\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        logger.info(f\"Sample pred: {decoded_preds[0]}, label: {decoded_labels[0]}\")\n",
    "        decoded_preds = [p.strip() if p.strip() else \"empty\" for p in decoded_preds]\n",
    "        decoded_labels = [l.strip() if l.strip() else \"empty\" for l in decoded_labels]\n",
    "        result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        return {\n",
    "            \"rouge1\": result.get(\"rouge1\", 0.0),\n",
    "            \"rouge2\": result.get(\"rouge2\", 0.0),\n",
    "            \"rougeL\": result.get(\"rougeL\", 0.0),\n",
    "            \"rougeLsum\": result.get(\"rougeLsum\", 0.0)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Summarization metrics error: {e}. Returning defaults.\")\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0}\n",
    "\n",
    "# TRAINING ARGS \n",
    "def get_training_args(method_name, task_name):\n",
    "    is_peft = method_name == \"lora\"\n",
    "    lr = 3e-4 if is_peft else 1e-5\n",
    "    \n",
    "    if DATASET_SIZE == 'full':\n",
    "        epochs = 5 if task_name == 'summarization' else 3\n",
    "        batch, eval_steps = 8, 500\n",
    "    elif DATASET_SIZE <= 500:\n",
    "        epochs, batch, eval_steps = 10, 4, 20\n",
    "    else:\n",
    "        epochs, batch, eval_steps = 3, 8, 100\n",
    "\n",
    "    if DATASET_SIZE != 'full':\n",
    "        total_steps = (DATASET_SIZE // batch) * epochs\n",
    "        eval_steps = max(1, min(total_steps // 5, 50))\n",
    "        logging_steps = max(1, eval_steps // 2)\n",
    "        save_steps = eval_steps\n",
    "        eval_strategy = \"steps\"\n",
    "        save_strategy = \"steps\"\n",
    "    else:\n",
    "        eval_strategy = \"epoch\"\n",
    "        save_strategy = \"epoch\"\n",
    "        logging_steps = 100\n",
    "        save_steps = None\n",
    "        eval_steps = None\n",
    "\n",
    "    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    use_fp16 = not use_bf16 and torch.cuda.is_available()\n",
    "    load_best = method_name in [\"lora\", \"full_ft\"]\n",
    "\n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"{OUTPUT_DIR}/results/{task_name}/{method_name}\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch,\n",
    "        per_device_eval_batch_size=batch * 2,\n",
    "        learning_rate=lr,\n",
    "        warmup_steps=1000 if DATASET_SIZE == 'full' else min(100, DATASET_SIZE // 10),\n",
    "        weight_decay=0.1,\n",
    "        eval_strategy=eval_strategy,\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy=save_strategy,\n",
    "        save_steps=save_steps,\n",
    "        load_best_model_at_end=load_best,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        save_total_limit=2,\n",
    "        logging_steps=logging_steps,\n",
    "        bf16=use_bf16,\n",
    "        fp16=use_fp16,\n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_drop_last=True,\n",
    "        report_to=\"none\",\n",
    "        predict_with_generate=True,\n",
    "        max_grad_norm=1.0,\n",
    "        gradient_accumulation_steps=4,\n",
    "        optim='adamw_torch',\n",
    "        gradient_checkpointing=False\n",
    "    )\n",
    "\n",
    "# MAIN TRAINING LOOP\n",
    "base_methods = [\"lora\", \"full_ft\"]\n",
    "methods_to_run = base_methods\n",
    "tasks = {\n",
    "    \"classification\": (tokenized_classification, compute_classification_metrics),\n",
    "    \"summarization\": (tokenized_summarization, compute_summarization_metrics)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "os.makedirs(f\"{OUTPUT_DIR}/results\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/plots\", exist_ok=True) \n",
    "\n",
    "for method_name in methods_to_run:\n",
    "    for task_name, (dataset, compute_metrics) in tasks.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EXPERIMENT: {method_name.upper()} on {task_name.upper()}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        try:\n",
    "            config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "            if config.num_heads != 8:\n",
    "                config.num_heads = 8\n",
    "            use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                config=config,\n",
    "                dtype=torch.bfloat16 if use_bf16 else torch.float32,\n",
    "                ignore_mismatched_sizes=True\n",
    "            )\n",
    "            model.to(device)\n",
    "\n",
    "            if method_name == \"full_ft\":\n",
    "                trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                total = sum(p.numel() for p in model.parameters())\n",
    "                print(f\"trainable params: {trainable:,} || all params: {total:,} || trainable%: 100.00\")\n",
    "            else:\n",
    "                d_model = model.config.d_model\n",
    "                peft_config = LoraConfig(\n",
    "                    r=32,\n",
    "                    lora_alpha=32,\n",
    "                    target_modules=[\"q\", \"v\"],\n",
    "                    lora_dropout=0.05,\n",
    "                    bias=\"none\",\n",
    "                    task_type=TaskType.SEQ_2_SEQ_LM\n",
    "                )\n",
    "                model = get_peft_model(model, peft_config)\n",
    "                model.print_trainable_parameters()\n",
    "\n",
    "            training_args = get_training_args(method_name, task_name)\n",
    "            data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=dataset[\"train\"],\n",
    "                eval_dataset=dataset[\"validation\"],\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_metrics,\n",
    "                tokenizer=tokenizer\n",
    "            )\n",
    "\n",
    "            print(\"Training...\")\n",
    "            train_result = trainer.train()\n",
    "\n",
    "            if not training_args.load_best_model_at_end and trainer.state.best_model_checkpoint:\n",
    "                print(f\"Loading best checkpoint manually: {trainer.state.best_model_checkpoint}\")\n",
    "                base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    MODEL_NAME,\n",
    "                    config=config,\n",
    "                    dtype=torch.bfloat16 if use_bf16 else torch.float32,\n",
    "                )\n",
    "                base_model.to(device)\n",
    "                model = PeftModel.from_pretrained(base_model, trainer.state.best_model_checkpoint) if method_name != \"full_ft\" else base_model\n",
    "                trainer.model = model\n",
    "                model.to(device)\n",
    "\n",
    "            print(\"Evaluating...\")\n",
    "            test_dataset = dataset.get(\"test\", dataset[\"validation\"])\n",
    "            gen_kwargs = {\n",
    "                \"max_length\": 5 if task_name == \"classification\" else 128,\n",
    "                \"num_beams\": 6,\n",
    "                \"early_stopping\": True,\n",
    "            }\n",
    "            training_args.generation_max_length = gen_kwargs[\"max_length\"]\n",
    "            training_args.generation_num_beams = gen_kwargs[\"num_beams\"]\n",
    "            test_metrics = trainer.evaluate(test_dataset)\n",
    "            predictions = trainer.predict(dataset[\"validation\"])\n",
    "            cleaned_predictions = np.where(predictions.predictions != -100, predictions.predictions, tokenizer.pad_token_id)\n",
    "            cleaned_predictions = np.clip(cleaned_predictions, 0, tokenizer.vocab_size - 1)\n",
    "            logger.info(f\"Sample generations: {tokenizer.batch_decode(cleaned_predictions[:5], skip_special_tokens=True)}\")\n",
    "            exp_name = f\"{method_name}_{task_name}\"\n",
    "            trainable = model.num_parameters(only_trainable=True) if hasattr(model, 'num_parameters') else sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            total = model.num_parameters() if hasattr(model, 'num_parameters') else sum(p.numel() for p in model.parameters())\n",
    "            \n",
    "            results[exp_name] = {\n",
    "                \"train_metrics\": train_result.metrics,\n",
    "                \"test_metrics\": test_metrics,\n",
    "                \"trainable_params\": trainable,\n",
    "                \"total_params\": total,\n",
    "                \"log_history\": trainer.state.log_history\n",
    "            }\n",
    "            \n",
    "            save_path = f\"{OUTPUT_DIR}/models/{task_name}/{method_name}\"\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            trainer.save_model(save_path)\n",
    "            print(f\"Completed and saved to {save_path}\\n\")\n",
    "            del model, trainer\n",
    "            safe_cleanup()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"ERROR in {method_name}_{task_name}: {e}\")\n",
    "            import traceback\n",
    "            logger.error(traceback.format_exc())\n",
    "            safe_cleanup()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL EXPERIMENTS COMPLETED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# RESULTS, PLOTS, REPORT, INSIGHTS (same as before, updated for both methods)\n",
    "if results:\n",
    "    print(\"\\nRESULTS SUMMARY:\")\n",
    "    print(\"=\"*60)\n",
    "    for exp_name, exp_data in results.items():\n",
    "        method_task_split = exp_name.split('_', 1)\n",
    "        method = method_task_split[0]\n",
    "        task = method_task_split[1] if len(method_task_split) > 1 else 'unknown'\n",
    "        metrics = exp_data[\"test_metrics\"]\n",
    "        pct = 100 * exp_data[\"trainable_params\"] / exp_data[\"total_params\"]\n",
    "        print(f\"\\n{method.upper()} - {task.capitalize()}:\")\n",
    "        print(f\" Trainable: {pct:.2f}%\")\n",
    "        if task == \"classification\":\n",
    "            print(f\" Accuracy: {metrics.get('eval_accuracy', 0):.4f}\")\n",
    "            print(f\" F1: {metrics.get('eval_f1', 0):.4f}\")\n",
    "        else:\n",
    "            print(f\" ROUGE-1: {metrics.get('eval_rouge1', 0):.4f}\")\n",
    "            print(f\" ROUGE-L: {metrics.get('eval_rougeL', 0):.4f}\")\n",
    "\n",
    "    print(\"\\nGenerating learning curves...\")\n",
    "    plot_paths = {}\n",
    "    plot_save_dir = f\"{OUTPUT_DIR}/plots\"\n",
    "    for exp_name, exp_data in results.items():\n",
    "        task_name = exp_name.split(\"_\", 1)[1]\n",
    "        plot_path = plot_learning_curves(exp_data[\"log_history\"], exp_name, task_name, save_dir=plot_save_dir)\n",
    "        plot_paths[exp_name] = plot_path\n",
    "\n",
    "    results_df = []\n",
    "    for exp_name, exp_data in results.items():\n",
    "        method, task = exp_name.split(\"_\", 1)\n",
    "        results_df.append({\n",
    "            \"Method\": method.upper(),\n",
    "            \"Task\": task.capitalize(),\n",
    "            \"Trainable %\": 100 * exp_data[\"trainable_params\"] / exp_data[\"total_params\"],\n",
    "            **{k: v for k, v in exp_data[\"test_metrics\"].items() if isinstance(v, (int, float))}\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results_df)\n",
    "    cols = [\"Method\", \"Task\", \"Trainable %\"]\n",
    "    metric_cols = [c for c in df.columns if c.startswith(\"eval_\")]\n",
    "    cols.extend(sorted(metric_cols))\n",
    "    df = df[cols]\n",
    "    df.to_csv(f\"{OUTPUT_DIR}/comparison_results.csv\", index=False)\n",
    "    print(f\"\\nResults saved to '{OUTPUT_DIR}/comparison_results.csv'\")\n",
    "\n",
    "    report_path = f\"{OUTPUT_DIR}/final_report.md\"\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(\"# LoRA vs Full Fine-Tuning Comparison - flan-t5-small\\n\\n\")\n",
    "        f.write(\"## Configuration\\n\")\n",
    "        f.write(f\"- Model: {MODEL_NAME}\\n\")\n",
    "        f.write(f\"- Dataset Size: {DATASET_SIZE}\\n\")\n",
    "        f.write(\"- Methods: LoRA, Full Fine-Tuning\\n\\n\")\n",
    "        f.write(\"## Summary Table\\n\\n\")\n",
    "        f.write(df.to_markdown(index=False))\n",
    "        f.write(\"\\n\\n## Learning Curves\\n\")\n",
    "        for exp_name, plot_path in plot_paths.items():\n",
    "            relative_plot_path = os.path.relpath(plot_path, start=os.path.dirname(report_path))\n",
    "            f.write(f\"- [{exp_name}]({relative_plot_path})\\n\")\n",
    "    \n",
    "    print(f\"Report saved to '{report_path}'\")\n",
    "\n",
    "    print(\"\\nOUTCOME INSIGHTS:\")\n",
    "    for task in tasks.keys():\n",
    "        task_exps = {k: v for k, v in results.items() if k.endswith(task)}\n",
    "        if task_exps:\n",
    "            min_trainable_method = min(task_exps, key=lambda k: 100 * task_exps[k][\"trainable_params\"] / task_exps[k][\"total_params\"])\n",
    "            min_pct = 100 * task_exps[min_trainable_method][\"trainable_params\"] / task_exps[min_trainable_method][\"total_params\"]\n",
    "            print(f\"- For {task.capitalize()}, {min_trainable_method.split('_')[0].upper()} has the lowest trainable params ({min_pct:.2f}%).\")\n",
    "            \n",
    "            key_metric = 'eval_accuracy' if task == 'classification' else 'eval_rougeL'\n",
    "            best_method = max(task_exps, key=lambda k: task_exps[k][\"test_metrics\"].get(key_metric, 0))\n",
    "            best_score = task_exps[best_method][\"test_metrics\"].get(key_metric, 0)\n",
    "            print(f\"- {best_method.split('_')[0].upper()} achieves the highest {key_metric.replace('eval_', '').upper()} score ({best_score:.4f}) on {task.capitalize()}.\")\n",
    "    \n",
    "    print(f\"View plots in {OUTPUT_DIR}/plots/ for all 4 learning curves.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo results were generated.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUCCESS - LoRA and Full Fine-Tuning completed!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
