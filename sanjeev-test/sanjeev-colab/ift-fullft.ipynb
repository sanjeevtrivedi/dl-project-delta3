{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b24021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96abe062",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PEFT Comparison: LoRA, Prefix, Prompt, Full FT\n",
    "Model: t5-small\n",
    "Tasks: SST-2 (Classification), SAMSum (Summarization)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import spacy\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    PrefixTuningConfig,\n",
    "    PromptTuningConfig,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "import logging\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ========================================\n",
    "# CONFIGURATION\n",
    "# ========================================\n",
    "MODEL_NAME = \"t5-small\"\n",
    "SUMMARIZATION_DATASET = \"knkarthick/samsum\"\n",
    "BENCHAMARK_GLUE = \"glue\"\n",
    "GLUE_DATASET_TASK_SC = \"sst2\"\n",
    "DATASET_SIZE = 400  # or 'full'\n",
    "RUN_ABLATIONS = False\n",
    "RANDOM_SEED = 42\n",
    "NUM_VIRTUAL_TOKENS = 20\n",
    "MAX_POS = 512\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() \n",
    "    else \"mps\" if torch.backends.mps.is_available() \n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# # Load spaCy\n",
    "# try:\n",
    "#     nlp = spacy.load(\"en_core_web_sm\")\n",
    "# except:\n",
    "#     os.system(\"python -m spacy download en_core_web_sm\")\n",
    "#     nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ========================================\n",
    "# UTILITIES\n",
    "# ========================================\n",
    "def limit_dataset_size(dataset, size):\n",
    "    if size == 'full':\n",
    "        return dataset\n",
    "    return dataset.select(range(min(size, len(dataset))))\n",
    "\n",
    "def setup_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "def safe_cleanup():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    elif device.type == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "# ========================================\n",
    "# DATA LOADING & PREPROCESSING\n",
    "# ========================================\n",
    "print(\"Loading datasets...\")\n",
    "classification_dataset = load_dataset(BENCHAMARK_GLUE, GLUE_DATASET_TASK_SC)\n",
    "summarization_dataset = load_dataset(SUMMARIZATION_DATASET)\n",
    "\n",
    "tokenizer = setup_tokenizer(MODEL_NAME)\n",
    "\n",
    "# Limit size BEFORE preprocessing\n",
    "if DATASET_SIZE != 'full':\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        size = DATASET_SIZE if split == 'train' else DATASET_SIZE // 4\n",
    "        if split in classification_dataset:\n",
    "            classification_dataset[split] = limit_dataset_size(classification_dataset[split], size)\n",
    "        if split in summarization_dataset:\n",
    "            summarization_dataset[split] = limit_dataset_size(summarization_dataset[split], size)\n",
    "\n",
    "print(\"Datasets loaded and size-limited.\\n\")\n",
    "\n",
    "# Preprocessing\n",
    "print(\"Preprocessing datasets...\")\n",
    "\n",
    "def preprocess_classification(examples):\n",
    "    inputs = [f\"sentiment: {s}\" for s in examples[\"sentence\"]]\n",
    "    labels = [\"positive\" if l == 1 else \"negative\" for l in examples[\"label\"]]\n",
    "    model_inputs = tokenizer(inputs, truncation=True, max_length=MAX_POS)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        label_enc = tokenizer(labels, truncation=True, max_length=8)\n",
    "    model_inputs[\"labels\"] = label_enc[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_classification = classification_dataset.map(\n",
    "    preprocess_classification,\n",
    "    batched=True,\n",
    "    remove_columns=[\"sentence\", \"label\", \"idx\"]\n",
    ")\n",
    "\n",
    "def preprocess_summarization(examples):\n",
    "    inputs = [f\"summarize: {d}\" for d in examples[\"dialogue\"]]\n",
    "    model_inputs = tokenizer(inputs, truncation=True, max_length=MAX_POS)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], truncation=True, max_length=128)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_summarization = summarization_dataset.map(\n",
    "    preprocess_summarization,\n",
    "    batched=True,\n",
    "    remove_columns=[\"dialogue\", \"summary\", \"id\"]\n",
    ")\n",
    "\n",
    "print(\"Preprocessing complete.\\n\")\n",
    "\n",
    "# ========================================\n",
    "# METRICS (FIXED: safe_decode)\n",
    "# ========================================\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def safe_decode(token_ids, tokenizer):\n",
    "    \"\"\"Filter out invalid token IDs before decoding\"\"\"\n",
    "    if token_ids is None:\n",
    "        return []\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    filtered = []\n",
    "    for seq in token_ids:\n",
    "        seq = [t for t in seq if 0 <= t < vocab_size]\n",
    "        filtered.append(seq)\n",
    "    return tokenizer.batch_decode(filtered, skip_special_tokens=True)\n",
    "\n",
    "def compute_classification_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    decoded_preds = safe_decode(preds, tokenizer)\n",
    "    decoded_labels = safe_decode(labels, tokenizer)\n",
    "    y_pred = [1 if p.strip() == \"positive\" else 0 for p in decoded_preds]\n",
    "    y_true = [1 if l.strip() == \"positive\" else 0 for l in decoded_labels]\n",
    "    acc = accuracy_metric.compute(predictions=y_pred, references=y_true)\n",
    "    f1 = f1_metric.compute(predictions=y_pred, references=y_true)\n",
    "    return {\"accuracy\": acc[\"accuracy\"], \"f1\": f1[\"f1\"]}\n",
    "\n",
    "def compute_summarization_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    decoded_preds = safe_decode(preds, tokenizer)\n",
    "    decoded_labels = safe_decode(labels, tokenizer)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {\n",
    "        \"rouge1\": result[\"rouge1\"],\n",
    "        \"rouge2\": result[\"rouge2\"],\n",
    "        \"rougeL\": result[\"rougeL\"],\n",
    "        \"rougeLsum\": result[\"rougeLsum\"]\n",
    "    }\n",
    "\n",
    "# ========================================\n",
    "# TRAINING ARGS\n",
    "# ========================================\n",
    "def get_training_args(method_name, task_name):\n",
    "    is_peft = method_name in [\"lora\", \"prefix\", \"prompt\"] or \"_ablated_\" in method_name\n",
    "    lr = 1e-3 if is_peft else 5e-5\n",
    "    \n",
    "    if DATASET_SIZE == 'full':\n",
    "        epochs, batch, eval_steps = 3, 8, 500\n",
    "    elif DATASET_SIZE <= 500:\n",
    "        epochs, batch, eval_steps = 5, 4, 50\n",
    "    else:\n",
    "        epochs, batch, eval_steps = 3, 8, 100\n",
    "\n",
    "    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    use_fp16 = False\n",
    "    load_best = method_name == \"full_ft\" or \"lora\" in method_name\n",
    "    \n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"./results/{task_name}/{method_name}\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch,\n",
    "        per_device_eval_batch_size=batch * 2,\n",
    "        learning_rate=lr,\n",
    "        warmup_steps=min(100, DATASET_SIZE // 10) if DATASET_SIZE != 'full' else 500,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"steps\" if DATASET_SIZE != 'full' else \"epoch\",\n",
    "        eval_steps=eval_steps if DATASET_SIZE != 'full' else None,\n",
    "        save_strategy=\"steps\" if DATASET_SIZE != 'full' else \"epoch\",\n",
    "        save_steps=eval_steps if DATASET_SIZE != 'full' else None,\n",
    "        load_best_model_at_end=load_best,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        save_total_limit=2,\n",
    "        logging_steps=20 if DATASET_SIZE != 'full' else 100,\n",
    "        bf16=use_bf16,\n",
    "        fp16=use_fp16,\n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_drop_last=True,\n",
    "        report_to=\"none\",\n",
    "        predict_with_generate=True,\n",
    "        max_grad_norm=1.0,\n",
    "        gradient_checkpointing=False,\n",
    "    )\n",
    "\n",
    "# ========================================\n",
    "# PLOTTING & ANALYSIS\n",
    "# ========================================\n",
    "def plot_learning_curves(log_history, exp_name, task_name, save_dir=\"./plots\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    steps = [log['step'] for log in log_history if 'step' in log]\n",
    "    train_losses = [log['train_loss'] for log in log_history if 'train_loss' in log]\n",
    "    eval_losses = [log['eval_loss'] for log in log_history if 'eval_loss' in log]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    axes[0].plot(steps[:len(train_losses)], train_losses, label='Train Loss', marker='o')\n",
    "    if eval_losses:\n",
    "        axes[0].plot(steps[:len(eval_losses)], eval_losses, label='Eval Loss', marker='s')\n",
    "    axes[0].set_xlabel('Step')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title(f'{exp_name} - Loss')\n",
    "    axes[0].legend()\n",
    "\n",
    "    metric_key = 'eval_accuracy' if task_name == \"classification\" else 'eval_rougeL'\n",
    "    metric_vals = [log.get(metric_key) for log in log_history if metric_key in log]\n",
    "    if metric_vals:\n",
    "        axes[1].plot(steps[:len(metric_vals)], metric_vals, label=metric_key.split('_')[1].upper(), color='green', marker='o')\n",
    "        axes[1].set_ylabel(metric_key.split('_')[1].upper())\n",
    "    axes[1].set_xlabel('Step')\n",
    "    axes[1].set_title(f'{exp_name} - Metric')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    path = f\"{save_dir}/{exp_name}_curves.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "    print(f\"Learning curves → {path}\")\n",
    "    return path\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, exp_name, save_dir=\"./plots\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix - {exp_name}')\n",
    "    path = f\"{save_dir}/{exp_name}_cm.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix → {path}\")\n",
    "    return path\n",
    "\n",
    "def save_sample_outputs(trainer, dataset, exp_name, n=5, save_dir=\"./samples\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    samples = dataset[\"test\"].select(range(min(n, len(dataset[\"test\"]))))\n",
    "    \n",
    "    preds = trainer.predict(samples)\n",
    "    decoded_preds = safe_decode(preds.predictions, tokenizer)\n",
    "    decoded_labels = safe_decode(preds.label_ids, tokenizer)\n",
    "    \n",
    "    input_ids = samples[\"input_ids\"]\n",
    "    inputs = [tokenizer.decode(ids, skip_special_tokens=True).replace(\"summarize: \", \"\") for ids in input_ids]\n",
    "\n",
    "    # SAFE INDEXING\n",
    "    num_samples = min(len(inputs), len(decoded_preds), len(decoded_labels))\n",
    "    \n",
    "    path = f\"{save_dir}/{exp_name}_samples.txt\"\n",
    "    with open(path, \"w\") as f:\n",
    "        for i in range(num_samples):\n",
    "            f.write(f\"INPUT:\\n{inputs[i]}\\n\\n\")\n",
    "            f.write(f\"PREDICTED:\\n{decoded_preds[i]}\\n\\n\")\n",
    "            f.write(f\"TRUE:\\n{decoded_labels[i]}\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "    print(f\"Sample outputs → {path}\")\n",
    "    return path\n",
    "\n",
    "def detect_hallucinations(preds, refs):\n",
    "    pred_ents = []\n",
    "    ref_ents = []\n",
    "    for p, r in zip(preds, refs):\n",
    "        pred_doc = nlp(p)\n",
    "        ref_doc = nlp(r)\n",
    "        pred_ents.append({ent.text.lower() for ent in pred_doc.ents})\n",
    "        ref_ents.append({ent.text.lower() for ent in ref_doc.ents})\n",
    "    \n",
    "    hallucinations = []\n",
    "    for p_set, r_set in zip(pred_ents, ref_ents):\n",
    "        extra = p_set - r_set\n",
    "        hallucinations.append(len(extra))\n",
    "    return hallucinations\n",
    "\n",
    "def plot_length_analysis(pred_lens, true_lens, exp_name, save_dir=\"./plots\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    df = pd.DataFrame({\"Predicted Length\": pred_lens, \"True Length\": true_lens})\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(df, kde=True, bins=20, alpha=0.7)\n",
    "    plt.title(f'Summary Length Distribution - {exp_name}')\n",
    "    plt.xlabel('Length (tokens)')\n",
    "    plt.legend(['Predicted', 'True'])\n",
    "    \n",
    "    path = f\"{save_dir}/{exp_name}_length.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "    print(f\"Length plot → {path}\")\n",
    "    return path\n",
    "\n",
    "# ========================================\n",
    "# MAIN LOOP\n",
    "# ========================================\n",
    "base_methods = [\"lora\", \"prefix\", \"prompt\", \"full_ft\"]\n",
    "ablation_methods = [\"lora_ablated_alpha0\", \"prefix_ablated_no_proj\", \"prompt_ablated_short\"]\n",
    "methods_to_run = base_methods + (ablation_methods if RUN_ABLATIONS else [])\n",
    "\n",
    "tasks = {\n",
    "    \"classification\": (tokenized_classification, compute_classification_metrics),\n",
    "    \"summarization\": (tokenized_summarization, compute_summarization_metrics)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "os.makedirs(\"./plots\", exist_ok=True)\n",
    "os.makedirs(\"./samples\", exist_ok=True)\n",
    "os.makedirs(\"./hallucinations\", exist_ok=True)\n",
    "\n",
    "for method_name in methods_to_run:\n",
    "    for task_name, (dataset, compute_metrics) in tasks.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"RUNNING: {method_name.upper()} → {task_name.upper()}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        try:\n",
    "            config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "            use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                config=config,\n",
    "                dtype=torch.bfloat16 if use_bf16 else torch.float32\n",
    "            ).to(device)\n",
    "\n",
    "            if method_name != \"full_ft\":\n",
    "                d_model = model.config.d_model\n",
    "                num_heads = model.config.num_heads\n",
    "                total_layers = model.config.num_layers + model.config.num_decoder_layers\n",
    "                peft_configs = {\n",
    "                    \"lora\": LoraConfig(r=16, lora_alpha=32, target_modules=[\"q\", \"v\"], lora_dropout=0.05, bias=\"none\", task_type=TaskType.SEQ_2_SEQ_LM),\n",
    "                    \"lora_ablated_alpha0\": LoraConfig(r=16, lora_alpha=0, target_modules=[\"q\", \"v\"], lora_dropout=0.05, bias=\"none\", task_type=TaskType.SEQ_2_SEQ_LM),\n",
    "                    \"prefix\": PrefixTuningConfig(task_type=TaskType.SEQ_2_SEQ_LM, num_virtual_tokens=NUM_VIRTUAL_TOKENS, token_dim=d_model, num_attention_heads=num_heads, num_layers=total_layers, num_transformer_submodules=2, encoder_hidden_size=d_model, prefix_projection=True),\n",
    "                    \"prefix_ablated_no_proj\": PrefixTuningConfig(task_type=TaskType.SEQ_2_SEQ_LM, num_virtual_tokens=NUM_VIRTUAL_TOKENS, token_dim=d_model, num_attention_heads=num_heads, num_layers=total_layers, num_transformer_submodules=2, encoder_hidden_size=d_model, prefix_projection=False),\n",
    "                    \"prompt\": PromptTuningConfig(num_virtual_tokens=NUM_VIRTUAL_TOKENS, task_type=TaskType.SEQ_2_SEQ_LM),\n",
    "                    \"prompt_ablated_short\": PromptTuningConfig(num_virtual_tokens=NUM_VIRTUAL_TOKENS // 2, task_type=TaskType.SEQ_2_SEQ_LM),\n",
    "                }\n",
    "                model = get_peft_model(model, peft_configs[method_name])\n",
    "                model.print_trainable_parameters()\n",
    "                model.train()\n",
    "                model.config.use_cache = False\n",
    "            else:\n",
    "                trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                total = sum(p.numel() for p in model.parameters())\n",
    "                print(f\"trainable: {trainable:,} || total: {total:,} || %: 100.00\")\n",
    "\n",
    "            args = get_training_args(method_name, task_name)\n",
    "            collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model, args=args, train_dataset=dataset[\"train\"],\n",
    "                eval_dataset=dataset[\"validation\"], data_collator=collator,\n",
    "                compute_metrics=compute_metrics, tokenizer=tokenizer\n",
    "            )\n",
    "\n",
    "            print(\"Training...\")\n",
    "            trainer.train()\n",
    "\n",
    "            print(\"Evaluating...\")\n",
    "            test_ds = dataset.get(\"test\", dataset[\"validation\"])\n",
    "            test_metrics = trainer.evaluate(test_ds)\n",
    "\n",
    "            # === ANALYSIS ===\n",
    "            exp_name = f\"{method_name}_{task_name}\"\n",
    "            preds = trainer.predict(test_ds)\n",
    "            decoded_preds = safe_decode(preds.predictions, tokenizer)\n",
    "            decoded_labels = safe_decode(preds.label_ids, tokenizer)\n",
    "\n",
    "            # 1. Confusion Matrix\n",
    "            cm_path = None\n",
    "            if task_name == \"classification\":\n",
    "                y_pred = [1 if p.strip() == \"positive\" else 0 for p in decoded_preds]\n",
    "                y_true = [1 if l.strip() == \"positive\" else 0 for l in decoded_labels]\n",
    "                cm_path = plot_confusion_matrix(y_true, y_pred, exp_name)\n",
    "\n",
    "            # 2. Sample Outputs\n",
    "            sample_path = None\n",
    "            if task_name == \"summarization\":\n",
    "                sample_path = save_sample_outputs(trainer, dataset, exp_name, n=5)\n",
    "\n",
    "            # 3. ROUGE\n",
    "            rouge1 = test_metrics.get(\"eval_rouge1\", 0)\n",
    "            rouge2 = test_metrics.get(\"eval_rouge2\", 0)\n",
    "            rougeL = test_metrics.get(\"eval_rougeL\", 0)\n",
    "\n",
    "            # 4. Hallucination\n",
    "            hall_path = None\n",
    "            if task_name == \"summarization\":\n",
    "                halls = detect_hallucinations(decoded_preds, decoded_labels)\n",
    "                avg_hall = np.mean(halls)\n",
    "                with open(f\"./hallucinations/{exp_name}_hallucinations.json\", \"w\") as f:\n",
    "                    json.dump({\"avg_hallucinated_entities\": avg_hall, \"per_sample\": halls}, f, indent=2)\n",
    "                hall_path = f\"./hallucinations/{exp_name}_hallucinations.json\"\n",
    "                print(f\"Hallucination report → {hall_path} (Avg: {avg_hall:.2f})\")\n",
    "\n",
    "            # 5. Length\n",
    "            len_path = None\n",
    "            if task_name == \"summarization\":\n",
    "                pred_lens = [len(tokenizer.encode(p, add_special_tokens=False)) for p in decoded_preds]\n",
    "                true_lens = [len(tokenizer.encode(t, add_special_tokens=False)) for t in decoded_labels]\n",
    "                len_path = plot_length_analysis(pred_lens, true_lens, exp_name)\n",
    "\n",
    "            # Save results\n",
    "            trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            total = sum(p.numel() for p in model.parameters())\n",
    "            results[exp_name] = {\n",
    "                \"test_metrics\": test_metrics,\n",
    "                \"trainable_params\": trainable,\n",
    "                \"total_params\": total,\n",
    "                \"log_history\": trainer.state.log_history,\n",
    "                \"cm_plot\": cm_path,\n",
    "                \"sample_path\": sample_path,\n",
    "                \"hallucination_path\": hall_path,\n",
    "                \"length_plot\": len_path\n",
    "            }\n",
    "\n",
    "            save_path = f\"./models/{task_name}/{method_name}\"\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            trainer.save_model(save_path)\n",
    "            print(f\"Model saved → {save_path}\\n\")\n",
    "\n",
    "            del model, trainer\n",
    "            safe_cleanup()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"ERROR in {method_name}_{task_name}: {e}\")\n",
    "            import traceback\n",
    "            logger.error(traceback.format_exc())\n",
    "            safe_cleanup()\n",
    "\n",
    "# ========================================\n",
    "# FINAL REPORT\n",
    "# ========================================\n",
    "if results:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RESULTS & ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    rows = []\n",
    "    for exp, data in results.items():\n",
    "        m, t = exp.split(\"_\", 1)\n",
    "        row = {\"Method\": m.upper(), \"Task\": t.capitalize(), \"Trainable %\": round(100 * data[\"trainable_params\"] / data[\"total_params\"], 2)}\n",
    "        row.update({k.replace(\"eval_\", \"\"): v for k, v in data[\"test_metrics\"].items() if \"eval_\" in k})\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(\"peft_results_final.csv\", index=False)\n",
    "\n",
    "    with open(\"FINAL_REPORT.md\", \"w\") as f:\n",
    "        f.write(\"# PEFT Comparison - Final Report\\n\\n\")\n",
    "        f.write(f\"**Model**: {MODEL_NAME} | **Size**: {DATASET_SIZE}\\n\\n\")\n",
    "        f.write(\"## Metrics Table\\n\\n\")\n",
    "        f.write(df.to_markdown(index=False))\n",
    "        f.write(\"\\n\\n## Outputs\\n\")\n",
    "        for exp, data in results.items():\n",
    "            f.write(f\"\\n### {exp.upper()}\\n\")\n",
    "            if data[\"cm_plot\"]: f.write(f\"- [Confusion Matrix]({data['cm_plot']})\\n\")\n",
    "            if data[\"sample_path\"]: f.write(f\"- [Sample Outputs]({data['sample_path']})\\n\")\n",
    "            if data[\"hallucination_path\"]: f.write(f\"- [Hallucination Report]({data['hallucination_path']})\\n\")\n",
    "            if data[\"length_plot\"]: f.write(f\"- [Length Analysis]({data['length_plot']})\\n\")\n",
    "\n",
    "    print(\"Report → FINAL_REPORT.md\")\n",
    "    print(\"CSV → peft_results_final.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUCCESS: All experiments completed!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
