{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f76dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 1. Environment and dependencies\n",
    "# ================================\n",
    "!pip install --no-deps evaluate protobuf<4.0\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    PeftModel,\n",
    ")\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ================\n",
    "# 2. Device setup\n",
    "# ================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =================\n",
    "# 3. Configuration\n",
    "# =================\n",
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "\n",
    "# NOTE: No external HF datasets are used now.\n",
    "# BENCHMARK_GLUE = \"glue\"\n",
    "# GLUE_DATASET_TASK_SC = \"sst2\"\n",
    "# SUMMARIZATION_DATASET = \"knkarthick/samsum\"\n",
    "\n",
    "PROGRAM_NAME = \"ift-lora\"\n",
    "DATASET_SIZE = \"full\"  # still used as a logical flag, but we define tiny datasets below\n",
    "RUN_ABLATIONS = False\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "NUM_VIRTUAL_TOKENS = 50\n",
    "MAX_POS = 512\n",
    "\n",
    "OUTPUT_DIR = f\"/kaggle/working/outputs_{PROGRAM_NAME}\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"LoRA COMPARISON - {MODEL_NAME.split('/')[-1]}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset size: {DATASET_SIZE}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(\"Methods: LoRA\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# ==========================\n",
    "# 4. Helper / utility funcs\n",
    "# ==========================\n",
    "def limit_dataset_size(dataset, size):\n",
    "    if size == \"full\":\n",
    "        return dataset\n",
    "    if isinstance(size, int) and size > 0:\n",
    "        return dataset.select(range(min(size, len(dataset))))\n",
    "    raise ValueError(f\"Invalid size {size}\")\n",
    "\n",
    "def setup_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "def safe_cleanup():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def plot_learning_curves(log_history, exp_name, task_name, save_dir=\"plots\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    steps = [log[\"step\"] for log in log_history if \"step\" in log and \"eval_loss\" not in log]\n",
    "    eval_steps = [log[\"step\"] for log in log_history if \"eval_loss\" in log]\n",
    "    train_losses = [log[\"loss\"] for log in log_history if \"loss\" in log]\n",
    "    eval_losses = [log[\"eval_loss\"] for log in log_history if \"eval_loss\" in log]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Loss curve\n",
    "    train_steps_for_loss = [log[\"step\"] for log in log_history if \"loss\" in log]\n",
    "    axes[0].plot(train_steps_for_loss, train_losses, label=\"Train Loss\", marker=\"o\", alpha=0.7)\n",
    "    if eval_losses:\n",
    "        axes[0].plot(eval_steps, eval_losses, label=\"Eval Loss\", marker=\"s\")\n",
    "    axes[0].set_xlabel(\"Step\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].set_title(f\"{exp_name} - Loss Curve\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Task-specific metric\n",
    "    if task_name == \"classification\":\n",
    "        eval_accs = [log[\"eval_accuracy\"] for log in log_history if \"eval_accuracy\" in log]\n",
    "        if eval_accs:\n",
    "            axes[1].plot(eval_steps, eval_accs, label=\"Eval Accuracy\", marker=\"o\", color=\"green\")\n",
    "        axes[1].set_ylabel(\"Accuracy\")\n",
    "    else:\n",
    "        eval_rougels = [log[\"eval_rougeL\"] for log in log_history if \"eval_rougeL\" in log]\n",
    "        if eval_rougels:\n",
    "            axes[1].plot(eval_steps, eval_rougels, label=\"Eval ROUGE-L\", marker=\"o\", color=\"green\")\n",
    "        axes[1].set_ylabel(\"ROUGE-L\")\n",
    "\n",
    "    axes[1].set_xlabel(\"Step\")\n",
    "    axes[1].set_title(f\"{exp_name} - {task_name.capitalize()} Metric\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(save_dir, f\"{exp_name}_curves.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Learning curves saved to {plot_path}\")\n",
    "    return plot_path\n",
    "\n",
    "def plot_ablation_comparisons(results, task_name, save_dir=\"plots\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    methods = list(results.keys())\n",
    "    baselines = [m for m in methods if \"ablated\" not in m]\n",
    "    ablations = [m for m in methods if \"ablated\" in m]\n",
    "\n",
    "    if not ablations:\n",
    "        return None\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    trainable_pcts = [\n",
    "        100 * results[m][\"trainable_params\"] / results[m][\"total_params\"]\n",
    "        for m in methods\n",
    "    ]\n",
    "    sns.barplot(x=methods, y=trainable_pcts, ax=axes[0])\n",
    "    axes[0].set_ylabel(\"Trainable %\")\n",
    "    axes[0].set_title(f\"Trainable Params Comparison - {task_name.capitalize()}\")\n",
    "    axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    if task_name == \"classification\":\n",
    "        metrics = [results[m][\"test_metrics\"].get(\"eval_accuracy\", 0) for m in methods]\n",
    "        metric_label = \"Accuracy\"\n",
    "    else:\n",
    "        metrics = [results[m][\"test_metrics\"].get(\"eval_rougeL\", 0) for m in methods]\n",
    "        metric_label = \"ROUGE-L\"\n",
    "\n",
    "    sns.barplot(x=methods, y=metrics, ax=axes[1])\n",
    "    axes[1].set_ylabel(metric_label)\n",
    "    axes[1].set_title(f\"Performance Comparison - {task_name.capitalize()}\")\n",
    "    axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(save_dir, f\"ablation_comparison_{task_name}.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Ablation comparison plot saved to {plot_path}\")\n",
    "    return plot_path\n",
    "\n",
    "# ===========================\n",
    "# 5. Offline toy datasets\n",
    "# ===========================\n",
    "\n",
    "print(\"Loading offline toy datasets (no HF Hub download)...\")\n",
    "\n",
    "# Small SST-2-like classification dataset\n",
    "sst2_train_examples = {\n",
    "    \"sentence\": [\n",
    "        \"I love this movie, it is fantastic!\",\n",
    "        \"This film was terrible and boring.\",\n",
    "        \"What a great experience, highly recommended.\",\n",
    "        \"I hated every minute of this.\",\n",
    "        \"The plot was interesting and engaging.\",\n",
    "        \"The acting was awful and the script was bad.\",\n",
    "    ],\n",
    "    \"label\": [1, 0, 1, 0, 1, 0],\n",
    "    \"idx\": list(range(6)),\n",
    "}\n",
    "\n",
    "sst2_val_examples = {\n",
    "    \"sentence\": [\n",
    "        \"Absolutely wonderful!\",\n",
    "        \"Not good at all.\",\n",
    "    ],\n",
    "    \"label\": [1, 0],\n",
    "    \"idx\": [100, 101],\n",
    "}\n",
    "\n",
    "sst2_test_examples = {\n",
    "    \"sentence\": [\n",
    "        \"It was okay, not the best.\",\n",
    "        \"Really enjoyed it.\",\n",
    "    ],\n",
    "    \"label\": [0, 1],  # ground truth for evaluation\n",
    "    \"idx\": [200, 201],\n",
    "}\n",
    "\n",
    "classification_dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_dict(sst2_train_examples),\n",
    "        \"validation\": Dataset.from_dict(sst2_val_examples),\n",
    "        \"test\": Dataset.from_dict(sst2_test_examples),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Small SAMSum-like summarization dataset\n",
    "samsum_train_examples = {\n",
    "    \"id\": [\"1\", \"2\"],\n",
    "    \"dialogue\": [\n",
    "        \"A: I baked cookies.\\nB: Really?\\nA: Yes, do you want some?\\nB: Sure!\",\n",
    "        \"A: Who are you voting for in this election?\\nB: The liberals as always.\\nA: Me too!\",\n",
    "    ],\n",
    "    \"summary\": [\n",
    "        \"A baked cookies and B wants some.\",\n",
    "        \"A and B are voting for the liberals in this election.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "samsum_val_examples = {\n",
    "    \"id\": [\"3\"],\n",
    "    \"dialogue\": [\n",
    "        \"A: Hi, what's up?\\nB: I'm in a bad mood.\\nA: Why?\\nB: I procrastinated.\",\n",
    "    ],\n",
    "    \"summary\": [\n",
    "        \"B is in a bad mood because of procrastination.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "samsum_test_examples = {\n",
    "    \"id\": [\"4\"],\n",
    "    \"dialogue\": [\n",
    "        \"A: When and where are we meeting?\\nB: I thought you were busy.\\nA: I quit my job.\",\n",
    "    ],\n",
    "    \"summary\": [\n",
    "        \"A quit job and wants to meet B.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "summarization_dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_dict(samsum_train_examples),\n",
    "        \"validation\": Dataset.from_dict(samsum_val_examples),\n",
    "        \"test\": Dataset.from_dict(samsum_test_examples),\n",
    "    }\n",
    ")\n",
    "\n",
    "tokenizer = setup_tokenizer(MODEL_NAME)\n",
    "\n",
    "print(\"Datasets loaded (offline toy versions).\")\n",
    "\n",
    "# =========================\n",
    "# 6. Preprocessing functions\n",
    "# =========================\n",
    "def preprocess_classification(examples):\n",
    "    inputs = [f\"Classify sentiment: {text}\" for text in examples[\"sentence\"]]\n",
    "    max_input_len = MAX_POS - NUM_VIRTUAL_TOKENS\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_len,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    labels_text = [\"negative\" if label == 0 else \"positive\" for label in examples[\"label\"]]\n",
    "    labels = tokenizer(\n",
    "        text_target=labels_text,\n",
    "        max_length=10,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_summarization(examples):\n",
    "    inputs = [f\"Summarize the following conversation: {dialogue}\" for dialogue in examples[\"dialogue\"]]\n",
    "    max_input_len = MAX_POS - NUM_VIRTUAL_TOKENS\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_len,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    max_label_len = 128 - NUM_VIRTUAL_TOKENS\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"summary\"],\n",
    "        max_length=max_label_len,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )[\"input_ids\"]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "print(\"\\nPreprocessing...\")\n",
    "tokenized_classification = classification_dataset.map(\n",
    "    preprocess_classification, batched=True, remove_columns=classification_dataset[\"train\"].column_names\n",
    ")\n",
    "tokenized_summarization = summarization_dataset.map(\n",
    "    preprocess_summarization, batched=True, remove_columns=summarization_dataset[\"train\"].column_names\n",
    ")\n",
    "print(\"Preprocessing complete.\\n\")\n",
    "\n",
    "# =========================\n",
    "# 7. Decoding helper\n",
    "# =========================\n",
    "def decode_example(example: dict, tokenizer, task: str):\n",
    "    input_txt = tokenizer.decode(example[\"input_ids\"], skip_special_tokens=False)\n",
    "    input_txt = input_txt.split(tokenizer.eos_token)[0].replace(tokenizer.eos_token, \"\")\n",
    "    label_ids = [tok_id if tok_id != -100 else tokenizer.pad_token_id for tok_id in example[\"labels\"]]\n",
    "    label_txt = tokenizer.decode(label_ids, skip_special_tokens=True)\n",
    "    input_preview = \" \".join(map(str, example[\"input_ids\"][:30]))\n",
    "    label_preview = \" \".join(map(str, label_ids[:15]))\n",
    "    return {\n",
    "        \"input_text\": input_txt,\n",
    "        \"label_text\": label_txt,\n",
    "        \"input_ids_preview\": input_preview,\n",
    "        \"label_ids_preview\": label_preview,\n",
    "    }\n",
    "\n",
    "print(\"Classification post-preprocessing\\n\")\n",
    "for i, ex in enumerate(tokenized_classification[\"train\"].select(range(min(5, len(tokenized_classification[\"train\"])) ))):\n",
    "    decoded = decode_example(ex, tokenizer, task=\"classification\")\n",
    "    print(f\"--- Example {i+1} ---\")\n",
    "    print(\"INPUT:\", decoded[\"input_text\"])\n",
    "    print(\"LABEL:\", decoded[\"label_text\"])\n",
    "    print(\"input_ids first 30:\", decoded[\"input_ids_preview\"])\n",
    "    print(\"label_ids first 15:\", decoded[\"label_ids_preview\"])\n",
    "    print()\n",
    "\n",
    "print(\"Summarisation post-preprocessing\\n\")\n",
    "for i, ex in enumerate(tokenized_summarization[\"train\"].select(range(min(5, len(tokenized_summarization[\"train\"])) ))):\n",
    "    decoded = decode_example(ex, tokenizer, task=\"summarization\")\n",
    "    print(f\"--- Example {i+1} ---\")\n",
    "    print(\"INPUT:\", decoded[\"input_text\"])\n",
    "    print(\"SUMMARY:\", decoded[\"label_text\"])\n",
    "    print(\"input_ids first 30:\", decoded[\"input_ids_preview\"])\n",
    "    print(\"label_ids first 15:\", decoded[\"label_ids_preview\"])\n",
    "    print()\n",
    "\n",
    "# ==================\n",
    "# 8. Metrics setup\n",
    "# ==================\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_classification_metrics(eval_pred):\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        if len(predictions.shape) == 3:\n",
    "            predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "\n",
    "        if np.any(predictions < 0) or np.any(labels < 0):\n",
    "            logger.warning(\"Found negative values in predictions or labels. Clamping to 0.\")\n",
    "            predictions = np.clip(predictions, 0, None)\n",
    "            labels = np.clip(labels, 0, None)\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        logger.info(f\"Sample pred: {decoded_preds[0]}, label: {decoded_labels[0]}\")\n",
    "\n",
    "        decoded_preds = [p.strip().lower() for p in decoded_preds]\n",
    "        decoded_labels = [l.strip().lower() for l in decoded_labels]\n",
    "\n",
    "        pred_binary = [1 if p == \"positive\" else 0 for p in decoded_preds]\n",
    "        label_binary = [1 if l == \"positive\" else 0 for l in decoded_labels]\n",
    "\n",
    "        acc = accuracy_metric.compute(predictions=pred_binary, references=label_binary)\n",
    "        f1 = f1_metric.compute(predictions=pred_binary, references=label_binary, average=\"weighted\")\n",
    "        return {\n",
    "            \"accuracy\": acc.get(\"accuracy\", 0.0),\n",
    "            \"f1\": f1.get(\"f1\", 0.0),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Classification metrics error: {e}. Returning defaults.\")\n",
    "        return {\"accuracy\": 0.0, \"f1\": 0.0}\n",
    "\n",
    "def compute_summarization_metrics(eval_pred):\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        if len(predictions.shape) == 3:\n",
    "            predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "        if np.any(predictions < 0) or np.any(labels < 0):\n",
    "            logger.warning(\"Found negative values in predictions or labels. Clamping to 0.\")\n",
    "            predictions = np.clip(predictions, 0, None)\n",
    "            labels = np.clip(labels, 0, None)\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        logger.info(f\"Sample pred: {decoded_preds[0]}, label: {decoded_labels[0]}\")\n",
    "\n",
    "        decoded_preds = [p.strip() if p.strip() else \"\" for p in decoded_preds]\n",
    "        decoded_labels = [l.strip() if l.strip() else \"\" for l in decoded_labels]\n",
    "\n",
    "        result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        return {\n",
    "            \"rouge1\": result.get(\"rouge1\", 0.0),\n",
    "            \"rouge2\": result.get(\"rouge2\", 0.0),\n",
    "            \"rougeL\": result.get(\"rougeL\", 0.0),\n",
    "            \"rougeLsum\": result.get(\"rougeLsum\", 0.0),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Summarization metrics error: {e}. Returning defaults.\")\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0}\n",
    "\n",
    "# =====================\n",
    "# 9. Training arguments\n",
    "# =====================\n",
    "def get_training_args(method_name, task_name):\n",
    "    is_peft = (\"lora\" in method_name) or (\"ablated\" in method_name)\n",
    "    lr = 3e-4 if is_peft else 1e-5\n",
    "\n",
    "    if DATASET_SIZE == \"full\":\n",
    "        epochs = 5 if task_name == \"summarization\" else 3\n",
    "        batch_size = 2\n",
    "        eval_steps = None\n",
    "        eval_strategy = \"epoch\"\n",
    "        save_strategy = \"epoch\"\n",
    "        logging_steps = 10\n",
    "        save_steps = None\n",
    "    else:\n",
    "        epochs, batch_size, eval_steps = 3, 2, 10\n",
    "        eval_strategy = \"steps\"\n",
    "        save_strategy = \"steps\"\n",
    "        logging_steps = max(1, eval_steps // 2)\n",
    "        save_steps = eval_steps\n",
    "\n",
    "    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    use_fp16 = (not use_bf16) and torch.cuda.is_available()\n",
    "\n",
    "    load_best = \"lora\" in method_name\n",
    "\n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"{OUTPUT_DIR}/results/{task_name}/{method_name}\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=lr,\n",
    "        warmup_steps=200 if DATASET_SIZE == \"full\" else 50,\n",
    "        weight_decay=0.1,\n",
    "        eval_strategy=eval_strategy,\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy=save_strategy,\n",
    "        save_steps=save_steps,\n",
    "        load_best_model_at_end=load_best,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        save_total_limit=2,\n",
    "        logging_steps=logging_steps,\n",
    "        bf16=use_bf16,\n",
    "        fp16=use_fp16,\n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_drop_last=True,\n",
    "        report_to=\"none\",\n",
    "        predict_with_generate=True,\n",
    "        max_grad_norm=1.0,\n",
    "        gradient_accumulation_steps=1,\n",
    "        labels_smoothing_factor=0.0,\n",
    "        optim=\"adamw_torch\",\n",
    "        gradient_checkpointing=False,\n",
    "    )\n",
    "\n",
    "# ==========================\n",
    "# 10. Main training loop\n",
    "# ==========================\n",
    "base_methods = [\"lora\"]\n",
    "ablation_methods = [\"lora_ablated_alpha0\"] if RUN_ABLATIONS else []\n",
    "methods_to_run = base_methods + ablation_methods\n",
    "\n",
    "tasks = {\n",
    "    \"classification\": (tokenized_classification, compute_classification_metrics),\n",
    "    \"summarization\": (tokenized_summarization, compute_summarization_metrics),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "os.makedirs(f\"{OUTPUT_DIR}/results\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/plots\", exist_ok=True)\n",
    "\n",
    "for method_name in methods_to_run:\n",
    "    for task_name, (dataset, compute_metrics) in tasks.items():\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"EXPERIMENT {method_name.upper()} on {task_name.upper()}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        try:\n",
    "            config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "            if getattr(config, \"num_heads\", None) != 8:\n",
    "                config.num_heads = 8\n",
    "\n",
    "            use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                config=config,\n",
    "                torch_dtype=torch.bfloat16 if use_bf16 else torch.float32,\n",
    "                ignore_mismatched_sizes=True,\n",
    "            )\n",
    "\n",
    "            model.to(device)\n",
    "\n",
    "            d_model = model.config.d_model\n",
    "            num_heads = getattr(model.config, \"num_heads\", 8)\n",
    "\n",
    "            peft_configs_local = {}\n",
    "\n",
    "            if method_name == \"lora\":\n",
    "                peft_configs_local[\"lora\"] = LoraConfig(\n",
    "                    r=32,\n",
    "                    lora_alpha=32,\n",
    "                    target_modules=[\"q\", \"v\"],\n",
    "                    lora_dropout=0.05,\n",
    "                    bias=\"none\",\n",
    "                    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "                )\n",
    "            elif method_name == \"lora_ablated_alpha0\":\n",
    "                peft_configs_local[\"lora_ablated_alpha0\"] = LoraConfig(\n",
    "                    r=32,\n",
    "                    lora_alpha=0,\n",
    "                    target_modules=[\"q\", \"v\"],\n",
    "                    lora_dropout=0.05,\n",
    "                    bias=\"none\",\n",
    "                    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "                )\n",
    "\n",
    "            if method_name in peft_configs_local:\n",
    "                model = get_peft_model(model, peft_configs_local[method_name])\n",
    "                model.print_trainable_parameters()\n",
    "\n",
    "            training_args = get_training_args(method_name, task_name)\n",
    "\n",
    "            data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=dataset[\"train\"],\n",
    "                eval_dataset=dataset[\"validation\"],\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_metrics,\n",
    "                tokenizer=tokenizer,\n",
    "            )\n",
    "\n",
    "            print(\"Training...\")\n",
    "            train_result = trainer.train()\n",
    "\n",
    "            print(\"Evaluating...\")\n",
    "            test_dataset = dataset[\"test\"]\n",
    "            gen_kwargs = {\n",
    "                \"max_length\": 5 if task_name == \"classification\" else 64,\n",
    "                \"num_beams\": 4,\n",
    "                \"early_stopping\": True,\n",
    "            }\n",
    "            training_args.generation_max_length = gen_kwargs[\"max_length\"]\n",
    "            training_args.generation_num_beams = gen_kwargs[\"num_beams\"]\n",
    "\n",
    "            test_metrics = trainer.evaluate(test_dataset)\n",
    "\n",
    "            predictions = trainer.predict(dataset[\"validation\"])\n",
    "            cleaned_predictions = np.where(\n",
    "                predictions.predictions != -100,\n",
    "                predictions.predictions,\n",
    "                tokenizer.pad_token_id,\n",
    "            )\n",
    "            cleaned_predictions = np.clip(cleaned_predictions, 0, tokenizer.vocab_size - 1)\n",
    "            logger.info(\n",
    "                f\"Sample generations: {tokenizer.batch_decode(cleaned_predictions[:5], skip_special_tokens=True)}\"\n",
    "            )\n",
    "\n",
    "            trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            total = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "            exp_name = f\"{method_name}_{task_name}\"\n",
    "            results[exp_name] = {\n",
    "                \"train_metrics\": train_result.metrics,\n",
    "                \"test_metrics\": test_metrics,\n",
    "                \"trainable_params\": trainable,\n",
    "                \"total_params\": total,\n",
    "                \"log_history\": trainer.state.log_history,\n",
    "            }\n",
    "\n",
    "            save_path = f\"{OUTPUT_DIR}/models/{task_name}/{method_name}\"\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            trainer.save_model(save_path)\n",
    "            print(f\"Completed and saved to {save_path}\")\n",
    "\n",
    "            del model, trainer\n",
    "            safe_cleanup()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"ERROR in {method_name} {task_name}: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            try:\n",
    "                del model, trainer\n",
    "            except:\n",
    "                pass\n",
    "            safe_cleanup()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ALL EXPERIMENTS COMPLETED\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ==========================\n",
    "# 11. Results summary/report\n",
    "# ==========================\n",
    "if results:\n",
    "    print(\"\\nSUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    for exp_name, exp_data in results.items():\n",
    "        method_task_split = exp_name.split(\"_\", 1)\n",
    "        method = method_task_split[0]\n",
    "        task = method_task_split[1] if len(method_task_split) > 1 else \"unknown\"\n",
    "\n",
    "        metrics = exp_data[\"test_metrics\"]\n",
    "        pct = 100 * exp_data[\"trainable_params\"] / exp_data[\"total_params\"]\n",
    "        print(f\"{method.upper()} - {task.capitalize()}\")\n",
    "        print(f\"  Trainable: {pct:.2f}%\")\n",
    "        if task == \"classification\":\n",
    "            print(f\"  Accuracy: {metrics.get('eval_accuracy', 0):.4f}\")\n",
    "            print(f\"  F1:       {metrics.get('eval_f1', 0):.4f}\")\n",
    "        else:\n",
    "            print(f\"  ROUGE-1:  {metrics.get('eval_rouge1', 0):.4f}\")\n",
    "            print(f\"  ROUGE-L:  {metrics.get('eval_rougeL', 0):.4f}\")\n",
    "        print()\n",
    "\n",
    "    print(\"Learning curves...\")\n",
    "    plot_paths = {}\n",
    "    plot_save_dir = f\"{OUTPUT_DIR}/plots\"\n",
    "    os.makedirs(plot_save_dir, exist_ok=True)\n",
    "\n",
    "    for exp_name, exp_data in results.items():\n",
    "        method_task_split = exp_name.split(\"_\", 1)\n",
    "        task = method_task_split[1] if len(method_task_split) > 1 else \"unknown\"\n",
    "        plot_path = plot_learning_curves(exp_data[\"log_history\"], exp_name, task, save_dir=plot_save_dir)\n",
    "        plot_paths[exp_name] = plot_path\n",
    "\n",
    "    ablation_plot_paths = {}\n",
    "    if RUN_ABLATIONS:\n",
    "        print(\"Ablation comparison plots...\")\n",
    "        for task_name in tasks.keys():\n",
    "            task_results = {k: v for k, v in results.items() if k.endswith(task_name)}\n",
    "            if task_results:\n",
    "                ablation_plot_path = plot_ablation_comparisons(task_results, task_name, save_dir=plot_save_dir)\n",
    "                if ablation_plot_path:\n",
    "                    ablation_plot_paths[task_name] = ablation_plot_path\n",
    "\n",
    "    results_df_rows = []\n",
    "    for exp_name, exp_data in results.items():\n",
    "        method, task = exp_name.split(\"_\", 1)\n",
    "        row = {\n",
    "            \"Method\": method.upper(),\n",
    "            \"Task\": task.capitalize(),\n",
    "            \"Trainable %\": 100 * exp_data[\"trainable_params\"] / exp_data[\"total_params\"],\n",
    "        }\n",
    "        for k, v in exp_data[\"test_metrics\"].items():\n",
    "            if isinstance(v, (int, float)):\n",
    "                row[k] = v\n",
    "        results_df_rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(results_df_rows)\n",
    "    cols = [\"Method\", \"Task\", \"Trainable %\"]\n",
    "    metric_cols = [c for c in df.columns if c.startswith(\"eval_\")]\n",
    "    cols.extend(sorted(metric_cols))\n",
    "    df = df[cols]\n",
    "    df.to_csv(f\"{OUTPUT_DIR}/lora_results.csv\", index=False)\n",
    "    print(f\"Results saved to {OUTPUT_DIR}/lora_results.csv\")\n",
    "\n",
    "    report_path = f\"{OUTPUT_DIR}/lora_final_report.md\"\n",
    "    report_dir = os.path.dirname(report_path)\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(\"# LoRA Adaptation Results - T5-small\\n\\n\")\n",
    "        f.write(\"## Configuration\\n\")\n",
    "        f.write(f\"- Model: {MODEL_NAME}\\n\")\n",
    "        f.write(f\"- Dataset Size: {DATASET_SIZE}\\n\")\n",
    "        f.write(\"- Methods: LoRA\\n\")\n",
    "        if RUN_ABLATIONS:\n",
    "            f.write(\"- Ablations: enabled\\n\")\n",
    "        f.write(\"\\n## Summary Table\\n\")\n",
    "        f.write(df.to_markdown(index=False))\n",
    "        f.write(\"\\n\\n## Learning Curves\\n\")\n",
    "        for exp_name, plot_path in plot_paths.items():\n",
    "            relative_plot_path = os.path.relpath(plot_path, start=report_dir)\n",
    "            f.write(f\"- {exp_name}: {relative_plot_path}\\n\")\n",
    "        if RUN_ABLATIONS and ablation_plot_paths:\n",
    "            f.write(\"\\n## Ablation Comparisons\\n\")\n",
    "            for task_name, plot_path in ablation_plot_paths.items():\n",
    "                relative_plot_path = os.path.relpath(plot_path, start=report_dir)\n",
    "                f.write(f\"- {task_name.capitalize()} Ablation Comparison: {relative_plot_path}\\n\")\n",
    "\n",
    "    print(f\"Report saved to {report_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
