{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.4.6)\n",
      "Requirement already satisfied: protobuf<4.0 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.20.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install evaluate without upgrading any packages unnecessarily\n",
    "!pip install --no-deps evaluate \"protobuf<4.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os   \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import traceback\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    PrefixTuningConfig,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "import logging\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION \n",
    "MODEL_NAME = \"google/flan-t5-small\" # flan-t5-small model is giving issues - config dim bug (num_heads=6 mismatch)\n",
    "SUMMARIZATION_DATASET = \"knkarthick/samsum\"\n",
    "\n",
    "BENCHMARK_GLUE=\"glue\"\n",
    "GLUE_DATASET_TASK_SC = \"sst2\"  # SST-2 for sentiment classification\n",
    "\n",
    "DATASET_SIZE = 'full' # 100 or 500 or 'full' \n",
    "RUN_ABLATIONS = True  # Toggle to enable/disable ablation study (modular flag)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "NUM_VIRTUAL_TOKENS = 20 # CHANGE: Increased from 20 to 50 for better adaptation in prefix/prompt - Why: Longer tokens allow stronger task-specific tuning, fixing weak/flat metrics in prefix/prompt\n",
    "MAX_POS = 512\n",
    "\n",
    "OUTPUT_DIR = './kaggle/working/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Prefix-Tuning COMPARISON - T5-small\n",
      "============================================================\n",
      "Dataset size: full\n",
      "Model: google/flan-t5-small\n",
      "Methods: Prefix-Tuning\n",
      "Ablations Enabled: Including ablated variants for study\n",
      "Note: For prefix ablation, removing projection layer\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Prefix-Tuning COMPARISON - T5-small\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset size: {DATASET_SIZE}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(\"Methods: Prefix-Tuning\")\n",
    "if RUN_ABLATIONS:\n",
    "    print(\"Ablations Enabled: Including ablated variants for study\")\n",
    "    print(\"Note: For prefix ablation, removing projection layer\")\n",
    "print(\"=\"*60)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_dataset_size(dataset, size):\n",
    "    if size == 'full':\n",
    "        return dataset\n",
    "    if isinstance(size, int) and size > 0:\n",
    "        return dataset.select(range(min(size, len(dataset))))\n",
    "    raise ValueError(f\"Invalid size: {size}\")\n",
    "\n",
    "def setup_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "def safe_cleanup():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(log_history, exp_name, task_name, save_dir=\"./plots\"):\n",
    "    \"\"\"Plot train/eval loss and task-specific metrics vs step.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "   \n",
    "    # Extract data\n",
    "    #steps = [log['step'] for log in log_history if 'step' in log and 'eval_loss' not in log] # Get train steps\n",
    "    eval_steps = [log['step'] for log in log_history if 'eval_loss' in log] # Get eval steps\n",
    "    train_losses = [log['loss'] for log in log_history if 'loss' in log] # 'loss' is train loss\n",
    "    eval_losses = [log['eval_loss'] for log in log_history if 'eval_loss' in log]\n",
    "   \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "   \n",
    "    # Loss curve\n",
    "    # Match train loss steps to eval steps for cleaner plots if they differ\n",
    "    train_steps_for_loss = [log['step'] for log in log_history if 'loss' in log]\n",
    "    axes[0].plot(train_steps_for_loss, train_losses, label='Train Loss', marker='o', alpha=0.7)\n",
    "    if eval_losses:\n",
    "        axes[0].plot(eval_steps, eval_losses, label='Eval Loss', marker='s')\n",
    "    axes[0].set_xlabel('Step')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title(f'{exp_name} - Loss Curve')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Task-specific metric\n",
    "    if task_name == \"classification\":\n",
    "        eval_accs = [log['eval_accuracy'] for log in log_history if 'eval_accuracy' in log]\n",
    "        if eval_accs:\n",
    "            axes[1].plot(eval_steps, eval_accs, label='Eval Accuracy', marker='o', color='green')\n",
    "            axes[1].set_ylabel('Accuracy')\n",
    "    else: # summarization\n",
    "        eval_rouge_ls = [log['eval_rougeL'] for log in log_history if 'eval_rougeL' in log]\n",
    "        if eval_rouge_ls:\n",
    "            axes[1].plot(eval_steps, eval_rouge_ls, label='Eval ROUGE-L', marker='o', color='green')\n",
    "            axes[1].set_ylabel('ROUGE-L')\n",
    "   \n",
    "    axes[1].set_xlabel('Step')\n",
    "    axes[1].set_title(f'{exp_name} - {task_name.capitalize()} Metric')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(save_dir, f\"{exp_name}_curves.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Learning curves saved to {plot_path}\")\n",
    "    return plot_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ablation_comparisons(results, task_name, save_dir=\"./plots\"):\n",
    "    \"\"\"Graphical analysis: Compare baselines vs ablations for a task.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    methods = list(results.keys())\n",
    "    baselines = [m for m in methods if \"_ablated_\" not in m]\n",
    "    ablations = [m for m in methods if \"_ablated_\" in m]\n",
    "    \n",
    "    if not ablations:\n",
    "        return None\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Trainable params comparison\n",
    "    trainable_pcts = [100 * results[m][\"trainable_params\"] / results[m][\"total_params\"] for m in methods]\n",
    "    sns.barplot(x=methods, y=trainable_pcts, ax=axes[0])\n",
    "    axes[0].set_ylabel('Trainable %')\n",
    "    axes[0].set_title(f'Trainable Params Comparison - {task_name.capitalize()}')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Metric comparison (use key metric)\n",
    "    if task_name == \"classification\":\n",
    "        metrics = [results[m][\"test_metrics\"].get(\"eval_accuracy\", 0) for m in methods]\n",
    "        metric_label = 'Accuracy'\n",
    "    else:\n",
    "        metrics = [results[m][\"test_metrics\"].get(\"eval_rougeL\", 0) for m in methods]\n",
    "        metric_label = 'ROUGE-L'\n",
    "    \n",
    "    sns.barplot(x=methods, y=metrics, ax=axes[1])\n",
    "    axes[1].set_ylabel(metric_label)\n",
    "    axes[1].set_title(f'Performance Comparison - {task_name.capitalize()}')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(save_dir, f\"ablation_comparison_{task_name}.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Ablation comparison plot saved to {plot_path}\")\n",
    "    return plot_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Datasets loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## LOAD DATASETS \n",
    "print(\"Loading datasets\")\n",
    "# Classification dataset - SST-2\n",
    "classification_dataset = load_dataset(BENCHMARK_GLUE, GLUE_DATASET_TASK_SC)\n",
    "# Summarization dataset - SAMSum\n",
    "summarization_dataset = load_dataset(SUMMARIZATION_DATASET)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = setup_tokenizer(MODEL_NAME)\n",
    "\n",
    "if DATASET_SIZE != 'full':\n",
    "    print(f\"Limiting dataset size to {DATASET_SIZE} for train.\")\n",
    "    classification_dataset['train'] = limit_dataset_size(classification_dataset['train'], DATASET_SIZE)\n",
    "    classification_dataset['validation'] = limit_dataset_size(classification_dataset['validation'], DATASET_SIZE // 4)\n",
    "    classification_dataset['test'] = limit_dataset_size(classification_dataset.get('test', classification_dataset['validation']), DATASET_SIZE // 4)\n",
    "    \n",
    "    summarization_dataset['train'] = limit_dataset_size(summarization_dataset['train'], DATASET_SIZE)\n",
    "    summarization_dataset['validation'] = limit_dataset_size(summarization_dataset['validation'], DATASET_SIZE // 4)\n",
    "    summarization_dataset['test'] = limit_dataset_size(summarization_dataset['test'], DATASET_SIZE // 4)\n",
    "\n",
    "print(\"Datasets loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sample Datasets\n",
      "Classification Train Samples (Before Preprocessing):\n",
      "{'sentence': 'hide new secretions from the parental units ', 'label': 0, 'idx': 0}\n",
      "{'sentence': 'contains no wit , only labored gags ', 'label': 0, 'idx': 1}\n",
      "{'sentence': 'that loves its characters and communicates something rather beautiful about human nature ', 'label': 1, 'idx': 2}\n",
      "{'sentence': 'remains utterly satisfied to remain the same throughout ', 'label': 0, 'idx': 3}\n",
      "{'sentence': 'on the worst revenge-of-the-nerds clichés the filmmakers could dredge up ', 'label': 0, 'idx': 4}\n",
      "{'sentence': \"that 's far too tragic to merit such superficial treatment \", 'label': 0, 'idx': 5}\n",
      "{'sentence': 'demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . ', 'label': 1, 'idx': 6}\n",
      "{'sentence': 'of saucy ', 'label': 1, 'idx': 7}\n",
      "{'sentence': \"a depressed fifteen-year-old 's suicidal poetry \", 'label': 0, 'idx': 8}\n",
      "{'sentence': \"are more deeply thought through than in most ` right-thinking ' films \", 'label': 1, 'idx': 9}\n",
      "\n",
      "Summarization Train Samples (Before Preprocessing):\n",
      "{'id': '13818513', 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\nJerry: Sure!\\nAmanda: I'll bring you tomorrow :-)\", 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'}\n",
      "{'id': '13728867', 'dialogue': 'Olivia: Who are you voting for in this election? \\nOliver: Liberals as always.\\nOlivia: Me too!!\\nOliver: Great', 'summary': 'Olivia and Olivier are voting for liberals in this election. '}\n",
      "{'id': '13681000', 'dialogue': \"Tim: Hi, what's up?\\nKim: Bad mood tbh, I was going to do lots of stuff but ended up procrastinating\\nTim: What did you plan on doing?\\nKim: Oh you know, uni stuff and unfucking my room\\nKim: Maybe tomorrow I'll move my ass and do everything\\nKim: We were going to defrost a fridge so instead of shopping I'll eat some defrosted veggies\\nTim: For doing stuff I recommend Pomodoro technique where u use breaks for doing chores\\nTim: It really helps\\nKim: thanks, maybe I'll do that\\nTim: I also like using post-its in kaban style\", 'summary': 'Kim may try the pomodoro technique recommended by Tim to get more stuff done.'}\n",
      "{'id': '13730747', 'dialogue': \"Edward: Rachel, I think I'm in ove with Bella..\\nrachel: Dont say anything else..\\nEdward: What do you mean??\\nrachel: Open your fu**ing door.. I'm outside\", 'summary': 'Edward thinks he is in love with Bella. Rachel wants Edward to open his door. Rachel is outside. '}\n",
      "{'id': '13728094', 'dialogue': \"Sam: hey  overheard rick say something\\nSam: i don't know what to do :-/\\nNaomi: what did he say??\\nSam: he was talking on the phone with someone\\nSam: i don't know who\\nSam: and he was telling them that he wasn't very happy here\\nNaomi: damn!!!\\nSam: he was saying he doesn't like being my roommate\\nNaomi: wow, how do you feel about it?\\nSam: i thought i was a good rommate\\nSam: and that we have a nice place\\nNaomi: that's true man!!!\\nNaomi: i used to love living with you before i moved in with me boyfriend\\nNaomi: i don't know why he's saying that\\nSam: what should i do???\\nNaomi: honestly if it's bothering you that much you should talk to him\\nNaomi: see what's going on\\nSam: i don't want to get in any kind of confrontation though\\nSam: maybe i'll just let it go\\nSam: and see how it goes in the future\\nNaomi: it's your choice sam\\nNaomi: if i were you i would just talk to him and clear the air\", 'summary': 'Sam is confused, because he overheard Rick complaining about him as a roommate. Naomi thinks Sam should talk to Rick. Sam is not sure what to do.'}\n",
      "{'id': '13716343', 'dialogue': \"Neville: Hi there, does anyone remember what date I got married on?\\nDon: Are you serious?\\nNeville: Dead serious. We're on vacation, and Tina's mad at me about something. I have a strange suspicion that this might have something to do with our wedding anniversary, but I have nowhere to check.\\nWyatt: Hang on, I'll ask my wife.\\nDon: Haha, someone's in a lot of trouble :D\\nWyatt: September 17. I hope you remember the year ;)\", 'summary': \"Wyatt reminds Neville his wedding anniversary is on the 17th of September. Neville's wife is upset and it might be because Neville forgot about their anniversary.\"}\n",
      "{'id': '13611672', 'dialogue': \"John: Ave. Was there any homework for tomorrow?\\nCassandra: hello :D Of course, as always :D\\nJohn: What exactly?\\nCassandra: I'm not sure so I'll check it for you in 20minutes. \\nJohn: Cool, thanks. Sorry I couldn't be there, but I was busy as fuck...my stupid boss as always was trying to piss me off\\nCassandra: No problem, what did he do this time?\\nJohn: Nothing special, just the same as always, treating us like children, commanding to do this and that...\\nCassandra: sorry to hear that. but why don't you just go to your chief and tell him everything?\\nJohn: I would, but I don't have any support from others, they are like goddamn pupets and pretend that everything's fine...I'm not gonna fix everything for everyone\\nCassandra: I understand...Nevertheless, just try to ignore him. I know it might sound ridiculous as fuck, but sometimes there's nothing more you can do.\\nJohn: yeah I know...maybe some beer this week?\\nCassandra: Sure, but I got some time after classes only...this week is gonna be busy\\nJohn: no problem, I can drive you home and we can go to some bar or whatever.\\nCassandra: cool. ok, I got this homework. it's page 15 ex. 2 and 3, I also asked the others to study another chapter, especially the vocabulary from the very first pages. Just read it.\\nJohn: gosh...I don't know if I'm smart enough to do it :'D\\nCassandra: you are, don't worry :P Just circle all the words you don't know and we'll continue on Monday.\\nJohn: ok...then I'll try my best :D\\nCassandra: sure, if you will have any questions just either text or call me and I'll help you.\\nJohn: I hope I won't have to waste your time xD\\nCassandra: you're not wasting my time, I'm your teacher, I'm here to help. This is what I get money for, also :P\\nJohn: just kidding :D ok, so i guess we'll stay in touch then\\nCassandra: sure, have a nice evening :D\\nJohn: you too, se ya\\nCassandra: Byeeeee\", 'summary': \"John didn't show up for class due to some work issues with his boss. Cassandra, his teacher told him which exercises to do, and which chapter to study. They are going to meet up for a beer sometime this week after class. \"}\n",
      "{'id': '13730463', 'dialogue': \"Sarah: I found a song on youtube and I think you'll like it\\nJames: What song?\\nSarah: <file_other>\\nJames: Oh. I know it! \\nJames: I heard it before in some compilation\\nSarah: I can't stop playing it over and over\\nJames: That's exactly how I know lyrics to all of the songs on my playlist :D\\nSarah: Haha. No lyrics here though. Instrumental ;D\\nJames: Instrumental songs are different kind of music. \\nJames: But you have to remember that the activity you do when you listen to this song\\nJames: Is the actvity your brain will connect to the song\\nJames: And everytime you play this song at home\\nJames: You'll be thinking of your work\\nSarah: Yeah, I know that. That's why we sometimes say - I used to like that song, but now it just reminds me of bad memories\\nJames: Yup. Everytime you change your partner, you have to get rid of your favorite music :D\\nSarah: Hahaha. True, true.\", 'summary': 'Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to the context they were played in and brings to mind the associated memories.'}\n",
      "{'id': '13809976', 'dialogue': 'Noah: When and where are we meeting? :)\\nMadison: I thought you were busy...?\\nNoah: Yeah, I WAS. I quit my job. \\nMadison: No way! :o :o :o Why? I thought you liked it...?\\nNoah: Well, I used to, until my boss turned into a complete cock... Long story.', 'summary': 'Noah wants to meet, he quit his job, because his boss was a dick.'}\n",
      "{'id': '13809912', 'dialogue': \"Matt: Do you want to go for date?\\nAgnes: Wow! You caught me out with this question Matt.\\nMatt: Why?\\nAgnes: I simply didn't expect this from you.\\nMatt: Well, expect the unexpected.\\nAgnes: Can I think about it?\\nMatt: What is there to think about?\\nAgnes: Well, I don't really know you.\\nMatt: This is the perfect time to get to know eachother\\nAgnes: Well that's true.\\nMatt: So let's go to the Georgian restaurant in Kazimierz.\\nAgnes: Now your convincing me.\\nMatt: Cool, saturday at 6pm?\\nAgnes: That's fine.\\nMatt: I can pick you up on the way to the restaurant.\\nAgnes: That's really kind of you.\\nMatt: No problem.\\nAgnes: See you on saturday.\\nMatt: Yes, looking forward to it.\\nAgnes: Me too.\", 'summary': \"Matt invites Agnes for a date to get to know each other better. They'll go to the Georgian restaurant in Kazimierz on Saturday at 6 pm, and he'll pick her up on the way to the place.\"}\n"
     ]
    }
   ],
   "source": [
    "# Print 10 samples from each train dataset before preprocessing\n",
    "print(\"Original Sample Datasets\")\n",
    "\n",
    "print(\"Classification Train Samples (Before Preprocessing):\")\n",
    "for i in range(min(10, len(classification_dataset['train']))):\n",
    "    print(classification_dataset[\"train\"][i])\n",
    "\n",
    "print(\"\\nSummarization Train Samples (Before Preprocessing):\")\n",
    "for i in range(min(10, len(summarization_dataset['train']))):\n",
    "    print(summarization_dataset[\"train\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for Classification\n",
    "def preprocess_classification(examples):\n",
    "    # Create input sentences with the required prefix\n",
    "    inputs = [f\"Classify Sentiment as either positive or negative: {text}\" for text in examples[\"sentence\"]]\n",
    "    \n",
    "    # Define max length for inputs\n",
    "    max_input_len = MAX_POS - NUM_VIRTUAL_TOKENS\n",
    "    \n",
    "    # Tokenize inputs with truncation and padding\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_len, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Convert labels from numerical to text\n",
    "    labels_text = [\"negative\" if label == 0 else \"positive\" for label in examples[\"label\"]]\n",
    "    \n",
    "    # Tokenize labels similar to inputs\n",
    "    labels = tokenizer(text_target=labels_text, max_length=10, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Add tokenized labels to model inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_summarization(examples):\n",
    "    # Create input dialogues with the required prefix\n",
    "    inputs = [f\"Summarize the following conversation:\\n{dialogue}\" for dialogue in examples[\"dialogue\"]]\n",
    "    \n",
    "    # Define max length for inputs\n",
    "    max_input_len = MAX_POS - NUM_VIRTUAL_TOKENS\n",
    "    \n",
    "    # Tokenize inputs with truncation and padding\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_len, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Define max length for summaries\n",
    "    max_label_len = 128\n",
    "    \n",
    "    # Tokenize summaries with truncation and padding\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=max_label_len, truncation=True, padding=\"max_length\").input_ids\n",
    "    \n",
    "    # Add tokenized summaries to model inputs\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████| 1821/1821 [00:00<00:00, 2060.57 examples/s]\n",
      "Map: 100%|██████████████████████████████████████████████████████████████████| 819/819 [00:00<00:00, 1515.73 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-Preprocessing Sample Datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing\n",
    "print(\"\\nApplying preprocessing...\")\n",
    "tokenized_classification = classification_dataset.map(preprocess_classification, batched=True, remove_columns=classification_dataset[\"train\"].column_names)\n",
    "tokenized_summarization = summarization_dataset.map(preprocess_summarization, batched=True, remove_columns=summarization_dataset[\"train\"].column_names)\n",
    "\n",
    "# Print samples from each post preprocessing\n",
    "POST_PROCESS_SAMPLES = 5\n",
    "\n",
    "print(\"\\nPost-Preprocessing Sample Datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode a single example (input + label)\n",
    "def _decode_example(example: dict, tokenizer, task: str) -> dict:\n",
    "    \"\"\"\n",
    "    Returns a dict with:\n",
    "        - \"input_text\"   : the original prompt (e.g. \"Classify sentiment: …\")\n",
    "        - \"label_text\"   : the gold label (positive/negative or the full summary)\n",
    "        - \"input_ids\"    : first 30 tokens (for sanity check)\n",
    "        - \"label_ids\"    : first 15 tokens of the label\n",
    "    \"\"\"\n",
    "    # 1. Decode the **input** (skip special tokens, keep the prompt)\n",
    "    input_txt = tokenizer.decode(example[\"input_ids\"], skip_special_tokens=False)\n",
    "    # remove the padding part after the EOS token\n",
    "    input_txt = input_txt.split(tokenizer.eos_token)[0] + tokenizer.eos_token\n",
    "\n",
    "    # 2. Decode the **label**\n",
    "    # Labels contain -100 for ignored positions → replace with pad token first\n",
    "    label_ids = [\n",
    "        tok_id if tok_id != -100 else tokenizer.pad_token_id for tok_id in example[\"labels\"]\n",
    "    ]\n",
    "    label_txt = tokenizer.decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # 3. Short token previews (optional, makes the output tidy)\n",
    "    input_preview = \" \".join(map(str, example[\"input_ids\"][:30]))\n",
    "    label_preview = \" \".join(map(str, label_ids[:15]))\n",
    "\n",
    "    return {\n",
    "        \"input_text\": input_txt,\n",
    "        \"label_text\": label_txt,\n",
    "        \"input_ids_preview\": input_preview,\n",
    "        \"label_ids_preview\": label_preview,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Classification – post-preprocessing ===\n",
      "\n",
      "--- Example 1 ---\n",
      "INPUT  : Classify Sentiment as either positive or negative: hide new secretions from the parental units </s>\n",
      "LABEL  : negative\n",
      "\n",
      "--- Example 2 ---\n",
      "INPUT  : Classify Sentiment as either positive or negative: contains no wit , only labored gags </s>\n",
      "LABEL  : negative\n",
      "\n",
      "--- Example 3 ---\n",
      "INPUT  : Classify Sentiment as either positive or negative: that loves its characters and communicates something rather beautiful about human nature </s>\n",
      "LABEL  : positive\n",
      "\n",
      "--- Example 4 ---\n",
      "INPUT  : Classify Sentiment as either positive or negative: remains utterly satisfied to remain the same throughout </s>\n",
      "LABEL  : negative\n",
      "\n",
      "--- Example 5 ---\n",
      "INPUT  : Classify Sentiment as either positive or negative: on the worst revenge-of-the-nerds clichés the filmmakers could dredge up </s>\n",
      "LABEL  : negative\n",
      "\n",
      "=== Summarisation – post-preprocessing (5 examples) ===\n",
      "\n",
      "--- Example 1 ---\n",
      "INPUT  : Summarize the following conversation: Amanda: I baked cookies. Do you want some? Jerry: Sure! Amanda: I'll bring you tomorrow :-)</s>\n",
      "SUMMARY: Amanda baked cookies and will bring Jerry some tomorrow.\n",
      "\n",
      "--- Example 2 ---\n",
      "INPUT  : Summarize the following conversation: Olivia: Who are you voting for in this election? Oliver: Liberals as always. Olivia: Me too!! Oliver: Great</s>\n",
      "SUMMARY: Olivia and Olivier are voting for liberals in this election. \n",
      "\n",
      "--- Example 3 ---\n",
      "INPUT  : Summarize the following conversation: Tim: Hi, what's up? Kim: Bad mood tbh, I was going to do lots of stuff but ended up procrastinating Tim: What did you plan on doing? Kim: Oh you know, uni stuff and unfucking my room Kim: Maybe tomorrow I'll move my ass and do everything Kim: We were going to defrost a fridge so instead of shopping I'll eat some defrosted veggies Tim: For doing stuff I recommend Pomodoro technique where u use breaks for doing chores Tim: It really helps Kim: thanks, maybe I'll do that Tim: I also like using post-its in kaban style</s>\n",
      "SUMMARY: Kim may try the pomodoro technique recommended by Tim to get more stuff done.\n",
      "\n",
      "--- Example 4 ---\n",
      "INPUT  : Summarize the following conversation: Edward: Rachel, I think I'm in ove with Bella.. rachel: Dont say anything else.. Edward: What do you mean?? rachel: Open your fu**ing door.. I'm outside</s>\n",
      "SUMMARY: Edward thinks he is in love with Bella. Rachel wants Edward to open his door. Rachel is outside. \n",
      "\n",
      "--- Example 5 ---\n",
      "INPUT  : Summarize the following conversation: Sam: hey overheard rick say something Sam: i don't know what to do :-/ Naomi: what did he say?? Sam: he was talking on the phone with someone Sam: i don't know who Sam: and he was telling them that he wasn't very happy here Naomi: damn!!! Sam: he was saying he doesn't like being my roommate Naomi: wow, how do you feel about it? Sam: i thought i was a good rommate Sam: and that we have a nice place Naomi: that's true man!!! Naomi: i used to love living with you before i moved in with me boyfriend Naomi: i don't know why he's saying that Sam: what should i do??? Naomi: honestly if it's bothering you that much you should talk to him Naomi: see what's going on Sam: i don't want to get in any kind of confrontation though Sam: maybe i'll just let it go Sam: and see how it goes in the future Naomi: it's your choice sam Naomi: if i were you i would just talk to him and clear the air</s>\n",
      "SUMMARY: Sam is confused, because he overheard Rick complaining about him as a roommate. Naomi thinks Sam should talk to Rick. Sam is not sure what to do.\n",
      "\n",
      "Preprocessing complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classification samples\n",
    "print(\"\\n=== Classification – post-preprocessing ===\")\n",
    "for i, ex in enumerate(tokenized_classification[\"train\"].select(range(min(POST_PROCESS_SAMPLES, len(tokenized_classification[\"train\"]))))):\n",
    "    decoded = _decode_example(ex, tokenizer, task=\"classification\")\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"INPUT  : {decoded['input_text']}\")\n",
    "    print(f\"LABEL  : {decoded['label_text']}\")\n",
    "    # print(f\"input_ids  (first 30) : {decoded['input_ids_preview']}\")\n",
    "    # print(f\"label_ids  (first 15) : {decoded['label_ids_preview']}\")\n",
    "\n",
    "# Print summarisation samples\n",
    "print(\"\\n=== Summarisation – post-preprocessing (5 examples) ===\")\n",
    "for i, ex in enumerate(tokenized_summarization[\"train\"].select(range(min(POST_PROCESS_SAMPLES, len(tokenized_summarization[\"train\"]))))):\n",
    "    decoded = _decode_example(ex, tokenizer, task=\"summarization\")\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"INPUT  : {decoded['input_text']}\")\n",
    "    print(f\"SUMMARY: {decoded['label_text']}\")\n",
    "    # print(f\"input_ids  (first 30) : {decoded['input_ids_preview']}\")\n",
    "    # print(f\"label_ids  (first 15) : {decoded['label_ids_preview']}\")\n",
    "\n",
    "print(\"\\nPreprocessing complete\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_metrics(eval_pred):\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        \n",
    "        # Handling prediction tensors\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        if len(predictions.shape) == 3:\n",
    "            predictions = np.argmax(predictions, axis=-1)\n",
    "        \n",
    "        # Replace -100 in labels with pad_token_id\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "\n",
    "        # Validate predictions and labels for negative values\n",
    "        # if np.any(predictions < 0) or np.any(labels < 0):\n",
    "        #     logger.warning(f\"Found negative values in predictions or labels. Clamping to 0.\")\n",
    "        #     predictions = np.clip(predictions, 0, None)\n",
    "        #     labels = np.clip(labels, 0, None)\n",
    "        \n",
    "        # Decode the predictions and labels\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        # CHANGE: Added sample logging for debug - Why: To diagnose poor generations causing flat/low metrics\n",
    "        logger.info(f\"Sample pred: {decoded_preds[0]}, label: {decoded_labels[0]}\")  # Log first sample\n",
    "        \n",
    "        # Normalize the decoded texts\n",
    "        decoded_preds = [p.strip().lower() for p in decoded_preds]\n",
    "        decoded_labels = [l.strip().lower() for l in decoded_labels]\n",
    "        \n",
    "        # CHANGE: Use exact match instead of 'in' - Why: Prevents false positives from verbose outputs, fixing brittle mapping and low accuracy\n",
    "        map_label = {'positive': 1, 'negative': 0}\n",
    "        pred_binary = [map_label.get(p, -1) for p in decoded_preds] \n",
    "        label_binary = [map_label.get(l, -1) for l in decoded_labels]\n",
    "        \n",
    "        # Compute metrics\n",
    "        acc = accuracy_metric.compute(predictions=pred_binary, references=label_binary)\n",
    "        f1 = f1_metric.compute(predictions=pred_binary, references=label_binary, average=\"weighted\")\n",
    "        \n",
    "        # CHANGE: Ensure keys always returned - Why: Fixes empty plots by guaranteeing 'eval_accuracy' in logs\n",
    "        return {\"accuracy\": acc.get(\"accuracy\", 0.0), \"f1\": f1.get(\"f1\", 0.0)}\n",
    "    \n",
    "    except Exception as e:\n",
    "        # CHANGE: More verbose error logging - Why: Catches silent failures causing empty plots/0.0 metrics\n",
    "        logger.error(f\"Classification metrics error: {e}. Returning defaults.\")\n",
    "        return {\"accuracy\": 0.0, \"f1\": 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summarization_metrics(eval_pred):\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        \n",
    "        # Handling prediction tensors\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        if len(predictions.shape) == 3:\n",
    "            predictions = np.argmax(predictions, axis=-1)\n",
    "        \n",
    "        # Replace -100 in predictions/labels with pad_token_id\n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "        # Validate predictions and labels for negative values\n",
    "        if np.any(predictions < 0) or np.any(labels < 0):\n",
    "            logger.warning(f\"Found negative values in predictions or labels. Clamping to 0.\")\n",
    "            predictions = np.clip(predictions, 0, None)\n",
    "            labels = np.clip(labels, 0, None)\n",
    "        \n",
    "        # Decode the predictions and labels\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        # CHANGE: Added sample logging for debug - Why: To inspect poor generations causing decreasing ROUGE\n",
    "        logger.info(f\"Sample pred: {decoded_preds[0]}, label: {decoded_labels[0]}\")  # Log first sample\n",
    "        \n",
    "        # Normalize the decoded texts\n",
    "        decoded_preds = [p.strip() if p.strip() else \"empty\" for p in decoded_preds]\n",
    "        decoded_labels = [l.strip() if l.strip() else \"empty\" for l in decoded_labels]\n",
    "        \n",
    "        # Compute ROUGE scores\n",
    "        result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        \n",
    "        # CHANGE: Ensure keys always returned - Why: Fixes empty plots by guaranteeing 'eval_rougeL' in logs\n",
    "        return {\n",
    "            \"rouge1\": result.get(\"rouge1\", 0.0),\n",
    "            \"rouge2\": result.get(\"rouge2\", 0.0),\n",
    "            \"rougeL\": result.get(\"rougeL\", 0.0),\n",
    "            \"rougeLsum\": result.get(\"rougeLsum\", 0.0)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        # CHANGE: More verbose error logging - Why: Catches silent failures in metrics computation\n",
    "        logger.error(f\"Summarization metrics error: {e}. Returning defaults.\")\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix - @TODO: Integrate into main flow\n",
    "def plot_confusion_matrix(y_true, y_pred, classes=None, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, cbar=False, xticklabels=classes, yticklabels=classes)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def compute_and_plot_confusion_matrix_classification(decoded_labels, decoded_preds):\n",
    "    # Convert text labels to binary 0/1\n",
    "    label_binary = [1 if 'positive' in l else 0 for l in decoded_labels]\n",
    "    pred_binary = [1 if 'positive' in p else 0 for p in decoded_preds]\n",
    "    plot_confusion_matrix(label_binary, pred_binary, classes=['negative', 'positive'], title='Classification Confusion Matrix')\n",
    "\n",
    "def compute_and_plot_confusion_matrix_summarization(decoded_labels, decoded_preds, tokenizer):\n",
    "    # For summarization, generate token-level confusion matrix based on token matches\n",
    "    label_tokens = [tokenizer.tokenize(l) for l in decoded_labels]\n",
    "    pred_tokens = [tokenizer.tokenize(p) for p in decoded_preds]\n",
    "\n",
    "    true_tokens = []\n",
    "    pred_tokens_flat = []\n",
    "    for lt, pt in zip(label_tokens, pred_tokens):\n",
    "        min_len = min(len(lt), len(pt))\n",
    "        true_tokens.extend(lt[:min_len])\n",
    "        pred_tokens_flat.extend(pt[:min_len])\n",
    "\n",
    "    # Limit to top 10 tokens for visualization\n",
    "    all_tokens = list(set(true_tokens + pred_tokens_flat))\n",
    "    if len(all_tokens) > 10:\n",
    "        all_tokens = all_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_args(method_name, task_name):\n",
    "    is_peft = method_name in [\"prefix\"] or \"_ablated_\" in method_name\n",
    "    # CHANGE: Lowered LR for PEFT/ablation to 1e-3, Full FT to 1e-4 - Why: High LR caused instability/overfitting/decreasing metrics; matches t5-small recommendations\n",
    "    if \"no_proj\" in method_name:\n",
    "        lr = 1e-2  # MUCH higher LR for direct embedding training\n",
    "    elif is_peft:\n",
    "        lr = 1e-3  # Standard PEFT LR\n",
    "    else:\n",
    "        lr = 1e-4\n",
    "    \n",
    "    if DATASET_SIZE == 'full':\n",
    "        # CHANGE: Increased epochs to 5 for summarization - Why: Smaller dataset needs more passes for convergence, fixing underfitting/low ROUGE\n",
    "        epochs = 5 if task_name == 'summarization' else 3\n",
    "        batch, eval_steps = 8, 500\n",
    "    elif DATASET_SIZE <= 500:\n",
    "        # Use more epochs for very small datasets to allow for learning\n",
    "        epochs, batch, eval_steps = 10, 4, 20 # Eval more frequently\n",
    "    else:\n",
    "        epochs, batch, eval_steps = 3, 8, 100\n",
    "\n",
    "    # Adjust steps based on actual dataset size\n",
    "    if DATASET_SIZE != 'full':\n",
    "        total_steps = (DATASET_SIZE // batch) * epochs\n",
    "        # Ensure eval_steps is not 0 and is reasonable\n",
    "        eval_steps = max(1, min(total_steps // 5, 50)) # Eval 5 times per run, max 50\n",
    "        logging_steps = max(1, eval_steps // 2)\n",
    "        save_steps = eval_steps\n",
    "        eval_strategy = \"steps\"\n",
    "        save_strategy = \"steps\"\n",
    "    else:\n",
    "        eval_strategy = \"epoch\"\n",
    "        save_strategy = \"epoch\"\n",
    "        logging_steps = 100\n",
    "        save_steps = None\n",
    "        eval_steps = None\n",
    "\n",
    "    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    # CHANGE: Set fp16=True if not bf16 - Why: Faster training/mixed precision, fixing slow runs/low metrics if GPU supports\n",
    "    use_fp16 = not use_bf16 and torch.cuda.is_available()  # Enable fp16 on CUDA if bf16 unavailable\n",
    "    \n",
    "    # For prompt tuning in PEFT can cause errors\n",
    "    load_best = False\n",
    "    \n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"{OUTPUT_DIR}/results/{task_name}/{method_name}\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch,\n",
    "        per_device_eval_batch_size=batch * 2,\n",
    "        learning_rate=lr,\n",
    "        # CHANGE: Increased warmup_steps to 1000 - Why: Smoother optimization start, fixing oscillation/stuck loss in full FT/ablations\n",
    "        warmup_steps=1000 if DATASET_SIZE == 'full' else min(100, DATASET_SIZE // 10),\n",
    "        # CHANGE: Increased weight_decay to 0.1 - Why: Stronger regularization prevents overfitting, fixing loss→0 but metrics drop\n",
    "        weight_decay=0.1,\n",
    "        eval_strategy=eval_strategy,\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy=save_strategy,\n",
    "        save_steps=save_steps,\n",
    "        load_best_model_at_end=load_best,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        save_total_limit=2,\n",
    "        logging_steps=logging_steps,\n",
    "        bf16=use_bf16,\n",
    "        fp16=use_fp16,\n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_drop_last=True, # Avoid incomplete batches for stability\n",
    "        report_to=\"none\",\n",
    "        predict_with_generate=True,\n",
    "        max_grad_norm=1.0,  # Added to prevent gradient explosions\n",
    "        # CHANGE: Added gradient_accumulation_steps=4 - Why: Stabilizes training with small effective batches, fixing oscillation in ablations\n",
    "        gradient_accumulation_steps=4,\n",
    "        label_smoothing_factor=0.1,\n",
    "        # CHANGE: Set optim to 'adamw_torch' - Why: More robust for PEFT, fixing instability in ablations/Full FT\n",
    "        optim='adamw_torch',\n",
    "        # CHANGE: Set gradient_checkpointing=False - Why: Avoids grad flow issues in PEFT/T5, fixing \"no grad_fn\" error; trade memory for stability\n",
    "        gradient_checkpointing=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Seq2SeqTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs)\n",
    "        # Save past state if it exists\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            unwrapped_model = Accelerator().unwrap_model(model)\n",
    "            if isinstance(unwrapped_model, PeftModel):\n",
    "                model_base = unwrapped_model.base_model\n",
    "                if hasattr(model_base, \"model\"):\n",
    "                    model_name = model_base.model._get_name()\n",
    "                else:\n",
    "                    model_name = model_base._get_name()\n",
    "                if any(name in model_name for name in [\"GPT\", \"opt\", \"bloom\", \"llama\", \"gemma\"]):\n",
    "                    loss = self.label_smoother(outputs, labels, shift_labels=True)\n",
    "                else:\n",
    "                    loss = self.label_smoother(outputs, labels)\n",
    "            else:\n",
    "                loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXPERIMENT: PREFIX on CLASSIFICATION\n",
      "============================================================\n",
      "\n",
      "trainable params: 122,880 || all params: 77,084,032 || trainable%: 0.1594\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6315' max='6315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6315/6315 49:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.029600</td>\n",
       "      <td>5.857349</td>\n",
       "      <td>0.673611</td>\n",
       "      <td>0.748807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.478000</td>\n",
       "      <td>5.400231</td>\n",
       "      <td>0.821759</td>\n",
       "      <td>0.834204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.349300</td>\n",
       "      <td>5.312231</td>\n",
       "      <td>0.839120</td>\n",
       "      <td>0.843747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sample pred: positives and/s of the journey, label: positive\n",
      "INFO:__main__:Sample pred: positive, label: positive\n",
      "INFO:__main__:Sample pred: positive, label: positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best checkpoint manually: ./kaggle/working//results/classification/prefix\\checkpoint-6315\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sample pred: negative, label: positive\n",
      "INFO:__main__:Sample pred: positive, label: positive\n",
      "INFO:__main__:Sample generations: ['positive', 'negative', 'positive', 'positive', 'positive']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed and saved to ./kaggle/working/models/classification/prefix\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: PREFIX on SUMMARIZATION\n",
      "============================================================\n",
      "\n",
      "trainable params: 122,880 || all params: 77,084,032 || trainable%: 0.1594\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2305' max='2305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2305/2305 28:42, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>37.400200</td>\n",
       "      <td>38.505974</td>\n",
       "      <td>0.202003</td>\n",
       "      <td>0.036803</td>\n",
       "      <td>0.169504</td>\n",
       "      <td>0.169427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11.872200</td>\n",
       "      <td>8.995544</td>\n",
       "      <td>0.144529</td>\n",
       "      <td>0.012681</td>\n",
       "      <td>0.130235</td>\n",
       "      <td>0.130386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.897400</td>\n",
       "      <td>6.531295</td>\n",
       "      <td>0.163005</td>\n",
       "      <td>0.017603</td>\n",
       "      <td>0.140838</td>\n",
       "      <td>0.140959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.007700</td>\n",
       "      <td>6.324940</td>\n",
       "      <td>0.166332</td>\n",
       "      <td>0.021464</td>\n",
       "      <td>0.145510</td>\n",
       "      <td>0.145547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.846300</td>\n",
       "      <td>6.263951</td>\n",
       "      <td>0.169062</td>\n",
       "      <td>0.021727</td>\n",
       "      <td>0.148443</td>\n",
       "      <td>0.148551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sample pred: B:----), label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: B), label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: B:--- and willied. B: B:, label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: B:-style. B:, label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: B:-style. B:, label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best checkpoint manually: ./kaggle/working//results/summarization/prefix\\checkpoint-2305\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sample pred: Betty., label: Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: B: it., label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample generations: ['B: it.', \"Emma, but Lauren’s about Christmas. Lauren's Christmas. Lauren\", \"Iggy. Jackie's Jackie. Jackie. Jackie. Jackie. Jackie\", \"Marlakymied under the under the under the under the under the under the under Marla'st Marla under Marla's. Kiki underwear. Marla. Marla's under the under Kiki\", 'Robert.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed and saved to ./kaggle/working/models/summarization/prefix\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: PREFIX_ABLATED_NO_PROJ on CLASSIFICATION\n",
      "============================================================\n",
      "\n",
      "trainable params: 122,880 || all params: 77,084,032 || trainable%: 0.1594\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6315' max='6315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6315/6315 49:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.761100</td>\n",
       "      <td>4.781622</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.594500</td>\n",
       "      <td>4.611721</td>\n",
       "      <td>0.869213</td>\n",
       "      <td>0.869161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.558200</td>\n",
       "      <td>4.592362</td>\n",
       "      <td>0.864583</td>\n",
       "      <td>0.864442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sample pred: positive, label: positive\n",
      "INFO:__main__:Sample pred: positive, label: positive\n",
      "INFO:__main__:Sample pred: positive, label: positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best checkpoint manually: ./kaggle/working//results/classification/prefix_ablated_no_proj\\checkpoint-6315\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sample pred: negative, label: positive\n",
      "INFO:__main__:Sample pred: positive, label: positive\n",
      "INFO:__main__:Sample generations: ['positive', 'negative', 'positive', 'positive', 'negative']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed and saved to ./kaggle/working/models/classification/prefix_ablated_no_proj\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: PREFIX_ABLATED_NO_PROJ on SUMMARIZATION\n",
      "============================================================\n",
      "\n",
      "trainable params: 122,880 || all params: 77,084,032 || trainable%: 0.1594\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2305' max='2305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2305/2305 28:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.739900</td>\n",
       "      <td>6.154093</td>\n",
       "      <td>0.165298</td>\n",
       "      <td>0.019744</td>\n",
       "      <td>0.142535</td>\n",
       "      <td>0.142718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.704600</td>\n",
       "      <td>5.452825</td>\n",
       "      <td>0.201278</td>\n",
       "      <td>0.045582</td>\n",
       "      <td>0.172744</td>\n",
       "      <td>0.172568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.436300</td>\n",
       "      <td>5.307314</td>\n",
       "      <td>0.211396</td>\n",
       "      <td>0.054468</td>\n",
       "      <td>0.181068</td>\n",
       "      <td>0.181004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.312200</td>\n",
       "      <td>5.281513</td>\n",
       "      <td>0.219109</td>\n",
       "      <td>0.058132</td>\n",
       "      <td>0.183499</td>\n",
       "      <td>0.183593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.273200</td>\n",
       "      <td>5.245189</td>\n",
       "      <td>0.219610</td>\n",
       "      <td>0.058110</td>\n",
       "      <td>0.185768</td>\n",
       "      <td>0.185807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sample pred: B:-and agen, label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: B:-toothed to get he isn the dog., label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: B:-and    ., label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: B: B:-siderac ., label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: B: B:-siderac a a ., label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best checkpoint manually: ./kaggle/working//results/summarization/prefix_ablated_no_proj\\checkpoint-2305\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sample pred: Betty Larry called Larry called Larry called Larry called Larry called Larry called Larry called Larry called Larry called Larry called Larry called Betty., label: Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: B: B:), label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample generations: ['B: B:)', 'Emma.', 'Jackie is pregnant Iggy felt immature.', \"Marla doesn's Kiki found under her under her under her under her under her under Marla found tries to look likes under her bed underwear.\", 'Robert:']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed and saved to ./kaggle/working/models/summarization/prefix_ablated_no_proj\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MAIN TRAINING LOOP \n",
    "base_methods = [\"prefix\"]\n",
    "ablation_methods = [\"prefix_ablated_no_proj\"] if RUN_ABLATIONS else []\n",
    "methods_to_run = base_methods + ablation_methods\n",
    "tasks = {\n",
    "    \"classification\": (tokenized_classification, compute_classification_metrics),\n",
    "    \"summarization\": (tokenized_summarization, compute_summarization_metrics)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "os.makedirs(f\"{OUTPUT_DIR}/results\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/plots\", exist_ok=True) \n",
    "\n",
    "for method_name in methods_to_run:\n",
    "    for task_name, (dataset, compute_metrics) in tasks.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EXPERIMENT: {method_name.upper()} on {task_name.upper()}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        try:\n",
    "            config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "            use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                config=config,\n",
    "                torch_dtype=torch.bfloat16 if use_bf16 else torch.float32,\n",
    "            )\n",
    "\n",
    "            model.to(device)\n",
    "            \n",
    "            # Note: t5-small has correct dims (num_heads=8, head_dim=64); PEFT handles DynamicCache natively.\n",
    "            # Create PEFT configs dynamically from model.config\n",
    "            d_model = model.config.d_model\n",
    "            num_heads = model.config.num_heads\n",
    "            total_layers = model.config.num_layers\n",
    "            effective_token_dim = num_heads * model.config.d_kv\n",
    "            peft_configs_local = {\n",
    "                \"prefix\": PrefixTuningConfig(\n",
    "                    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "                    inference_mode=False,\n",
    "                    num_virtual_tokens=NUM_VIRTUAL_TOKENS,\n",
    "                    token_dim=effective_token_dim,\n",
    "                    num_transformer_submodules=2,\n",
    "                    num_attention_heads=num_heads,\n",
    "                    num_layers=total_layers,\n",
    "                    encoder_hidden_size=d_model,\n",
    "                    prefix_projection=False  # Baseline with projection\n",
    "                ),\n",
    "                \"prefix_ablated_no_proj\": PrefixTuningConfig(  # Ablation: Remove projection layer\n",
    "                    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "                    inference_mode=False,\n",
    "                    num_virtual_tokens=NUM_VIRTUAL_TOKENS,\n",
    "                    token_dim=effective_token_dim,\n",
    "                    num_transformer_submodules=2,\n",
    "                    num_attention_heads=num_heads,\n",
    "                    num_layers=total_layers,\n",
    "                    encoder_hidden_size=d_model,\n",
    "                    prefix_projection=False  # Ablated\n",
    "                )\n",
    "            }\n",
    "            model = get_peft_model(model, peft_configs_local[method_name])\n",
    "            model.print_trainable_parameters()\n",
    "            \n",
    "            training_args = get_training_args(method_name, task_name)\n",
    "            data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "            trainer = CustomTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=dataset[\"train\"],\n",
    "                eval_dataset=dataset[\"validation\"],\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_metrics,\n",
    "                tokenizer=tokenizer\n",
    "            )\n",
    "\n",
    "            print(\"Training...\")\n",
    "            train_result = trainer.train()\n",
    "            \n",
    "            # Manual load best model for prefix/prompt methods\n",
    "            if not training_args.load_best_model_at_end and trainer.state.best_model_checkpoint:\n",
    "                print(f\"Loading best checkpoint manually: {trainer.state.best_model_checkpoint}\")\n",
    "                base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    MODEL_NAME,\n",
    "                    config=config,\n",
    "                    torch_dtype=torch.bfloat16 if use_bf16 else torch.float32,\n",
    "                )\n",
    "                base_model.to(device)\n",
    "                model = PeftModel.from_pretrained(base_model, trainer.state.best_model_checkpoint)\n",
    "                trainer.model = model\n",
    "                model.to(device)  # Ensure the full PEFT model is on device\n",
    "            \n",
    "            print(\"Evaluating...\")\n",
    "            test_dataset = dataset.get(\"test\", dataset[\"validation\"])\n",
    "            gen_kwargs = {\n",
    "                # CHANGE: For classification, max_length=5; summ=128; num_beams=6 - Why: Short for classification enforces concise labels (fixes verbose outputs/low acc); more beams improves quality (fixes poor ROUGE)\n",
    "                \"max_length\": 5 if task_name == \"classification\" else 128,\n",
    "                \"num_beams\": 1 if task_name == \"classification\" else 4,\n",
    "                \"repetition_penalty\": 1.0 if task_name==\"classification\" else 2.0,\n",
    "                \"length_penalty\": 0.0 if task_name == \"classification\" else 1.0,\n",
    "                \"early_stopping\": True,\n",
    "                \"do_sample\": True if task_name == \"classification\" else False,\n",
    "                \"top_p\": 0.95,\n",
    "                \"temperature\": 0.7\n",
    "            }\n",
    "            \n",
    "            # Set generation kwargs for trainer.evaluate\n",
    "            training_args.generation_max_length = gen_kwargs[\"max_length\"]\n",
    "            training_args.generation_num_beams = gen_kwargs[\"num_beams\"]\n",
    "            test_metrics = trainer.evaluate(test_dataset)\n",
    "            # CHANGE: Added trainer.predict for sample logging post-eval - Why: Debugs generations, fixing empty/low metrics\n",
    "            predictions = trainer.predict(dataset[\"validation\"])\n",
    "            # CHANGE: Clean predictions before decoding - Why: Handles -100/invalid IDs, fixing OverflowError in batch_decode\n",
    "            cleaned_predictions = np.where(predictions.predictions != -100, predictions.predictions, tokenizer.pad_token_id)\n",
    "            cleaned_predictions = np.clip(cleaned_predictions, 0, tokenizer.vocab_size - 1)\n",
    "            logger.info(f\"Sample generations: {tokenizer.batch_decode(cleaned_predictions[:5], skip_special_tokens=True)}\")\n",
    "            exp_name = f\"{method_name}_{task_name}\"\n",
    "            trainable = model.num_parameters(only_trainable=True) if hasattr(model, 'num_parameters') else sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            total = model.num_parameters() if hasattr(model, 'num_parameters') else sum(p.numel() for p in model.parameters())\n",
    "            \n",
    "            results[exp_name] = {\n",
    "                \"train_metrics\": train_result.metrics,\n",
    "                \"test_metrics\": test_metrics,\n",
    "                \"trainable_params\": trainable,\n",
    "                \"total_params\": total,\n",
    "                \"log_history\": trainer.state.log_history # Collect for plotting\n",
    "            }\n",
    "            \n",
    "            save_path = f\"{OUTPUT_DIR}models/{task_name}/{method_name}\"\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            trainer.save_model(save_path)\n",
    "            print(f\"Completed and saved to {save_path}\\n\")\n",
    "            del model, trainer\n",
    "            safe_cleanup()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"ERROR in {method_name}_{task_name}: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            try:\n",
    "                del model, trainer\n",
    "            except:\n",
    "                pass\n",
    "            safe_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ALL EXPERIMENTS COMPLETED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL EXPERIMENTS COMPLETED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTS SUMMARY:\n",
      "============================================================\n",
      "\n",
      "PREFIX - Classification:\n",
      " Trainable: 0.00%\n",
      " Accuracy: 0.5863\n",
      " F1: 0.7392\n",
      "\n",
      "PREFIX - Summarization:\n",
      " Trainable: 0.00%\n",
      " ROUGE-1: 0.1740\n",
      " ROUGE-L: 0.1506\n",
      "\n",
      "PREFIX - Ablated_no_proj_classification:\n",
      " Trainable: 0.00%\n",
      " ROUGE-1: 0.0000\n",
      " ROUGE-L: 0.0000\n",
      "\n",
      "PREFIX - Ablated_no_proj_summarization:\n",
      " Trainable: 0.00%\n",
      " ROUGE-1: 0.2305\n",
      " ROUGE-L: 0.1965\n",
      "\n",
      "ABLATION DELTAS:\n",
      "Delta for PREFIX_ABLATED_NO_PROJ_CLASSIFICATION: {'eval_loss': 0.0, 'eval_accuracy': 0.0, 'eval_f1': 0.0, 'eval_runtime': 0.0, 'eval_samples_per_second': 0.0, 'eval_steps_per_second': 0.0}\n",
      "Delta for PREFIX_ABLATED_NO_PROJ_SUMMARIZATION: {'eval_loss': 0.0, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_runtime': 0.0, 'eval_samples_per_second': 0.0, 'eval_steps_per_second': 0.0}\n",
      "\n",
      "Generating learning curves...\n",
      "Learning curves saved to ./kaggle/working//plots\\prefix_classification_curves.png\n",
      "Learning curves saved to ./kaggle/working//plots\\prefix_summarization_curves.png\n",
      "Learning curves saved to ./kaggle/working//plots\\prefix_ablated_no_proj_classification_curves.png\n",
      "Learning curves saved to ./kaggle/working//plots\\prefix_ablated_no_proj_summarization_curves.png\n",
      "\n",
      "Generating ablation comparison plots...\n",
      "Ablation comparison plot saved to ./kaggle/working//plots\\ablation_comparison_classification.png\n",
      "Ablation comparison plot saved to ./kaggle/working//plots\\ablation_comparison_summarization.png\n",
      "\n",
      "Results saved to './kaggle/working//prefix_results.csv'\n",
      "Report saved to './kaggle/working//prefix_final_report.md' (includes plot links)\n",
      "\n",
      "OUTCOME INSIGHTS:\n",
      "- For Classification, PREFIX has the lowest trainable params (0.00%).\n",
      "- PREFIX achieves the highest ACCURACY score (0.5863) on Classification.\n",
      "- For Summarization, PREFIX has the lowest trainable params (0.00%).\n",
      "- PREFIX achieves the highest ROUGEL score (0.1965) on Summarization.\n",
      "- Ablation in PREFIX_ABLATED_NO_PROJ_CLASSIFICATION leads to no change in performance (delta: 0.0000).\n",
      "- Ablation in PREFIX_ABLATED_NO_PROJ_SUMMARIZATION leads to no change in performance (delta: 0.0000).\n",
      "View plots in ./kaggle/working//plots/ for detailed curves (loss/metric vs step) and comparisons.\n",
      "\n",
      "============================================================\n",
      "SUCCESS - Prefix-Tuning method completed! With ablations!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "if results:\n",
    "    print(\"\\nRESULTS SUMMARY:\")\n",
    "    print(\"=\"*60)\n",
    "    for exp_name, exp_data in results.items():\n",
    "        # Handle cases where task name might have underscores\n",
    "        method_task_split = exp_name.split('_', 1)\n",
    "        method = method_task_split[0]\n",
    "        task = method_task_split[1] if len(method_task_split) > 1 else 'unknown'\n",
    "        \n",
    "        metrics = exp_data[\"test_metrics\"]\n",
    "        pct = 100 * exp_data[\"trainable_params\"] / exp_data[\"total_params\"]\n",
    "        print(f\"\\n{method.upper()} - {task.capitalize()}:\")\n",
    "        print(f\" Trainable: {pct:.2f}%\")\n",
    "        if task == \"classification\":\n",
    "            print(f\" Accuracy: {metrics.get('eval_accuracy', 0):.4f}\")\n",
    "            print(f\" F1: {metrics.get('eval_f1', 0):.4f}\")\n",
    "        else:\n",
    "            print(f\" ROUGE-1: {metrics.get('eval_rouge1', 0):.4f}\")\n",
    "            print(f\" ROUGE-L: {metrics.get('eval_rougeL', 0):.4f}\")\n",
    "\n",
    "    # Ablation deltas if enabled\n",
    "    if RUN_ABLATIONS:\n",
    "        print(\"\\nABLATION DELTAS:\")\n",
    "        for exp_name, exp_data in results.items():\n",
    "            if \"_ablated_\" in exp_name:\n",
    "                method_task_split = exp_name.split('_ablated_')[0]\n",
    "                task = exp_name.split('_', 1)[1] # Get task name\n",
    "                base_method_name = f\"{method_task_split}_{task}\"\n",
    "                \n",
    "                if base_method_name in results:\n",
    "                    base_metrics = results[base_method_name][\"test_metrics\"]\n",
    "                    delta = {k: exp_data[\"test_metrics\"].get(k, 0) - base_metrics.get(k, 0) for k in base_metrics if \"eval_\" in k}\n",
    "                    print(f\"Delta for {exp_name.upper()}: {delta}\")\n",
    "\n",
    "    # Plot learning curves for each experiment\n",
    "    print(\"\\nGenerating learning curves...\")\n",
    "    plot_paths = {}\n",
    "    plot_save_dir = f\"{OUTPUT_DIR}/plots\" # [FIX] Define plot save dir\n",
    "    for exp_name, exp_data in results.items():\n",
    "        task_name = exp_name.split(\"_\", 1)[1]\n",
    "        # [FIX] Pass the correct save_dir to the plotting function\n",
    "        plot_path = plot_learning_curves(exp_data[\"log_history\"], exp_name, task_name, save_dir=plot_save_dir)\n",
    "        plot_paths[exp_name] = plot_path\n",
    "    \n",
    "    # Graphical ablation comparisons per task\n",
    "    ablation_plot_paths = {}\n",
    "    if RUN_ABLATIONS:\n",
    "        print(\"\\nGenerating ablation comparison plots...\")\n",
    "        for task_name in tasks.keys():\n",
    "            task_results = {k: v for k, v in results.items() if k.endswith(f\"_{task_name}\")}\n",
    "            if task_results:\n",
    "                # [FIX] Pass the correct save_dir to the plotting function\n",
    "                ablation_plot_path = plot_ablation_comparisons(task_results, task_name, save_dir=plot_save_dir)\n",
    "                if ablation_plot_path:\n",
    "                    ablation_plot_paths[task_name] = ablation_plot_path\n",
    "\n",
    "    # --- Results DataFrame ---\n",
    "    results_df = []\n",
    "    for exp_name, exp_data in results.items():\n",
    "        method, task = exp_name.split(\"_\", 1)\n",
    "        results_df.append({\n",
    "            \"Method\": method.upper(),\n",
    "            \"Task\": task.capitalize(),\n",
    "            \"Trainable %\": 100 * exp_data[\"trainable_params\"] / exp_data[\"total_params\"],\n",
    "            **{k: v for k, v in exp_data[\"test_metrics\"].items() if isinstance(v, (int, float))}\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results_df)\n",
    "    cols = [\"Method\", \"Task\", \"Trainable %\"]\n",
    "    metric_cols = [c for c in df.columns if c.startswith(\"eval_\")]\n",
    "    cols.extend(sorted(metric_cols))\n",
    "    df = df[cols]\n",
    "    df.to_csv(f\"{OUTPUT_DIR}/prefix_results.csv\", index=False)\n",
    "    print(f\"\\nResults saved to '{OUTPUT_DIR}/prefix_results.csv'\")\n",
    "    \n",
    "    # --- Final Report --- \n",
    "    # Use relative paths for plots in the markdown report\n",
    "    report_path = f\"{OUTPUT_DIR}/prefix_final_report.md\"\n",
    "    report_dir = os.path.dirname(report_path)\n",
    "\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(f\"# Prefix-Tuning Adaptation Results - T5-small\\n\\n\")\n",
    "        f.write(f\"## Configuration\\n\")\n",
    "        f.write(f\"- Model: {MODEL_NAME} (switched from flan-t5-small to fix config dim bug)\\n\")\n",
    "        f.write(f\"- Dataset Size: {DATASET_SIZE}\\n\")\n",
    "        f.write(f\"- Methods: Prefix-Tuning\\n\")\n",
    "        if RUN_ABLATIONS:\n",
    "            f.write(f\"- Ablations: Enabled (including ablated variants); prefix ablation removes projection layer\\n\")\n",
    "        f.write(f\"- Special: Native DynamicCache support; correct dims (num_heads=8, head_dim=64)\\n\\n\")\n",
    "        f.write(f\"## Summary Table\\n\\n\")\n",
    "        f.write(df.to_markdown(index=False))\n",
    "        f.write(\"\\n\\n## Learning Curves\\n\")\n",
    "        for exp_name, plot_path in plot_paths.items():\n",
    "            relative_plot_path = os.path.relpath(plot_path, start=report_dir)\n",
    "            f.write(f\"- [{exp_name}]({relative_plot_path})\\n\")\n",
    "        if RUN_ABLATIONS and ablation_plot_paths:\n",
    "            f.write(\"\\n## Ablation Comparisons\\n\")\n",
    "            for task_name, plot_path in ablation_plot_paths.items():\n",
    "                relative_plot_path = os.path.relpath(plot_path, start=report_dir)\n",
    "                f.write(f\"- [{task_name.capitalize()} Ablation Comparison]({relative_plot_path})\\n\")\n",
    "    \n",
    "    print(f\"Report saved to '{report_path}' (includes plot links)\")\n",
    "\n",
    "    # Generate dynamic outcome insights based on results\n",
    "    print(\"\\nOUTCOME INSIGHTS:\")\n",
    "    if results:\n",
    "        # General insights from trainable params and metrics\n",
    "        for task in tasks.keys():\n",
    "            task_exps = {k: v for k, v in results.items() if k.endswith(task)}\n",
    "            if task_exps:\n",
    "                # Find method with lowest trainable %\n",
    "                min_trainable_method = min(task_exps, key=lambda k: 100 * task_exps[k][\"trainable_params\"] / task_exps[k][\"total_params\"])\n",
    "                min_pct = 100 * task_exps[min_trainable_method][\"trainable_params\"] / task_exps[min_trainable_method][\"total_params\"]\n",
    "                print(f\"- For {task.capitalize()}, {min_trainable_method.split('_')[0].upper()} has the lowest trainable params ({min_pct:.2f}%).\")\n",
    "                \n",
    "                # Find best performing method (use key metric)\n",
    "                key_metric = 'eval_accuracy' if task == 'classification' else 'eval_rougeL'\n",
    "                best_method = max(task_exps, key=lambda k: task_exps[k][\"test_metrics\"].get(key_metric, 0))\n",
    "                best_score = task_exps[best_method][\"test_metrics\"].get(key_metric, 0)\n",
    "                print(f\"- {best_method.split('_')[0].upper()} achieves the highest {key_metric.replace('eval_', '').upper()} score ({best_score:.4f}) on {task.capitalize()}.\")\n",
    "        \n",
    "        # Ablation-specific insights\n",
    "        if RUN_ABLATIONS:\n",
    "            for exp_name, exp_data in results.items():\n",
    "                if \"_ablated_\" in exp_name:\n",
    "                    method_task_split = exp_name.split('_ablated_')[0]\n",
    "                    task = exp_name.split('_', 1)[1] # Get task name\n",
    "                    base_method_name = f\"{method_task_split}_{task}\"\n",
    "                    \n",
    "                    if base_method_name in results:\n",
    "                        base_metrics = results[base_method_name][\"test_metrics\"]\n",
    "                        delta = {k: exp_data[\"test_metrics\"].get(k, 0) - base_metrics.get(k, 0) for k in base_metrics if \"eval_\" in k}\n",
    "                        key_delta = delta.get('eval_accuracy' if task == 'classification' else 'eval_rougeL', 0)\n",
    "                        impact = \"degradation\" if key_delta < 0 else \"improvement\" if key_delta > 0 else \"no change\"\n",
    "                        print(f\"- Ablation in {exp_name.upper()} leads to {impact} in performance (delta: {key_delta:.4f}).\")\n",
    "        \n",
    "        print(f\"View plots in {OUTPUT_DIR}/plots/ for detailed curves (loss/metric vs step) and comparisons.\")\n",
    "else:\n",
    "    print(\"\\nNo results were generated. Check the training loop for errors.\")\n",
    "\n",
    "# In[15]:\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUCCESS - Prefix-Tuning method completed!\" + (\" With ablations!\" if RUN_ABLATIONS else \"\"))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DL Virtual Environment",
   "language": "python",
   "name": "dl-virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
