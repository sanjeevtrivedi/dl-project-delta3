{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2cad601-500c-4c56-8073-7cb351dccdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "851260ea-111b-4ae4-8108-8a391a4887e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Kaggle: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# KAGGLE TOGGLE \n",
    "IS_KAGGLE = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', None) is not None  # Detect if running on Kaggle\n",
    "print(f\"Running on Kaggle: {IS_KAGGLE}\")\n",
    "if IS_KAGGLE:\n",
    "    os.system('pip install -r /kaggle/input/requirements/requirements.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "599dde30-eb05-435d-9500-c188d891303a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harsh\\Programs\\python_envs\\dl-virtualenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import argparse, json, math, os, random, re, time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, evaluate, torch\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    ")\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    PrefixTuningConfig,\n",
    "    PromptTuningConfig,\n",
    "    PromptTuningInit,\n",
    "    TaskType,\n",
    ")\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "import json  # For saving log_history if needed\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d111690d-42ed-42c3-a242-010d0eeb0335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# DEVICE DETECTION\n",
    "DEVICE = (\n",
    "    torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if IS_KAGGLE\n",
    "    else (\n",
    "        torch.device(\"mps\")\n",
    "        if torch.backends.mps.is_available()\n",
    "        else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    )\n",
    ")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# CONFIGURATION\n",
    "MODEL_NAME = \"t5-small\"  # Switched from flan-t5-small to avoid config dim bug (num_heads=6 mismatch)\n",
    "SUMMARIZATION_DATASET = \"knkarthick/samsum\"\n",
    "\n",
    "BENCHAMARK_GLUE = \"glue\"\n",
    "GLUE_DATASET_TASK_SC = \"sst2\"  # SST-2 for sentiment classification\n",
    "\n",
    "DATASET_SIZE = 'full'  # or 'full'\n",
    "RUN_ABLATIONS = False  # Toggle to enable/disable ablation study (modular flag)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "NUM_VIRTUAL_TOKENS = 20  # For truncation safety\n",
    "MAX_POS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f628c66-5721-429b-8e4f-6d0396ba916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d3aad66-3fa7-4b5f-a496-6af1baa15a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU:  1\n",
      "GPU Name:  NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da17e970-d7b2-4e73-890d-83c47c19432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50308b47-4277-484e-b967-1a68d17cc44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945fbf43-fc4c-49c4-b58d-1323089aac4c",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe33585b-9950-433c-95ff-574f36f29b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def limit_dataset_size(dataset, size):\n",
    "    if size == 'full':\n",
    "        return dataset\n",
    "    if isinstance(size, int) and size > 0:\n",
    "        return dataset.select(range(min(size, len(dataset))))\n",
    "    raise ValueError(f\"Invalid size: {size}\")\n",
    "\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def count_trainable_parameters(model) -> Tuple[int, int]:\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total\n",
    "\n",
    "\n",
    "def safe_cleanup():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    elif device.type == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "def readable(n: int) -> str:\n",
    "    if n >= 1e9:\n",
    "        return f\"{n/1e9:.2f}B\"\n",
    "    if n >= 1e6:\n",
    "        return f\"{n/1e6:.2f}M\"\n",
    "    if n >= 1e3:\n",
    "        return f\"{n/1e3:.1f}K\"\n",
    "    return str(n)\n",
    "\n",
    "\n",
    "def decode(tokenizer, ids) -> str:\n",
    "    ids = [int(i) for i in ids if i >= 0]\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dcb9f6-1bfc-401e-aaac-5c922abece2a",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e50efe94-a23e-47ba-a70d-3f80742d202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SST-2 text-to-text label mapping\n",
    "SST2_LABELS = [\"negative\", \"positive\"]\n",
    "\n",
    "def prepare_sst2(\n",
    "    tokenizer,\n",
    "    max_source_len=256,\n",
    "    max_target_len=8,\n",
    "    train_size='full',\n",
    "    eval_size='full',\n",
    "):\n",
    "    # Build prompts/targets\n",
    "    def build_prompt_target_sst2(ex):\n",
    "        label_txt = \"positive\" if ex[\"label\"] == 1 else \"negative\"\n",
    "        return {\n",
    "            \"prompt\": f\"sst2: sentence: {ex['sentence']}\\nSentiment:\",\n",
    "            \"target\": label_txt,\n",
    "        }\n",
    "\n",
    "    ds = load_dataset(BENCHAMARK_GLUE, GLUE_DATASET_TASK_SC)\n",
    "    ds = ds.map(build_prompt_target_sst2)\n",
    "\n",
    "    ds[\"train\"] = limit_dataset_size(ds[\"train\"], train_size)\n",
    "    ds[\"validation\"] = limit_dataset_size(ds[\"validation\"], eval_size)\n",
    "    eval_split = \"validation\"\n",
    "\n",
    "    def tok_fn(batch):\n",
    "        inputs = tokenizer(\n",
    "            batch[\"prompt\"],\n",
    "            max_length=max_source_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                batch[\"target\"],\n",
    "                max_length=max_target_len,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "            )\n",
    "        inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return inputs\n",
    "\n",
    "    tokenized = ds.map(tok_fn, batched=True, remove_columns=ds[\"train\"].column_names)\n",
    "    return tokenized, eval_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aea679ac-ef63-41f2-8676-896dd0bed101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_samsum(\n",
    "    tokenizer,\n",
    "    max_source_len=768,\n",
    "    max_target_len=128,\n",
    "    train_size='full',\n",
    "    eval_size='full',\n",
    "):\n",
    "    def build_prompt_target_samsum(ex):\n",
    "        return {\n",
    "            \"prompt\": f\"summarize: {ex['dialogue']}\",\n",
    "            \"target\": ex[\"summary\"],\n",
    "        }\n",
    "\n",
    "    ds = load_dataset(SUMMARIZATION_DATASET)\n",
    "    ds = ds.map(build_prompt_target_samsum)\n",
    "\n",
    "    ds[\"train\"] = limit_dataset_size(ds[\"train\"], train_size)\n",
    "    ds[\"validation\"] = limit_dataset_size(ds[\"validation\"], eval_size)\n",
    "    eval_split = \"validation\"\n",
    "\n",
    "    def tok_fn(batch):\n",
    "        inputs = tokenizer(\n",
    "            batch[\"prompt\"],\n",
    "            max_length=max_source_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                batch[\"target\"],\n",
    "                max_length=max_target_len,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "            )\n",
    "        inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return inputs\n",
    "\n",
    "    tokenized = ds.map(tok_fn, batched=True, remove_columns=ds[\"train\"].column_names)\n",
    "    return tokenized, eval_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84606e9c-9d59-4822-9016-f980ce3c8cb8",
   "metadata": {},
   "source": [
    "### PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60059afe-8a27-4b10-a154-29f5531ab4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_prompt_tuning(model, tokenizer_name: str, num_virtual_tokens=20, init_text: str | None = None):\n",
    "    cfg = PromptTuningConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        num_virtual_tokens=num_virtual_tokens,\n",
    "        prompt_tuning_init=PromptTuningInit.TEXT if init_text else PromptTuningInit.RANDOM,\n",
    "        prompt_tuning_init_text=init_text,\n",
    "        tokenizer_name_or_path=tokenizer_name,\n",
    "    )\n",
    "    return get_peft_model(model, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f98d142-afc9-4f55-9605-9b1553bbe39b",
   "metadata": {},
   "source": [
    "### Metrics & Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97e8ce40-be65-40c6-bfff-328793757495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sst2_parse_label(text: str) -> str:\n",
    "    t = text.lower()\n",
    "    if \"positive\" in t and \"negative\" not in t: return \"positive\"\n",
    "    if \"negative\" in t and \"positive\" not in t: return \"negative\"\n",
    "    if t.startswith(\"pos\"): return \"positive\"\n",
    "    if t.startswith(\"neg\"): return \"negative\"\n",
    "    # fallback neutral heuristic\n",
    "    return \"positive\" if \"pos\" in t else \"negative\"\n",
    "\n",
    "def compute_sst2_metrics(preds_text: List[str], refs_text: List[str]) -> Dict[str, float]:\n",
    "    # Map labels to 0/1\n",
    "    pred = [1 if sst2_parse_label(p) == \"positive\" else 0 for p in preds_text]\n",
    "    ref = [1 if ((\"positive\" in r.lower()) or (\"pos\" in r.lower())) else 0 for r in refs_text]\n",
    "    acc = accuracy_score(ref, pred)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(ref, pred, average=\"binary\", zero_division=0)\n",
    "    return {\"accuracy\": float(acc), \"precision\": float(pr), \"recall\": float(rc), \"f1\": float(f1)}, ref, pred\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, out_path: Path, labels=(\"negative\", \"positive\"), title=\"Confusion Matrix (SST-2)\"):\n",
    "    from itertools import product\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    ticks = np.arange(len(labels))\n",
    "    plt.xticks(ticks, labels, rotation=45)\n",
    "    plt.yticks(ticks, labels)\n",
    "    thresh = cm.max() / 2.0 if cm.max() > 0 else 0.5\n",
    "    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], \"d\"), ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_loss_curve(trainer, out_path: Path):\n",
    "    logs = [e for e in trainer.state.log_history if \"loss\" in e]\n",
    "    if not logs: return\n",
    "    steps = [e.get(\"step\", i) for i, e in enumerate(logs)]\n",
    "    losses = [e[\"loss\"] for e in logs]\n",
    "    fig = plt.figure()\n",
    "    plt.plot(steps, losses)\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Training loss\")\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def compute_summarization_metrics(preds: List[str], refs: List[str]) -> Dict[str, float]:\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "    chrf = evaluate.load(\"chrf\")\n",
    "\n",
    "    rouge_scores = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "    sacrebleu_score = sacrebleu.compute(predictions=preds, references=[[r] for r in refs])[\"score\"]\n",
    "    chrf_score = chrf.compute(predictions=preds, references=refs)[\"score\"]\n",
    "\n",
    "    gen_lens = np.array([len(p.split()) for p in preds], dtype=float)\n",
    "    ref_lens = np.array([len(r.split()) for r in refs], dtype=float)\n",
    "    length_ratio = float(np.mean(gen_lens / np.maximum(ref_lens, 1)))\n",
    "    return {\n",
    "        \"rouge1\": float(rouge_scores[\"rouge1\"]),\n",
    "        \"rouge2\": float(rouge_scores[\"rouge2\"]),\n",
    "        \"rougeL\": float(rouge_scores[\"rougeL\"]),\n",
    "        \"rougeLsum\": float(rouge_scores.get(\"rougeLsum\", rouge_scores[\"rougeL\"])),\n",
    "        \"sacrebleu\": float(sacrebleu_score),\n",
    "        \"chrf\": float(chrf_score),\n",
    "        \"avg_gen_len\": float(np.mean(gen_lens)),\n",
    "        \"avg_ref_len\": float(np.mean(ref_lens)),\n",
    "        \"length_ratio\": length_ratio,\n",
    "    }\n",
    "\n",
    "def plot_length_hist(lengths, out_path: Path, title=\"Generated Summary Lengths (words)\"):\n",
    "    fig = plt.figure()\n",
    "    plt.hist(lengths, bins=30)\n",
    "    plt.xlabel(\"Length (words)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_ratio_hist(ratios, out_path: Path, title=\"Generated/Reference Length Ratio\"):\n",
    "    fig = plt.figure()\n",
    "    plt.hist(ratios, bins=30)\n",
    "    plt.xlabel(\"Length Ratio\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_scatter(x, y, out_path: Path, xlabel: str, ylabel: str, title: str):\n",
    "    fig = plt.figure()\n",
    "    plt.scatter(x, y, alpha=0.5)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f484ca20-9536-4bc9-a486-c2e087861162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1ed6cd6-c58a-4299-8be6-4bdadb5f60c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RunConfig:\n",
    "    task: str               # \"samsum\" or \"sst2\"\n",
    "    method: str             # \"prompt_tuning\", \"lora\", \"full_finetune\"\n",
    "    # Models: summarization must use knkarthick/samsum; classification default t5-small\n",
    "    model_name: str | None = None\n",
    "    epochs: int = 1\n",
    "    batch_size: int = 8\n",
    "    lr: float = 5e-4\n",
    "    weight_decay: float = 0.0\n",
    "    warmup_ratio: float = 0.03\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    max_source_len: int = 768\n",
    "    max_target_len: int = 128\n",
    "    gen_max_new_tokens: int = 128\n",
    "    prompt_tokens: int = 20\n",
    "    prompt_init_text: str | None = None\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    max_train_samples: int | None = None\n",
    "    seed: int = 42\n",
    "    train_size: int|'full' = 'full'\n",
    "    eval_size: int|'full' = 'full'\n",
    "    output_root: Path = Path(\"outputs_two_tasks\")\n",
    "\n",
    "def build_model_and_tokenizer(cfg: RunConfig) -> Tuple[T5ForConditionalGeneration, T5TokenizerFast]:\n",
    "    # Choose model per task if not specified\n",
    "    model_name = cfg.model_name\n",
    "    if cfg.task == \"samsum\" and model_name is None:\n",
    "        model_name = \"knkarthick/samsum\"\n",
    "    if cfg.task == \"sst2\" and model_name is None:\n",
    "        model_name = \"t5-small\"\n",
    "\n",
    "    tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name).to(DEVICE)\n",
    "\n",
    "    if cfg.method == \"prompt_tuning\":\n",
    "        model = apply_prompt_tuning(model, tokenizer_name=model_name,\n",
    "                                    num_virtual_tokens=cfg.prompt_tokens,\n",
    "                                    init_text=cfg.prompt_init_text)\n",
    "    elif cfg.method == \"lora\":\n",
    "        model = apply_lora(model, r=cfg.lora_r, alpha=cfg.lora_alpha, dropout=cfg.lora_dropout)\n",
    "    elif cfg.method == \"full_finetune\":\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "    else:\n",
    "        raise ValueError(\"Unknown method: \" + cfg.method)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def train_and_eval(cfg: RunConfig) -> Dict[str, float]:\n",
    "    set_seed(cfg.seed)\n",
    "    model, tokenizer = build_model_and_tokenizer(cfg)\n",
    "\n",
    "    # Data\n",
    "    if cfg.task == \"samsum\":\n",
    "        tokenized, eval_split = prepare_samsum(\n",
    "        tokenizer,\n",
    "        max_source_len=cfg.max_source_len,\n",
    "        max_target_len=cfg.max_target_len,\n",
    "        train_size=cfg.train_size,\n",
    "        eval_size=cfg.eval_size,\n",
    "    )\n",
    "    elif cfg.task == \"sst2\":\n",
    "        tokenized, eval_split = prepare_sst2(\n",
    "        tokenizer,\n",
    "        max_source_len=256,\n",
    "        max_target_len=8,\n",
    "        train_size=cfg.train_size,\n",
    "        eval_size=cfg.eval_size,\n",
    "    )\n",
    "    else:\n",
    "        raise ValueError(\"task must be 'samsum' or 'sst2'\")\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "    out_dir = cfg.output_root / f\"{cfg.task}__{cfg.method}__seed{cfg.seed}\"\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(out_dir),\n",
    "        per_device_train_batch_size=cfg.batch_size,\n",
    "        per_device_eval_batch_size=max(1, cfg.batch_size // 2),\n",
    "        num_train_epochs=cfg.epochs,\n",
    "        learning_rate=cfg.lr,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "        warmup_ratio=cfg.warmup_ratio,\n",
    "        gradient_accumulation_steps=cfg.gradient_accumulation_steps,\n",
    "        logging_steps=20,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=cfg.gen_max_new_tokens,\n",
    "        report_to=[],\n",
    "        bf16=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized[\"train\"],\n",
    "        eval_dataset=tokenized[eval_split],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # Parameter stats\n",
    "    trainable, total = count_trainable_parameters(model)\n",
    "    stats = {\n",
    "        \"task\": cfg.task,\n",
    "        \"method\": cfg.method,\n",
    "        \"model_name\": \"t5-small\",\n",
    "        \"seed\": cfg.seed,\n",
    "        \"device\": DEVICE.type,\n",
    "        \"trainable_params\": int(trainable),\n",
    "        \"total_params\": int(total),\n",
    "        \"trainable_readable\": readable(trainable),\n",
    "        \"total_readable\": readable(total),\n",
    "    }\n",
    "\n",
    "    # Train\n",
    "    start = time.time()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    trainer.train()\n",
    "    elapsed = time.time() - start\n",
    "    stats[\"train_time_sec\"] = float(elapsed)\n",
    "    if torch.cuda.is_available():\n",
    "        stats[\"peak_gpu_mem_bytes\"] = int(torch.cuda.max_memory_allocated())\n",
    "        stats[\"peak_gpu_mem_gb\"] = float(stats[\"peak_gpu_mem_bytes\"] / (1024 ** 3))\n",
    "    else:\n",
    "        stats[\"peak_gpu_mem_bytes\"] = 0\n",
    "        stats[\"peak_gpu_mem_gb\"] = 0.0\n",
    "\n",
    "    # Predictions\n",
    "    pred_output = trainer.predict(tokenized[eval_split])\n",
    "    preds_ids = pred_output.predictions[0] if isinstance(pred_output.predictions, tuple) else pred_output.predictions\n",
    "    decoded_preds = [decode(tokenizer, seq) for seq in preds_ids]\n",
    "    decoded_refs = [decode(tokenizer, np.array(row[\"labels\"])) for row in tokenized[eval_split]]\n",
    "\n",
    "    # Task-specific metrics & plots\n",
    "    if cfg.task == \"sst2\":\n",
    "        metrics, y_true, y_pred = compute_sst2_metrics(decoded_preds, decoded_refs)\n",
    "        stats.update(metrics)\n",
    "        # Plots\n",
    "        plot_loss_curve(trainer, out_dir / \"loss_curve.png\")\n",
    "        plot_confusion_matrix(y_true, y_pred, out_dir / \"confusion_matrix.png\")\n",
    "    else:\n",
    "        mets = compute_summarization_metrics(decoded_preds, decoded_refs)\n",
    "        stats.update(mets)\n",
    "        # Plots\n",
    "        plot_loss_curve(trainer, out_dir / \"loss_curve.png\")\n",
    "        gen_lens = np.array([len(p.split()) for p in decoded_preds], dtype=float)\n",
    "        ref_lens = np.array([len(r.split()) for r in decoded_refs], dtype=float)\n",
    "        ratios = gen_lens / np.maximum(ref_lens, 1)\n",
    "        plot_length_hist(gen_lens, out_dir / \"gen_length_hist.png\")\n",
    "        plot_ratio_hist(ratios, out_dir / \"length_ratio_hist.png\")\n",
    "        # ROUGE-Lsum vs length ratio (as an example diagnostic)\n",
    "        # Recompute per-example ROUGE-L if desired; for speed, we correlate length ratio with gen length\n",
    "        plot_scatter(gen_lens, ratios, out_dir / \"length_vs_ratio_scatter.png\",\n",
    "                     xlabel=\"Generated length (words)\", ylabel=\"Length ratio (gen/ref)\",\n",
    "                     title=\"Generated Length vs Length Ratio\")\n",
    "\n",
    "    print(stats)\n",
    "    # Save metrics JSON\n",
    "    with open(out_dir / \"metrics.json\", \"w\") as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2466e93-2784-4af5-b7c3-39be87384e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_comparison_plots(results: List[Dict], out_dir=Path(\"comparison_two_tasks\")):\n",
    "    ensure_dir(out_dir)\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(out_dir / \"summary_by_run.csv\", index=False)\n",
    "\n",
    "    # Per-task bar charts for primary metrics\n",
    "    # Summarization -> ROUGE-Lsum; Classification -> Accuracy\n",
    "    for task in [\"samsum\", \"sst2\"]:\n",
    "        sub = df[df[\"task\"] == task]\n",
    "        if sub.empty: continue\n",
    "        if task == \"samsum\":\n",
    "            primary = \"rougeLsum\"\n",
    "            secondary = \"sacrebleu\"\n",
    "            title = \"SAMSum: ROUGE-Lsum & SacreBLEU by Method\"\n",
    "            ylab = \"Score\"\n",
    "        else:\n",
    "            primary = \"accuracy\"\n",
    "            secondary = \"f1\"\n",
    "            title = \"SST-2: Accuracy & F1 by Method\"\n",
    "            ylab = \"Score\"\n",
    "\n",
    "        # Bar chart (primary/secondary)\n",
    "        x = np.arange(len(sub))\n",
    "        fig = plt.figure()\n",
    "        width = 0.35\n",
    "        plt.bar(x - width/2, sub[primary], width, label=primary)\n",
    "        if secondary in sub:\n",
    "            plt.bar(x + width/2, sub[secondary], width, label=secondary)\n",
    "        plt.xticks(x, sub[\"method\"], rotation=0)\n",
    "        plt.ylabel(ylab)\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(out_dir / f\"{task}_metrics_bar.png\", dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Params vs primary metric (log scale on X)\n",
    "        fig = plt.figure()\n",
    "        plt.scatter(sub[\"trainable_params\"], sub[primary])\n",
    "        for _, r in sub.iterrows():\n",
    "            plt.annotate(r[\"method\"], (r[\"trainable_params\"], r[primary]))\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Trainable params (log)\")\n",
    "        plt.ylabel(primary)\n",
    "        plt.title(f\"{task.upper()}: Params vs {primary}\")\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(out_dir / f\"{task}_params_vs_primary.png\", dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser(description=\"Two-task PEFT Benchmark (SAMSum + SST-2)\")\n",
    "    ap.add_argument(\"--tasks\", type=str, default=\"samsum,sst2\", help=\"comma-separated from {samsum,sst2}\")\n",
    "    ap.add_argument(\"--methods\", type=str, default=\"prompt_tuning\", help=\"comma-separated\")\n",
    "    ap.add_argument(\"--epochs\", type=int, default=1)\n",
    "    ap.add_argument(\"--batch_size\", type=int, default=8)\n",
    "    ap.add_argument(\"--lr\", type=float, default=5e-4)\n",
    "    ap.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
    "    ap.add_argument(\"--warmup_ratio\", type=float, default=0.03)\n",
    "    ap.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n",
    "    ap.add_argument(\"--max_source_len\", type=int, default=768)\n",
    "    ap.add_argument(\"--max_target_len\", type=int, default=128)\n",
    "    ap.add_argument(\"--gen_max_new_tokens\", type=int, default=128)\n",
    "    ap.add_argument(\"--prompt_tokens\", type=int, default=20)\n",
    "    ap.add_argument(\"--prompt_init_text\", type=str, default=None)\n",
    "    ap.add_argument(\"--lora_r\", type=int, default=8)\n",
    "    ap.add_argument(\"--lora_alpha\", type=int, default=16)\n",
    "    ap.add_argument(\"--lora_dropout\", type=float, default=0.05)\n",
    "    ap.add_argument(\"--max_train_samples\", type=int, default=None)\n",
    "    ap.add_argument(\"--seed\", type=int, default=42)\n",
    "    ap.add_argument(\"--output_root\", type=str, default=\"outputs_two_tasks\")\n",
    "    ap.add_argument(\"--plots_only\", action=\"store_true\", help=\"Aggregate existing metrics and make comparison plots\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    tasks = [t.strip() for t in args.tasks.split(\",\") if t.strip()]\n",
    "    methods = [m.strip() for m in args.methods.split(\",\") if m.strip()]\n",
    "\n",
    "    results = []\n",
    "    if args.plots_only:\n",
    "        # Collect existing metrics\n",
    "        for mf in Path(args.output_root).rglob(\"metrics.json\"):\n",
    "            with open(mf) as f:\n",
    "                results.append(json.load(f))\n",
    "    else:\n",
    "        for task in tasks:\n",
    "            for method in methods:\n",
    "                cfg = RunConfig(\n",
    "                    task=task,\n",
    "                    method=method,\n",
    "                    model_name=None,  # auto: samsum->knkarthick/samsum, sst2->t5-small\n",
    "                    epochs=args.epochs,\n",
    "                    batch_size=args.batch_size,\n",
    "                    lr=args.lr,\n",
    "                    weight_decay=args.weight_decay,\n",
    "                    warmup_ratio=args.warmup_ratio,\n",
    "                    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "                    max_source_len=args.max_source_len,\n",
    "                    max_target_len=args.max_target_len if task == \"samsum\" else 8,\n",
    "                    gen_max_new_tokens=args.gen_max_new_tokens if task == \"samsum\" else 4,\n",
    "                    prompt_tokens=args.prompt_tokens,\n",
    "                    prompt_init_text=(None if args.prompt_init_text in (None, \"\", \"None\") else args.prompt_init_text),\n",
    "                    lora_r=args.lora_r,\n",
    "                    lora_alpha=args.lora_alpha,\n",
    "                    lora_dropout=args.lora_dropout,\n",
    "                    max_train_samples=args.max_train_samples,\n",
    "                    seed=args.seed,\n",
    "                    output_root=Path(args.output_root),\n",
    "                )\n",
    "                stats = train_and_eval(cfg)\n",
    "                results.append(stats)\n",
    "\n",
    "    # Save summary and make comparison plots\n",
    "    comp_dir = Path(\"comparison_two_tasks\")\n",
    "    ensure_dir(comp_dir)\n",
    "    pd.DataFrame(results).to_csv(comp_dir / \"summary_all.csv\", index=False)\n",
    "    make_comparison_plots(results, out_dir=comp_dir)\n",
    "\n",
    "    # Print compact table\n",
    "    cols = [\"task\", \"method\", \"trainable_readable\", \"train_time_sec\"]\n",
    "    extra = []\n",
    "    if any(r.get(\"accuracy\") is not None for r in results): extra += [\"accuracy\", \"f1\"]\n",
    "    if any(r.get(\"rougeLsum\") is not None for r in results): extra += [\"rougeLsum\", \"sacrebleu\"]\n",
    "    cols += [c for c in extra if c in results[0].keys()]\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df[ [c for c in cols if c in df.columns] ].sort_values([\"task\", \"method\"]).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "181fdc6a-ccaa-4d5e-8eaa-236fc9166152",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_sst2_small = RunConfig(\n",
    "    task=\"sst2\",\n",
    "    method=\"prompt_tuning\",\n",
    "    model_name=MODEL_NAME,\n",
    "    epochs=5,\n",
    "    batch_size=8,\n",
    "    train_size=DATASET_SIZE,\n",
    "    eval_size=DATASET_SIZE,\n",
    ")\n",
    "\n",
    "cfg_sam_small = RunConfig(\n",
    "    task=\"samsum\",\n",
    "    method=\"prompt_tuning\",\n",
    "    model_name=MODEL_NAME,\n",
    "    epochs=5,\n",
    "    batch_size=8,\n",
    "    train_size=DATASET_SIZE,\n",
    "    eval_size=DATASET_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bca358e-d4b1-4d1c-b4f7-2c318b90b240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 67349/67349 [00:09<00:00, 6991.63 examples/s]\n",
      "Map: 100%|██████████| 872/872 [00:00<00:00, 5824.99 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42095' max='42095' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42095/42095 1:17:37, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.947800</td>\n",
       "      <td>0.571611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.269200</td>\n",
       "      <td>0.358719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.159600</td>\n",
       "      <td>0.253496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.991100</td>\n",
       "      <td>0.183593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.031100</td>\n",
       "      <td>0.164494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': 'sst2', 'method': 'prompt_tuning', 'model_name': 't5-small', 'seed': 42, 'device': 'cuda', 'trainable_params': 20480, 'total_params': 60527104, 'trainable_readable': '20.5K', 'total_readable': '60.53M', 'train_time_sec': 4659.218475818634, 'peak_gpu_mem_bytes': 866616832, 'peak_gpu_mem_gb': 0.8070998191833496, 'accuracy': 0.4908256880733945, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n"
     ]
    }
   ],
   "source": [
    "res_cls = train_and_eval(cfg_sst2_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfe61131-ddee-497c-9e17-634b5b182a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'sst2',\n",
       " 'method': 'prompt_tuning',\n",
       " 'model_name': 't5-small',\n",
       " 'seed': 42,\n",
       " 'device': 'cuda',\n",
       " 'trainable_params': 20480,\n",
       " 'total_params': 60527104,\n",
       " 'trainable_readable': '20.5K',\n",
       " 'total_readable': '60.53M',\n",
       " 'train_time_sec': 4659.218475818634,\n",
       " 'peak_gpu_mem_bytes': 866616832,\n",
       " 'peak_gpu_mem_gb': 0.8070998191833496,\n",
       " 'accuracy': 0.4908256880733945,\n",
       " 'precision': 0.0,\n",
       " 'recall': 0.0,\n",
       " 'f1': 0.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c094ced-6201-4b7c-999e-d31f95acfc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14731/14731 [00:09<00:00, 1487.02 examples/s]\n",
      "Map: 100%|██████████| 818/818 [00:00<00:00, 924.64 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5059' max='9210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5059/9210 33:46 < 27:43, 2.50 it/s, Epoch 2.75/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.914600</td>\n",
       "      <td>0.886336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.549600</td>\n",
       "      <td>0.877559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m res_sum = \u001b[43mtrain_and_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg_sam_small\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 126\u001b[39m, in \u001b[36mtrain_and_eval\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available():\n\u001b[32m    125\u001b[39m     torch.cuda.reset_peak_memory_stats()\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m elapsed = time.time() - start\n\u001b[32m    128\u001b[39m stats[\u001b[33m\"\u001b[39m\u001b[33mtrain_time_sec\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mfloat\u001b[39m(elapsed)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Harsh\\Programs\\python_envs\\dl-virtualenv\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Harsh\\Programs\\python_envs\\dl-virtualenv\\Lib\\site-packages\\transformers\\trainer.py:2679\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2674\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2683\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "res_sum = train_and_eval(cfg_sam_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2033a4d-dee3-4ceb-a47b-5980a3af77b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL Virtual Environment",
   "language": "python",
   "name": "dl-virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
