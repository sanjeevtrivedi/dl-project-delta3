{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e1cdbfd-6c0b-4367-b01c-f55d45c15a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os   \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d864e290-1fad-4dd5-9a22-6c77dcd3482d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 103\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import traceback\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    PrefixTuningConfig,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "import logging\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f84e5eb-a84e-4011-a2a6-aa1225e0a1ee",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a6ff1d0-a5ff-4afa-8218-8a8e66d0400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION \n",
    "MODEL_NAME = \"google/flan-t5-small\" # flan-t5-small model is giving issues - config dim bug (num_heads=6 mismatch)\n",
    "SUMMARIZATION_DATASET = \"knkarthick/samsum\"\n",
    "\n",
    "BENCHMARK_GLUE=\"glue\"\n",
    "GLUE_DATASET_TASK_SC = \"sst2\"  # SST-2 for sentiment classification\n",
    "\n",
    "DATASET_SIZE = 'full' # 100 or 500 or 'full' \n",
    "RUN_ABLATIONS = True  # Toggle to enable/disable ablation study (modular flag)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "NUM_VIRTUAL_TOKENS = 32 # CHANGE: Increased from 20 to 50 for better adaptation in prefix/prompt - Why: Longer tokens allow stronger task-specific tuning, fixing weak/flat metrics in prefix/prompt\n",
    "MAX_POS = 512\n",
    "\n",
    "OUTPUT_DIR = './kaggle/working_v2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42cbd230-aca6-426f-8e5f-536a4edca012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "008e53eb-8b97-4c3d-b11a-4593811d6378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Prefix-Tuning COMPARISON - T5-small\n",
      "============================================================\n",
      "Dataset size: full\n",
      "Model: google/flan-t5-small\n",
      "Methods: Prefix-Tuning\n",
      "Ablations Enabled: Including ablated variants for study\n",
      "Note: For prefix ablation, removing projection layer\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Prefix-Tuning COMPARISON - T5-small\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset size: {DATASET_SIZE}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(\"Methods: Prefix-Tuning\")\n",
    "if RUN_ABLATIONS:\n",
    "    print(\"Ablations Enabled: Including ablated variants for study\")\n",
    "    print(\"Note: For prefix ablation, removing projection layer\")\n",
    "print(\"=\"*60)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4511f9-e5c6-4926-a3ed-c02ef000115f",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f29486a2-a2fc-4ba8-98a0-8a98dd03590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_dataset_size(dataset, size):\n",
    "    if size == 'full':\n",
    "        return dataset\n",
    "    if isinstance(size, int) and size > 0:\n",
    "        return dataset.select(range(min(size, len(dataset))))\n",
    "    raise ValueError(f\"Invalid size: {size}\")\n",
    "\n",
    "def setup_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "def safe_cleanup():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888745bc-bba4-49d1-b2d7-c89d59cec5cb",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ebd6e38c-fa1f-4a0c-90ce-fac06c69662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(log_history, exp_name, task_name, save_dir=\"./plots\"):\n",
    "    \"\"\"Plot train/eval loss and task-specific metrics vs step.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "   \n",
    "    # Extract data\n",
    "    #steps = [log['step'] for log in log_history if 'step' in log and 'eval_loss' not in log] # Get train steps\n",
    "    eval_steps = [log['step'] for log in log_history if 'eval_loss' in log] # Get eval steps\n",
    "    train_losses = [log['loss'] for log in log_history if 'loss' in log] # 'loss' is train loss\n",
    "    eval_losses = [log['eval_loss'] for log in log_history if 'eval_loss' in log]\n",
    "   \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "   \n",
    "    # Loss curve\n",
    "    # Match train loss steps to eval steps for cleaner plots if they differ\n",
    "    train_steps_for_loss = [log['step'] for log in log_history if 'loss' in log]\n",
    "    axes[0].plot(train_steps_for_loss, train_losses, label='Train Loss', marker='o', alpha=0.7)\n",
    "    if eval_losses:\n",
    "        axes[0].plot(eval_steps, eval_losses, label='Eval Loss', marker='s')\n",
    "    axes[0].set_xlabel('Step')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title(f'{exp_name} - Loss Curve')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Task-specific metric\n",
    "    if task_name == \"classification\":\n",
    "        eval_accs = [log['eval_accuracy'] for log in log_history if 'eval_accuracy' in log]\n",
    "        if eval_accs:\n",
    "            axes[1].plot(eval_steps, eval_accs, label='Eval Accuracy', marker='o', color='green')\n",
    "            axes[1].set_ylabel('Accuracy')\n",
    "    else: # summarization\n",
    "        eval_rouge_ls = [log['eval_rougeL'] for log in log_history if 'eval_rougeL' in log]\n",
    "        if eval_rouge_ls:\n",
    "            axes[1].plot(eval_steps, eval_rouge_ls, label='Eval ROUGE-L', marker='o', color='green')\n",
    "            axes[1].set_ylabel('ROUGE-L')\n",
    "   \n",
    "    axes[1].set_xlabel('Step')\n",
    "    axes[1].set_title(f'{exp_name} - {task_name.capitalize()} Metric')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(save_dir, f\"{exp_name}_curves.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Learning curves saved to {plot_path}\")\n",
    "    return plot_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b955e88-d007-4ee9-9766-8b24370b0268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ablation_comparisons(results, task_name, save_dir=\"./plots\"):\n",
    "    \"\"\"Graphical analysis: Compare baselines vs ablations for a task.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    methods = list(results.keys())\n",
    "    baselines = [m for m in methods if \"_ablated_\" not in m]\n",
    "    ablations = [m for m in methods if \"_ablated_\" in m]\n",
    "    \n",
    "    if not ablations:\n",
    "        return None\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Trainable params comparison\n",
    "    trainable_pcts = [100 * results[m][\"trainable_params\"] / results[m][\"total_params\"] for m in methods]\n",
    "    sns.barplot(x=methods, y=trainable_pcts, ax=axes[0])\n",
    "    axes[0].set_ylabel('Trainable %')\n",
    "    axes[0].set_title(f'Trainable Params Comparison - {task_name.capitalize()}')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Metric comparison (use key metric)\n",
    "    if task_name == \"classification\":\n",
    "        metrics = [results[m][\"test_metrics\"].get(\"eval_accuracy\", 0) for m in methods]\n",
    "        metric_label = 'Accuracy'\n",
    "    else:\n",
    "        metrics = [results[m][\"test_metrics\"].get(\"eval_rougeL\", 0) for m in methods]\n",
    "        metric_label = 'ROUGE-L'\n",
    "    \n",
    "    sns.barplot(x=methods, y=metrics, ax=axes[1])\n",
    "    axes[1].set_ylabel(metric_label)\n",
    "    axes[1].set_title(f'Performance Comparison - {task_name.capitalize()}')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(save_dir, f\"ablation_comparison_{task_name}.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Ablation comparison plot saved to {plot_path}\")\n",
    "    return plot_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a8e775-82a1-4570-a3eb-83fdf092d72d",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "443458c1-8d8e-4ae7-985b-ce628be1220a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "Datasets loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## LOAD DATASETS \n",
    "print(\"Loading datasets\")\n",
    "# Summarization dataset - SAMSum\n",
    "summarization_dataset = load_dataset(SUMMARIZATION_DATASET)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = setup_tokenizer(MODEL_NAME)\n",
    "\n",
    "if DATASET_SIZE != 'full':\n",
    "    print(f\"Limiting dataset size to {DATASET_SIZE} for train.\")\n",
    "    \n",
    "    summarization_dataset['train'] = limit_dataset_size(summarization_dataset['train'], DATASET_SIZE)\n",
    "    summarization_dataset['validation'] = limit_dataset_size(summarization_dataset['validation'], DATASET_SIZE // 4)\n",
    "    summarization_dataset['test'] = limit_dataset_size(summarization_dataset['test'], DATASET_SIZE // 4)\n",
    "\n",
    "print(\"Datasets loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "04921f83-fbb0-442e-bb4e-7b062aef68cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sample Datasets\n",
      "\n",
      "Summarization Train Samples (Before Preprocessing):\n",
      "{'id': '13818513', 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\nJerry: Sure!\\nAmanda: I'll bring you tomorrow :-)\", 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'}\n",
      "{'id': '13728867', 'dialogue': 'Olivia: Who are you voting for in this election? \\nOliver: Liberals as always.\\nOlivia: Me too!!\\nOliver: Great', 'summary': 'Olivia and Olivier are voting for liberals in this election. '}\n",
      "{'id': '13681000', 'dialogue': \"Tim: Hi, what's up?\\nKim: Bad mood tbh, I was going to do lots of stuff but ended up procrastinating\\nTim: What did you plan on doing?\\nKim: Oh you know, uni stuff and unfucking my room\\nKim: Maybe tomorrow I'll move my ass and do everything\\nKim: We were going to defrost a fridge so instead of shopping I'll eat some defrosted veggies\\nTim: For doing stuff I recommend Pomodoro technique where u use breaks for doing chores\\nTim: It really helps\\nKim: thanks, maybe I'll do that\\nTim: I also like using post-its in kaban style\", 'summary': 'Kim may try the pomodoro technique recommended by Tim to get more stuff done.'}\n",
      "{'id': '13730747', 'dialogue': \"Edward: Rachel, I think I'm in ove with Bella..\\nrachel: Dont say anything else..\\nEdward: What do you mean??\\nrachel: Open your fu**ing door.. I'm outside\", 'summary': 'Edward thinks he is in love with Bella. Rachel wants Edward to open his door. Rachel is outside. '}\n",
      "{'id': '13728094', 'dialogue': \"Sam: hey  overheard rick say something\\nSam: i don't know what to do :-/\\nNaomi: what did he say??\\nSam: he was talking on the phone with someone\\nSam: i don't know who\\nSam: and he was telling them that he wasn't very happy here\\nNaomi: damn!!!\\nSam: he was saying he doesn't like being my roommate\\nNaomi: wow, how do you feel about it?\\nSam: i thought i was a good rommate\\nSam: and that we have a nice place\\nNaomi: that's true man!!!\\nNaomi: i used to love living with you before i moved in with me boyfriend\\nNaomi: i don't know why he's saying that\\nSam: what should i do???\\nNaomi: honestly if it's bothering you that much you should talk to him\\nNaomi: see what's going on\\nSam: i don't want to get in any kind of confrontation though\\nSam: maybe i'll just let it go\\nSam: and see how it goes in the future\\nNaomi: it's your choice sam\\nNaomi: if i were you i would just talk to him and clear the air\", 'summary': 'Sam is confused, because he overheard Rick complaining about him as a roommate. Naomi thinks Sam should talk to Rick. Sam is not sure what to do.'}\n",
      "{'id': '13716343', 'dialogue': \"Neville: Hi there, does anyone remember what date I got married on?\\nDon: Are you serious?\\nNeville: Dead serious. We're on vacation, and Tina's mad at me about something. I have a strange suspicion that this might have something to do with our wedding anniversary, but I have nowhere to check.\\nWyatt: Hang on, I'll ask my wife.\\nDon: Haha, someone's in a lot of trouble :D\\nWyatt: September 17. I hope you remember the year ;)\", 'summary': \"Wyatt reminds Neville his wedding anniversary is on the 17th of September. Neville's wife is upset and it might be because Neville forgot about their anniversary.\"}\n",
      "{'id': '13611672', 'dialogue': \"John: Ave. Was there any homework for tomorrow?\\nCassandra: hello :D Of course, as always :D\\nJohn: What exactly?\\nCassandra: I'm not sure so I'll check it for you in 20minutes. \\nJohn: Cool, thanks. Sorry I couldn't be there, but I was busy as fuck...my stupid boss as always was trying to piss me off\\nCassandra: No problem, what did he do this time?\\nJohn: Nothing special, just the same as always, treating us like children, commanding to do this and that...\\nCassandra: sorry to hear that. but why don't you just go to your chief and tell him everything?\\nJohn: I would, but I don't have any support from others, they are like goddamn pupets and pretend that everything's fine...I'm not gonna fix everything for everyone\\nCassandra: I understand...Nevertheless, just try to ignore him. I know it might sound ridiculous as fuck, but sometimes there's nothing more you can do.\\nJohn: yeah I know...maybe some beer this week?\\nCassandra: Sure, but I got some time after classes only...this week is gonna be busy\\nJohn: no problem, I can drive you home and we can go to some bar or whatever.\\nCassandra: cool. ok, I got this homework. it's page 15 ex. 2 and 3, I also asked the others to study another chapter, especially the vocabulary from the very first pages. Just read it.\\nJohn: gosh...I don't know if I'm smart enough to do it :'D\\nCassandra: you are, don't worry :P Just circle all the words you don't know and we'll continue on Monday.\\nJohn: ok...then I'll try my best :D\\nCassandra: sure, if you will have any questions just either text or call me and I'll help you.\\nJohn: I hope I won't have to waste your time xD\\nCassandra: you're not wasting my time, I'm your teacher, I'm here to help. This is what I get money for, also :P\\nJohn: just kidding :D ok, so i guess we'll stay in touch then\\nCassandra: sure, have a nice evening :D\\nJohn: you too, se ya\\nCassandra: Byeeeee\", 'summary': \"John didn't show up for class due to some work issues with his boss. Cassandra, his teacher told him which exercises to do, and which chapter to study. They are going to meet up for a beer sometime this week after class. \"}\n",
      "{'id': '13730463', 'dialogue': \"Sarah: I found a song on youtube and I think you'll like it\\nJames: What song?\\nSarah: <file_other>\\nJames: Oh. I know it! \\nJames: I heard it before in some compilation\\nSarah: I can't stop playing it over and over\\nJames: That's exactly how I know lyrics to all of the songs on my playlist :D\\nSarah: Haha. No lyrics here though. Instrumental ;D\\nJames: Instrumental songs are different kind of music. \\nJames: But you have to remember that the activity you do when you listen to this song\\nJames: Is the actvity your brain will connect to the song\\nJames: And everytime you play this song at home\\nJames: You'll be thinking of your work\\nSarah: Yeah, I know that. That's why we sometimes say - I used to like that song, but now it just reminds me of bad memories\\nJames: Yup. Everytime you change your partner, you have to get rid of your favorite music :D\\nSarah: Hahaha. True, true.\", 'summary': 'Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to the context they were played in and brings to mind the associated memories.'}\n",
      "{'id': '13809976', 'dialogue': 'Noah: When and where are we meeting? :)\\nMadison: I thought you were busy...?\\nNoah: Yeah, I WAS. I quit my job. \\nMadison: No way! :o :o :o Why? I thought you liked it...?\\nNoah: Well, I used to, until my boss turned into a complete cock... Long story.', 'summary': 'Noah wants to meet, he quit his job, because his boss was a dick.'}\n",
      "{'id': '13809912', 'dialogue': \"Matt: Do you want to go for date?\\nAgnes: Wow! You caught me out with this question Matt.\\nMatt: Why?\\nAgnes: I simply didn't expect this from you.\\nMatt: Well, expect the unexpected.\\nAgnes: Can I think about it?\\nMatt: What is there to think about?\\nAgnes: Well, I don't really know you.\\nMatt: This is the perfect time to get to know eachother\\nAgnes: Well that's true.\\nMatt: So let's go to the Georgian restaurant in Kazimierz.\\nAgnes: Now your convincing me.\\nMatt: Cool, saturday at 6pm?\\nAgnes: That's fine.\\nMatt: I can pick you up on the way to the restaurant.\\nAgnes: That's really kind of you.\\nMatt: No problem.\\nAgnes: See you on saturday.\\nMatt: Yes, looking forward to it.\\nAgnes: Me too.\", 'summary': \"Matt invites Agnes for a date to get to know each other better. They'll go to the Georgian restaurant in Kazimierz on Saturday at 6 pm, and he'll pick her up on the way to the place.\"}\n"
     ]
    }
   ],
   "source": [
    "# Print 10 samples from each train dataset before preprocessing\n",
    "print(\"Original Sample Datasets\")\n",
    "\n",
    "print(\"\\nSummarization Train Samples (Before Preprocessing):\")\n",
    "for i in range(min(10, len(summarization_dataset['train']))):\n",
    "    print(summarization_dataset[\"train\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaa728b-8b8a-4aa3-8e75-55b3b1149ff6",
   "metadata": {},
   "source": [
    "## Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97eff21a-b36c-444b-a641-b8cf80310b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_summarization(examples):\n",
    "    # Create input dialogues with the required prefix\n",
    "    inputs = [f\"Summarize the following conversation:\\n{dialogue}\" for dialogue in examples[\"dialogue\"]]\n",
    "    \n",
    "    # Define max length for inputs\n",
    "    max_input_len = MAX_POS - NUM_VIRTUAL_TOKENS\n",
    "    \n",
    "    # Tokenize inputs with truncation and padding\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_len, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Define max length for summaries\n",
    "    max_label_len = 128\n",
    "\n",
    "    labels_tokenized = tokenizer(text_target=examples[\"summary\"],\n",
    "                                 max_length=max_label_len,\n",
    "                                 truncation=True,\n",
    "                                 padding=\"max_length\")\n",
    "\n",
    "    # Convert pad token ids in labels -> -100 so loss ignores padding\n",
    "    labels = []\n",
    "    for seq in labels_tokenized[\"input_ids\"]:\n",
    "        labels.append([tok if tok != tokenizer.pad_token_id else -100 for tok in seq])\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2ec3c163-f7e6-45f9-82c6-6f750b7ea114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████| 819/819 [00:00<00:00, 1168.05 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-Preprocessing Sample Datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing\n",
    "print(\"\\nApplying preprocessing...\")\n",
    "tokenized_summarization = summarization_dataset.map(preprocess_summarization, batched=True, remove_columns=summarization_dataset[\"train\"].column_names)\n",
    "\n",
    "# Print samples from each post preprocessing\n",
    "POST_PROCESS_SAMPLES = 5\n",
    "\n",
    "print(\"\\nPost-Preprocessing Sample Datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e837f03-ffb4-44ce-a806-0b180a5cba91",
   "metadata": {},
   "source": [
    "## Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a887055-b7f3-4256-9f05-77cfc47aaaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode a single example (input + label)\n",
    "def _decode_example(example: dict, tokenizer, task: str) -> dict:\n",
    "    \"\"\"\n",
    "    Returns a dict with:\n",
    "        - \"input_text\"   : the original prompt (e.g. \"Classify sentiment: …\")\n",
    "        - \"label_text\"   : the gold label (positive/negative or the full summary)\n",
    "        - \"input_ids\"    : first 30 tokens (for sanity check)\n",
    "        - \"label_ids\"    : first 15 tokens of the label\n",
    "    \"\"\"\n",
    "    # 1. Decode the **input** (skip special tokens, keep the prompt)\n",
    "    input_txt = tokenizer.decode(example[\"input_ids\"], skip_special_tokens=False)\n",
    "    # remove the padding part after the EOS token\n",
    "    input_txt = input_txt.split(tokenizer.eos_token)[0] + tokenizer.eos_token\n",
    "\n",
    "    # 2. Decode the **label**\n",
    "    # Labels contain -100 for ignored positions → replace with pad token first\n",
    "    label_ids = [\n",
    "        tok_id if tok_id != -100 else tokenizer.pad_token_id for tok_id in example[\"labels\"]\n",
    "    ]\n",
    "    label_txt = tokenizer.decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # 3. Short token previews (optional, makes the output tidy)\n",
    "    input_preview = \" \".join(map(str, example[\"input_ids\"][:30]))\n",
    "    label_preview = \" \".join(map(str, label_ids[:15]))\n",
    "\n",
    "    return {\n",
    "        \"input_text\": input_txt,\n",
    "        \"label_text\": label_txt,\n",
    "        \"input_ids_preview\": input_preview,\n",
    "        \"label_ids_preview\": label_preview,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31866f51-53e6-48ab-8667-e8904619e6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summarisation – post-preprocessing (5 examples) ===\n",
      "\n",
      "--- Example 1 ---\n",
      "INPUT  : Summarize the following conversation: Amanda: I baked cookies. Do you want some? Jerry: Sure! Amanda: I'll bring you tomorrow :-)</s>\n",
      "SUMMARY: Amanda baked cookies and will bring Jerry some tomorrow.\n",
      "\n",
      "--- Example 2 ---\n",
      "INPUT  : Summarize the following conversation: Olivia: Who are you voting for in this election? Oliver: Liberals as always. Olivia: Me too!! Oliver: Great</s>\n",
      "SUMMARY: Olivia and Olivier are voting for liberals in this election. \n",
      "\n",
      "--- Example 3 ---\n",
      "INPUT  : Summarize the following conversation: Tim: Hi, what's up? Kim: Bad mood tbh, I was going to do lots of stuff but ended up procrastinating Tim: What did you plan on doing? Kim: Oh you know, uni stuff and unfucking my room Kim: Maybe tomorrow I'll move my ass and do everything Kim: We were going to defrost a fridge so instead of shopping I'll eat some defrosted veggies Tim: For doing stuff I recommend Pomodoro technique where u use breaks for doing chores Tim: It really helps Kim: thanks, maybe I'll do that Tim: I also like using post-its in kaban style</s>\n",
      "SUMMARY: Kim may try the pomodoro technique recommended by Tim to get more stuff done.\n",
      "\n",
      "--- Example 4 ---\n",
      "INPUT  : Summarize the following conversation: Edward: Rachel, I think I'm in ove with Bella.. rachel: Dont say anything else.. Edward: What do you mean?? rachel: Open your fu**ing door.. I'm outside</s>\n",
      "SUMMARY: Edward thinks he is in love with Bella. Rachel wants Edward to open his door. Rachel is outside. \n",
      "\n",
      "--- Example 5 ---\n",
      "INPUT  : Summarize the following conversation: Sam: hey overheard rick say something Sam: i don't know what to do :-/ Naomi: what did he say?? Sam: he was talking on the phone with someone Sam: i don't know who Sam: and he was telling them that he wasn't very happy here Naomi: damn!!! Sam: he was saying he doesn't like being my roommate Naomi: wow, how do you feel about it? Sam: i thought i was a good rommate Sam: and that we have a nice place Naomi: that's true man!!! Naomi: i used to love living with you before i moved in with me boyfriend Naomi: i don't know why he's saying that Sam: what should i do??? Naomi: honestly if it's bothering you that much you should talk to him Naomi: see what's going on Sam: i don't want to get in any kind of confrontation though Sam: maybe i'll just let it go Sam: and see how it goes in the future Naomi: it's your choice sam Naomi: if i were you i would just talk to him and clear the air</s>\n",
      "SUMMARY: Sam is confused, because he overheard Rick complaining about him as a roommate. Naomi thinks Sam should talk to Rick. Sam is not sure what to do.\n",
      "\n",
      "Preprocessing complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print summarisation samples\n",
    "print(\"\\n=== Summarisation – post-preprocessing (5 examples) ===\")\n",
    "for i, ex in enumerate(tokenized_summarization[\"train\"].select(range(min(POST_PROCESS_SAMPLES, len(tokenized_summarization[\"train\"]))))):\n",
    "    decoded = _decode_example(ex, tokenizer, task=\"summarization\")\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"INPUT  : {decoded['input_text']}\")\n",
    "    print(f\"SUMMARY: {decoded['label_text']}\")\n",
    "    # print(f\"input_ids  (first 30) : {decoded['input_ids_preview']}\")\n",
    "    # print(f\"label_ids  (first 15) : {decoded['label_ids_preview']}\")\n",
    "\n",
    "print(\"\\nPreprocessing complete\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d48e5-de8c-40eb-bb95-f2ae8140cd49",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "76b9ea1b-85c3-4956-aabd-48692bb8f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56d596b0-5b5a-495b-8276-c793b9226a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summarization_metrics(eval_pred):\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        \n",
    "        # Handling prediction tensors\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        if len(predictions.shape) == 3:\n",
    "            predictions = np.argmax(predictions, axis=-1)\n",
    "        \n",
    "        # Replace -100 in predictions/labels with pad_token_id\n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "        # Validate predictions and labels for negative values\n",
    "        if np.any(predictions < 0) or np.any(labels < 0):\n",
    "            logger.warning(f\"Found negative values in predictions or labels. Clamping to 0.\")\n",
    "            predictions = np.clip(predictions, 0, None)\n",
    "            labels = np.clip(labels, 0, None)\n",
    "        \n",
    "        # Decode the predictions and labels\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        # CHANGE: Added sample logging for debug - Why: To inspect poor generations causing decreasing ROUGE\n",
    "        logger.info(f\"Sample pred: {decoded_preds[0]}, label: {decoded_labels[0]}\")  # Log first sample\n",
    "        \n",
    "        # Normalize the decoded texts\n",
    "        decoded_preds = [p.strip() if p.strip() else \"empty\" for p in decoded_preds]\n",
    "        decoded_labels = [l.strip() if l.strip() else \"empty\" for l in decoded_labels]\n",
    "        \n",
    "        # Compute ROUGE scores\n",
    "        result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        \n",
    "        # CHANGE: Ensure keys always returned - Why: Fixes empty plots by guaranteeing 'eval_rougeL' in logs\n",
    "        return {\n",
    "            \"rouge1\": result.get(\"rouge1\", 0.0),\n",
    "            \"rouge2\": result.get(\"rouge2\", 0.0),\n",
    "            \"rougeL\": result.get(\"rougeL\", 0.0),\n",
    "            \"rougeLsum\": result.get(\"rougeLsum\", 0.0)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        # CHANGE: More verbose error logging - Why: Catches silent failures in metrics computation\n",
    "        logger.error(f\"Summarization metrics error: {e}. Returning defaults.\")\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "018052b2-f743-488c-a137-b69ac0e94f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix - @TODO: Integrate into main flow\n",
    "def plot_confusion_matrix(y_true, y_pred, classes=None, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, cbar=False, xticklabels=classes, yticklabels=classes)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def compute_and_plot_confusion_matrix_classification(decoded_labels, decoded_preds):\n",
    "    # Convert text labels to binary 0/1\n",
    "    label_binary = [1 if 'positive' in l else 0 for l in decoded_labels]\n",
    "    pred_binary = [1 if 'positive' in p else 0 for p in decoded_preds]\n",
    "    plot_confusion_matrix(label_binary, pred_binary, classes=['negative', 'positive'], title='Classification Confusion Matrix')\n",
    "\n",
    "def compute_and_plot_confusion_matrix_summarization(decoded_labels, decoded_preds, tokenizer):\n",
    "    # For summarization, generate token-level confusion matrix based on token matches\n",
    "    label_tokens = [tokenizer.tokenize(l) for l in decoded_labels]\n",
    "    pred_tokens = [tokenizer.tokenize(p) for p in decoded_preds]\n",
    "\n",
    "    true_tokens = []\n",
    "    pred_tokens_flat = []\n",
    "    for lt, pt in zip(label_tokens, pred_tokens):\n",
    "        min_len = min(len(lt), len(pt))\n",
    "        true_tokens.extend(lt[:min_len])\n",
    "        pred_tokens_flat.extend(pt[:min_len])\n",
    "\n",
    "    # Limit to top 10 tokens for visualization\n",
    "    all_tokens = list(set(true_tokens + pred_tokens_flat))\n",
    "    if len(all_tokens) > 10:\n",
    "        all_tokens = all_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb1a09-bb34-447e-8c19-3a36fc43e49a",
   "metadata": {},
   "source": [
    "## Training Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5a5b47a7-90ad-4e79-a2ca-dc82ed48801c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 104\n"
     ]
    }
   ],
   "source": [
    "def get_training_args(method_name, task_name):\n",
    "    is_peft = method_name in [\"prefix\"] or \"_ablated_\" in method_name\n",
    "    # CHANGE: Lowered LR for PEFT/ablation to 1e-3, Full FT to 1e-4 - Why: High LR caused instability/overfitting/decreasing metrics; matches t5-small recommendations\n",
    "    \n",
    "    if DATASET_SIZE == 'full':\n",
    "        # CHANGE: Increased epochs to 5 for summarization - Why: Smaller dataset needs more passes for convergence, fixing underfitting/low ROUGE\n",
    "        epochs = 5\n",
    "        batch, eval_steps = 8, 500\n",
    "    elif DATASET_SIZE <= 500:\n",
    "        # Use more epochs for very small datasets to allow for learning\n",
    "        epochs, batch, eval_steps = 10, 4, 20 # Eval more frequently\n",
    "    else:\n",
    "        epochs, batch, eval_steps = 3, 8, 100\n",
    "\n",
    "    # Adjust steps based on actual dataset size\n",
    "    if DATASET_SIZE != 'full':\n",
    "        total_steps = (DATASET_SIZE // batch) * epochs\n",
    "        # Ensure eval_steps is not 0 and is reasonable\n",
    "        eval_steps = max(1, min(total_steps // 5, 50)) # Eval 5 times per run, max 50\n",
    "        logging_steps = max(1, eval_steps // 2)\n",
    "        save_steps = eval_steps\n",
    "        eval_strategy = \"steps\"\n",
    "        save_strategy = \"steps\"\n",
    "    else:\n",
    "        eval_strategy = \"epoch\"\n",
    "        save_strategy = \"epoch\"\n",
    "        logging_steps = 100\n",
    "        save_steps = None\n",
    "        eval_steps = None\n",
    "\n",
    "    if \"no_proj\" in method_name:\n",
    "        lr = 1e-2  # High LR for ablation\n",
    "    elif is_peft:\n",
    "        lr = 5e-3 # CHANGED: Increased from 5e-4 to 1e-2. Soft prompts need high LR.\n",
    "        epochs = 20\n",
    "    else:\n",
    "        lr = 1e-4\n",
    "    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    # CHANGE: Set fp16=True if not bf16 - Why: Faster training/mixed precision, fixing slow runs/low metrics if GPU supports\n",
    "    use_fp16 = not use_bf16 and torch.cuda.is_available()  # Enable fp16 on CUDA if bf16 unavailable\n",
    "    \n",
    "    # For prompt tuning in PEFT can cause errors\n",
    "    load_best = False\n",
    "    \n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"{OUTPUT_DIR}/results/{task_name}/{method_name}\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch,\n",
    "        per_device_eval_batch_size=batch * 2,\n",
    "        learning_rate=lr,\n",
    "        # CHANGE: Increased warmup_steps to 1000 - Why: Smoother optimization start, fixing oscillation/stuck loss in full FT/ablations\n",
    "        #warmup_steps=1000 if DATASET_SIZE == 'full' else min(100, DATASET_SIZE // 10),\n",
    "        # CHANGE: Increased weight_decay to 0.1 - Why: Stronger regularization prevents overfitting, fixing loss→0 but metrics drop\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.05,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        eval_strategy=eval_strategy,\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy=save_strategy,\n",
    "        save_steps=save_steps,\n",
    "        load_best_model_at_end=load_best,\n",
    "        metric_for_best_model=\"rougeLsum\",\n",
    "        save_total_limit=2,\n",
    "        logging_steps=logging_steps,\n",
    "        bf16=use_bf16,\n",
    "        fp16=use_fp16,\n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_drop_last=True, # Avoid incomplete batches for stability\n",
    "        report_to=\"none\",\n",
    "        predict_with_generate=True,\n",
    "        max_grad_norm=1.0,  # Added to prevent gradient explosions\n",
    "        # CHANGE: Added gradient_accumulation_steps=4 - Why: Stabilizes training with small effective batches, fixing oscillation in ablations\n",
    "        gradient_accumulation_steps=4,\n",
    "        label_smoothing_factor=0.1,\n",
    "        # CHANGE: Set optim to 'adamw_torch' - Why: More robust for PEFT, fixing instability in ablations/Full FT\n",
    "        optim='adamw_torch',\n",
    "        # CHANGE: Set gradient_checkpointing=False - Why: Avoids grad flow issues in PEFT/T5, fixing \"no grad_fn\" error; trade memory for stability\n",
    "        gradient_checkpointing=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cc5f0c81-782e-4451-b29b-e83544d8639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Seq2SeqTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs)\n",
    "        # Save past state if it exists\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            unwrapped_model = Accelerator().unwrap_model(model)\n",
    "            if isinstance(unwrapped_model, PeftModel):\n",
    "                model_base = unwrapped_model.base_model\n",
    "                if hasattr(model_base, \"model\"):\n",
    "                    model_name = model_base.model._get_name()\n",
    "                else:\n",
    "                    model_name = model_base._get_name()\n",
    "                if any(name in model_name for name in [\"GPT\", \"opt\", \"bloom\", \"llama\", \"gemma\"]):\n",
    "                    loss = self.label_smoother(outputs, labels, shift_labels=True)\n",
    "                else:\n",
    "                    loss = self.label_smoother(outputs, labels)\n",
    "            else:\n",
    "                loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bd3b80-d5e7-4d40-9714-77869b6436a1",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "66ebb0e1-23a1-4745-8b25-65f9e1690f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXPERIMENT: PREFIX on SUMMARIZATION\n",
      "============================================================\n",
      "\n",
      "trainable params: 3,361,280 || all params: 80,322,432 || trainable%: 4.1847\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9220' max='9220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9220/9220 1:53:55, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.468100</td>\n",
       "      <td>3.475110</td>\n",
       "      <td>0.072427</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.066198</td>\n",
       "      <td>0.066153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.712900</td>\n",
       "      <td>3.534191</td>\n",
       "      <td>0.074126</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.065851</td>\n",
       "      <td>0.065656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.708300</td>\n",
       "      <td>6.644226</td>\n",
       "      <td>0.007710</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.007590</td>\n",
       "      <td>0.007546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.454100</td>\n",
       "      <td>6.670243</td>\n",
       "      <td>0.009754</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.009701</td>\n",
       "      <td>0.009692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.837700</td>\n",
       "      <td>4.511928</td>\n",
       "      <td>0.051689</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.049105</td>\n",
       "      <td>0.048851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.617400</td>\n",
       "      <td>4.028275</td>\n",
       "      <td>0.054418</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.054410</td>\n",
       "      <td>0.054290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.804600</td>\n",
       "      <td>3.415081</td>\n",
       "      <td>0.064918</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.062715</td>\n",
       "      <td>0.062717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.989300</td>\n",
       "      <td>3.529185</td>\n",
       "      <td>0.067315</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.063905</td>\n",
       "      <td>0.063847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.729900</td>\n",
       "      <td>3.779026</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.657100</td>\n",
       "      <td>3.360150</td>\n",
       "      <td>0.053936</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051310</td>\n",
       "      <td>0.051277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.575200</td>\n",
       "      <td>3.326592</td>\n",
       "      <td>0.058844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058940</td>\n",
       "      <td>0.058861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.596900</td>\n",
       "      <td>3.315360</td>\n",
       "      <td>0.040819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040928</td>\n",
       "      <td>0.040960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.548000</td>\n",
       "      <td>3.296968</td>\n",
       "      <td>0.051294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051433</td>\n",
       "      <td>0.051413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.534800</td>\n",
       "      <td>3.421736</td>\n",
       "      <td>0.055559</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.055611</td>\n",
       "      <td>0.055555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.559200</td>\n",
       "      <td>3.321539</td>\n",
       "      <td>0.059653</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.059680</td>\n",
       "      <td>0.059664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.546600</td>\n",
       "      <td>3.312852</td>\n",
       "      <td>0.045956</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.045884</td>\n",
       "      <td>0.045877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.506900</td>\n",
       "      <td>3.278290</td>\n",
       "      <td>0.046026</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.045972</td>\n",
       "      <td>0.045952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.518100</td>\n",
       "      <td>3.269029</td>\n",
       "      <td>0.046915</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.046901</td>\n",
       "      <td>0.046887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.487200</td>\n",
       "      <td>3.269055</td>\n",
       "      <td>0.048298</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.048227</td>\n",
       "      <td>0.048155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.501100</td>\n",
       "      <td>3.266884</td>\n",
       "      <td>0.047902</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.047794</td>\n",
       "      <td>0.047775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sample pred:                    , label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred:   s         is  Tom Tom Tom   , label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: ssss .... .........., label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred:                    , label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: sss                , label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred:      to, label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: to  , she to  to to to to to to to to to to to to to, label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred:   , but. to. to. to........., label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred:   . .. . . . . .   , label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: ,,, but, to to to to,,, to, to, to, to, label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: .,,,, to....... to,,,,,,, label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: .,,, to to. to to to to to to to to to to to to to, label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: .... to to to, to, to,, to,,,,,,, label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: .,.. to to to,,, to,,,,,,, to to, label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: to,,,, to to to, label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: to,,, to to to to to to,,, to, to, to to,, label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: to.,, to to to to, to,, to to, to, to, to, label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: to.. to, to to to, to,, to to, to, to, to, label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: to.. to, to to to to to,,, to, to, to, to, label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: to.. to, to to to, to,,, to, to, to, to, label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best checkpoint manually: ./kaggle/working_v2//results/summarization/prefix\\checkpoint-461\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sample pred:          for     and and is and  l and      Larry and    a a a a a a a a a a a and a a a and Larry calls Larry calls Larry called Larry called Larry called Larry called Larry called Larry called Larry called Larry called Larry called Larry called Larry calls Larry and Larry called Larry called Larry called her last time to her last time a her last time a to her last time a to her last time they a a Larry., label: Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred:          for                      ., label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample generations: ['         for                      .', '         for Emma Emma Emma Emma Emma Emma Emma Emma Emma Emma Emma Emma Emma Emma Emma Emma Emma Emma Emma Emma Emma Emma', '         for      Madison Madison  is Madison Madison Madison Madison Madison Madison Madison Madison     . Iggy and Iggy and Iggy and Iggy and Iggy and Iggy and Iggy is pregnant Madison is pregnant Madison is pregnant Madison is the Madison is pregnant Madison and Iggy and Iggy and Iggy and I Annie, Iggy is the Annie is pregnant. is pregnant. is a is pregnant. is pregnant.', '         for Mar Mar Mar Mar Mar Mar Mar Mar Mar Mar Mar Mar Mar Mar Mar Mar Mar Mar Mar Mar Mar Mar', '         for Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert Robert']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed and saved to ./kaggle/working_v2/models/summarization/prefix\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: PREFIX_ABLATED_NO_PROJ on SUMMARIZATION\n",
      "============================================================\n",
      "\n",
      "trainable params: 196,608 || all params: 77,157,760 || trainable%: 0.2548\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2305' max='2305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2305/2305 28:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.410000</td>\n",
       "      <td>3.240115</td>\n",
       "      <td>0.299260</td>\n",
       "      <td>0.099158</td>\n",
       "      <td>0.235816</td>\n",
       "      <td>0.235502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.368000</td>\n",
       "      <td>3.197887</td>\n",
       "      <td>0.290590</td>\n",
       "      <td>0.092733</td>\n",
       "      <td>0.231382</td>\n",
       "      <td>0.231657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.357000</td>\n",
       "      <td>3.186075</td>\n",
       "      <td>0.292785</td>\n",
       "      <td>0.091461</td>\n",
       "      <td>0.232314</td>\n",
       "      <td>0.232355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.351200</td>\n",
       "      <td>3.182191</td>\n",
       "      <td>0.290766</td>\n",
       "      <td>0.090851</td>\n",
       "      <td>0.231529</td>\n",
       "      <td>0.231805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.335400</td>\n",
       "      <td>3.180396</td>\n",
       "      <td>0.290458</td>\n",
       "      <td>0.090104</td>\n",
       "      <td>0.231682</td>\n",
       "      <td>0.231754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sample pred: B., label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: B: Is as in the animal shelter., label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: B: Is as in the ., label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: B: Is as in the ., label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: B: Is as in the ., label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best checkpoint manually: ./kaggle/working_v2//results/summarization/prefix_ablated_no_proj\\checkpoint-461\n",
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Sample pred: Betty's number is at the park with Betty has Betty's Betty's has Betty has Betty has Betty has Betty's number is not found. Amanda's Betty's's number. Larry's number Larry's's number Larry's's phone number. Amanda't., label: Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample pred: B., label: A will go to the animal shelter tomorrow to get a puppy for her son. They already visited the shelter last Monday and the son chose the puppy. \n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:__main__:Sample generations: ['B.', 'Lauren wants for her kids are filled with chocolates are all the advent calendar is going to buy an advent calendar is looking for the advent calendar is looking for Emma is looking for her children.', \"Jackie doesn's is pregnant Madison is pregnant, Jackie doesn's pregnant Madison is pregnant Madison is pregnant Madison is pregnant Madison is pregnant Madison is pregnant Madison is pregnant Madison't want to prepares.\", \"Marla finds under Marla finds under her under Marla found under her under her ass Marla found underwear that someone's is looking for Marla found spotted under her under her under her bed.'s under her bed. Kiki's under her bed isn's under her underwear is not her under her bed underwear underwear. Kiki' underwear. Kiki and Tamara's. Kiki'. Kiki. Kiki. Kiki's. Kiki. Kiki and Tamara. Kiki and Tamara are doing total \", 'Robert has to buy guitar cable. he has to Robert has to buy guitar cable. Robert has to buys will buy guitar cable is going to buy guitar cable is going to buy guitar cable.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed and saved to ./kaggle/working_v2/models/summarization/prefix_ablated_no_proj\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MAIN TRAINING LOOP \n",
    "base_methods = [\"prefix\"]\n",
    "ablation_methods = [\"prefix_ablated_no_proj\"] if RUN_ABLATIONS else []\n",
    "methods_to_run = base_methods + ablation_methods\n",
    "tasks = {\n",
    "    \"summarization\": (tokenized_summarization, compute_summarization_metrics)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "os.makedirs(f\"{OUTPUT_DIR}/results\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/plots\", exist_ok=True) \n",
    "\n",
    "for method_name in methods_to_run:\n",
    "    for task_name, (dataset, compute_metrics) in tasks.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EXPERIMENT: {method_name.upper()} on {task_name.upper()}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        try:\n",
    "            config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "            use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                config=config,\n",
    "                torch_dtype=torch.bfloat16 if use_bf16 else torch.float32,\n",
    "            )\n",
    "\n",
    "            model.to(device)\n",
    "            \n",
    "            # Note: t5-small has correct dims (num_heads=8, head_dim=64); PEFT handles DynamicCache natively.\n",
    "            # Create PEFT configs dynamically from model.config\n",
    "            d_model = model.config.d_model\n",
    "            num_heads = model.config.num_heads\n",
    "            total_layers = model.config.num_layers\n",
    "            effective_token_dim = num_heads * model.config.d_kv\n",
    "            peft_configs_local = {\n",
    "                \"prefix\": PrefixTuningConfig(\n",
    "                    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "                    inference_mode=False,\n",
    "                    num_virtual_tokens=NUM_VIRTUAL_TOKENS,\n",
    "                    token_dim=effective_token_dim,\n",
    "                    num_transformer_submodules=2,\n",
    "                    num_attention_heads=num_heads,\n",
    "                    num_layers=total_layers,\n",
    "                    encoder_hidden_size=d_model,\n",
    "                    prefix_projection=True  # Baseline with projection\n",
    "                ),\n",
    "                \"prefix_ablated_no_proj\": PrefixTuningConfig(  # Ablation: Remove projection layer\n",
    "                    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "                    inference_mode=False,\n",
    "                    num_virtual_tokens=NUM_VIRTUAL_TOKENS,\n",
    "                    token_dim=effective_token_dim,\n",
    "                    num_transformer_submodules=2,\n",
    "                    num_attention_heads=num_heads,\n",
    "                    num_layers=total_layers,\n",
    "                    encoder_hidden_size=d_model,\n",
    "                    prefix_projection=False  # Ablated\n",
    "                )\n",
    "            }\n",
    "            model = get_peft_model(model, peft_configs_local[method_name])\n",
    "            model.print_trainable_parameters()\n",
    "            \n",
    "            training_args = get_training_args(method_name, task_name)\n",
    "            data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "            trainer = CustomTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=dataset[\"train\"],\n",
    "                eval_dataset=dataset[\"validation\"],\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_metrics,\n",
    "                tokenizer=tokenizer\n",
    "            )\n",
    "\n",
    "            print(\"Training...\")\n",
    "            train_result = trainer.train()\n",
    "            \n",
    "            # Manual load best model for prefix/prompt methods\n",
    "            if not training_args.load_best_model_at_end and trainer.state.best_model_checkpoint:\n",
    "                print(f\"Loading best checkpoint manually: {trainer.state.best_model_checkpoint}\")\n",
    "                base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    MODEL_NAME,\n",
    "                    config=config,\n",
    "                    torch_dtype=torch.bfloat16 if use_bf16 else torch.float32,\n",
    "                )\n",
    "                base_model.to(device)\n",
    "                model = PeftModel.from_pretrained(base_model, trainer.state.best_model_checkpoint)\n",
    "                trainer.model = model\n",
    "                model.to(device)  # Ensure the full PEFT model is on device\n",
    "            \n",
    "            print(\"Evaluating...\")\n",
    "            test_dataset = dataset.get(\"test\", dataset[\"validation\"])\n",
    "            gen_kwargs = {\n",
    "                # CHANGE: For classification, max_length=5; summ=128; num_beams=6 - Why: Short for classification enforces concise labels (fixes verbose outputs/low acc); more beams improves quality (fixes poor ROUGE)\n",
    "                \"max_length\": 128,\n",
    "                \"num_beams\": 4,\n",
    "                # \"repetition_penalty\": 2.5,\n",
    "                # \"no_repeat_ngram_size\": 3,\n",
    "                \"early_stopping\": True,\n",
    "                \"do_sample\": False,\n",
    "                # \"top_p\": 0.95,\n",
    "                # \"temperature\": 0.7\n",
    "            }\n",
    "            \n",
    "            # Set generation kwargs for trainer.evaluate\n",
    "            training_args.generation_max_length = gen_kwargs[\"max_length\"]\n",
    "            training_args.generation_num_beams = gen_kwargs[\"num_beams\"]\n",
    "            test_metrics = trainer.evaluate(test_dataset)\n",
    "            # CHANGE: Added trainer.predict for sample logging post-eval - Why: Debugs generations, fixing empty/low metrics\n",
    "            predictions = trainer.predict(dataset[\"validation\"])\n",
    "            # CHANGE: Clean predictions before decoding - Why: Handles -100/invalid IDs, fixing OverflowError in batch_decode\n",
    "            cleaned_predictions = np.where(predictions.predictions != -100, predictions.predictions, tokenizer.pad_token_id)\n",
    "            cleaned_predictions = np.clip(cleaned_predictions, 0, tokenizer.vocab_size - 1)\n",
    "            logger.info(f\"Sample generations: {tokenizer.batch_decode(cleaned_predictions[:5], skip_special_tokens=True)}\")\n",
    "            exp_name = f\"{method_name}_{task_name}\"\n",
    "            trainable = model.num_parameters(only_trainable=True) if hasattr(model, 'num_parameters') else sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            total = model.num_parameters() if hasattr(model, 'num_parameters') else sum(p.numel() for p in model.parameters())\n",
    "            \n",
    "            results[exp_name] = {\n",
    "                \"train_metrics\": train_result.metrics,\n",
    "                \"test_metrics\": test_metrics,\n",
    "                \"trainable_params\": trainable,\n",
    "                \"total_params\": total,\n",
    "                \"log_history\": trainer.state.log_history # Collect for plotting\n",
    "            }\n",
    "            \n",
    "            save_path = f\"{OUTPUT_DIR}models/{task_name}/{method_name}\"\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            trainer.save_model(save_path)\n",
    "            print(f\"Completed and saved to {save_path}\\n\")\n",
    "            del model, trainer\n",
    "            safe_cleanup()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"ERROR in {method_name}_{task_name}: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            try:\n",
    "                del model, trainer\n",
    "            except:\n",
    "                pass\n",
    "            safe_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aadd7b07-22db-4b7c-84f7-6cc12dd87e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTS SUMMARY:\n",
      "============================================================\n",
      "\n",
      "PREFIX - Summarization:\n",
      " Trainable: 0.00%\n",
      " ROUGE-1: 0.1155\n",
      " ROUGE-L: 0.0935\n",
      "\n",
      "PREFIX - Ablated_no_proj_summarization:\n",
      " Trainable: 0.00%\n",
      " ROUGE-1: 0.2645\n",
      " ROUGE-L: 0.2096\n",
      "\n",
      "ABLATION DELTAS:\n",
      "Delta for PREFIX_ABLATED_NO_PROJ_SUMMARIZATION: {'eval_loss': 0.0, 'eval_rouge1': 0.0, 'eval_rouge2': 0.0, 'eval_rougeL': 0.0, 'eval_rougeLsum': 0.0, 'eval_runtime': 0.0, 'eval_samples_per_second': 0.0, 'eval_steps_per_second': 0.0}\n",
      "\n",
      "Generating learning curves...\n",
      "Learning curves saved to ./kaggle/working_v2//plots\\prefix_summarization_curves.png\n",
      "Learning curves saved to ./kaggle/working_v2//plots\\prefix_ablated_no_proj_summarization_curves.png\n",
      "\n",
      "Generating ablation comparison plots...\n",
      "Ablation comparison plot saved to ./kaggle/working_v2//plots\\ablation_comparison_summarization.png\n",
      "\n",
      "Results saved to './kaggle/working_v2//prefix_results.csv'\n",
      "Report saved to './kaggle/working_v2//prefix_final_report.md' (includes plot links)\n",
      "\n",
      "OUTCOME INSIGHTS:\n",
      "- For Summarization, PREFIX has the lowest trainable params (0.00%).\n",
      "- PREFIX achieves the highest ROUGEL score (0.2096) on Summarization.\n",
      "- Ablation in PREFIX_ABLATED_NO_PROJ_SUMMARIZATION leads to no change in performance (delta: 0.0000).\n",
      "View plots in ./kaggle/working_v2//plots/ for detailed curves (loss/metric vs step) and comparisons.\n",
      "\n",
      "============================================================\n",
      "SUCCESS - Prefix-Tuning method completed! With ablations!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "if results:\n",
    "    print(\"\\nRESULTS SUMMARY:\")\n",
    "    print(\"=\"*60)\n",
    "    for exp_name, exp_data in results.items():\n",
    "        # Handle cases where task name might have underscores\n",
    "        method_task_split = exp_name.split('_', 1)\n",
    "        method = method_task_split[0]\n",
    "        task = method_task_split[1] if len(method_task_split) > 1 else 'unknown'\n",
    "        \n",
    "        metrics = exp_data[\"test_metrics\"]\n",
    "        pct = 100 * exp_data[\"trainable_params\"] / exp_data[\"total_params\"]\n",
    "        print(f\"\\n{method.upper()} - {task.capitalize()}:\")\n",
    "        print(f\" Trainable: {pct:.2f}%\")\n",
    "        if task == \"classification\":\n",
    "            print(f\" Accuracy: {metrics.get('eval_accuracy', 0):.4f}\")\n",
    "            print(f\" F1: {metrics.get('eval_f1', 0):.4f}\")\n",
    "        else:\n",
    "            print(f\" ROUGE-1: {metrics.get('eval_rouge1', 0):.4f}\")\n",
    "            print(f\" ROUGE-L: {metrics.get('eval_rougeL', 0):.4f}\")\n",
    "\n",
    "    # Ablation deltas if enabled\n",
    "    if RUN_ABLATIONS:\n",
    "        print(\"\\nABLATION DELTAS:\")\n",
    "        for exp_name, exp_data in results.items():\n",
    "            if \"_ablated_\" in exp_name:\n",
    "                method_task_split = exp_name.split('_ablated_')[0]\n",
    "                task = exp_name.split('_', 1)[1] # Get task name\n",
    "                base_method_name = f\"{method_task_split}_{task}\"\n",
    "                \n",
    "                if base_method_name in results:\n",
    "                    base_metrics = results[base_method_name][\"test_metrics\"]\n",
    "                    delta = {k: exp_data[\"test_metrics\"].get(k, 0) - base_metrics.get(k, 0) for k in base_metrics if \"eval_\" in k}\n",
    "                    print(f\"Delta for {exp_name.upper()}: {delta}\")\n",
    "\n",
    "    # Plot learning curves for each experiment\n",
    "    print(\"\\nGenerating learning curves...\")\n",
    "    plot_paths = {}\n",
    "    plot_save_dir = f\"{OUTPUT_DIR}/plots\" # [FIX] Define plot save dir\n",
    "    for exp_name, exp_data in results.items():\n",
    "        task_name = exp_name.split(\"_\", 1)[1]\n",
    "        # [FIX] Pass the correct save_dir to the plotting function\n",
    "        plot_path = plot_learning_curves(exp_data[\"log_history\"], exp_name, task_name, save_dir=plot_save_dir)\n",
    "        plot_paths[exp_name] = plot_path\n",
    "    \n",
    "    # Graphical ablation comparisons per task\n",
    "    ablation_plot_paths = {}\n",
    "    if RUN_ABLATIONS:\n",
    "        print(\"\\nGenerating ablation comparison plots...\")\n",
    "        for task_name in tasks.keys():\n",
    "            task_results = {k: v for k, v in results.items() if k.endswith(f\"_{task_name}\")}\n",
    "            if task_results:\n",
    "                # [FIX] Pass the correct save_dir to the plotting function\n",
    "                ablation_plot_path = plot_ablation_comparisons(task_results, task_name, save_dir=plot_save_dir)\n",
    "                if ablation_plot_path:\n",
    "                    ablation_plot_paths[task_name] = ablation_plot_path\n",
    "\n",
    "    # --- Results DataFrame ---\n",
    "    results_df = []\n",
    "    for exp_name, exp_data in results.items():\n",
    "        method, task = exp_name.split(\"_\", 1)\n",
    "        results_df.append({\n",
    "            \"Method\": method.upper(),\n",
    "            \"Task\": task.capitalize(),\n",
    "            \"Trainable %\": 100 * exp_data[\"trainable_params\"] / exp_data[\"total_params\"],\n",
    "            **{k: v for k, v in exp_data[\"test_metrics\"].items() if isinstance(v, (int, float))}\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results_df)\n",
    "    cols = [\"Method\", \"Task\", \"Trainable %\"]\n",
    "    metric_cols = [c for c in df.columns if c.startswith(\"eval_\")]\n",
    "    cols.extend(sorted(metric_cols))\n",
    "    df = df[cols]\n",
    "    df.to_csv(f\"{OUTPUT_DIR}/prefix_results.csv\", index=False)\n",
    "    print(f\"\\nResults saved to '{OUTPUT_DIR}/prefix_results.csv'\")\n",
    "    \n",
    "    # --- Final Report --- \n",
    "    # Use relative paths for plots in the markdown report\n",
    "    report_path = f\"{OUTPUT_DIR}/prefix_final_report.md\"\n",
    "    report_dir = os.path.dirname(report_path)\n",
    "\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(f\"# Prefix-Tuning Adaptation Results - T5-small\\n\\n\")\n",
    "        f.write(f\"## Configuration\\n\")\n",
    "        f.write(f\"- Model: {MODEL_NAME} (switched from flan-t5-small to fix config dim bug)\\n\")\n",
    "        f.write(f\"- Dataset Size: {DATASET_SIZE}\\n\")\n",
    "        f.write(f\"- Methods: Prefix-Tuning\\n\")\n",
    "        if RUN_ABLATIONS:\n",
    "            f.write(f\"- Ablations: Enabled (including ablated variants); prefix ablation removes projection layer\\n\")\n",
    "        f.write(f\"- Special: Native DynamicCache support; correct dims (num_heads=8, head_dim=64)\\n\\n\")\n",
    "        f.write(f\"## Summary Table\\n\\n\")\n",
    "        f.write(df.to_markdown(index=False))\n",
    "        f.write(\"\\n\\n## Learning Curves\\n\")\n",
    "        for exp_name, plot_path in plot_paths.items():\n",
    "            relative_plot_path = os.path.relpath(plot_path, start=report_dir)\n",
    "            f.write(f\"- [{exp_name}]({relative_plot_path})\\n\")\n",
    "        if RUN_ABLATIONS and ablation_plot_paths:\n",
    "            f.write(\"\\n## Ablation Comparisons\\n\")\n",
    "            for task_name, plot_path in ablation_plot_paths.items():\n",
    "                relative_plot_path = os.path.relpath(plot_path, start=report_dir)\n",
    "                f.write(f\"- [{task_name.capitalize()} Ablation Comparison]({relative_plot_path})\\n\")\n",
    "    \n",
    "    print(f\"Report saved to '{report_path}' (includes plot links)\")\n",
    "\n",
    "    # Generate dynamic outcome insights based on results\n",
    "    print(\"\\nOUTCOME INSIGHTS:\")\n",
    "    if results:\n",
    "        # General insights from trainable params and metrics\n",
    "        for task in tasks.keys():\n",
    "            task_exps = {k: v for k, v in results.items() if k.endswith(task)}\n",
    "            if task_exps:\n",
    "                # Find method with lowest trainable %\n",
    "                min_trainable_method = min(task_exps, key=lambda k: 100 * task_exps[k][\"trainable_params\"] / task_exps[k][\"total_params\"])\n",
    "                min_pct = 100 * task_exps[min_trainable_method][\"trainable_params\"] / task_exps[min_trainable_method][\"total_params\"]\n",
    "                print(f\"- For {task.capitalize()}, {min_trainable_method.split('_')[0].upper()} has the lowest trainable params ({min_pct:.2f}%).\")\n",
    "                \n",
    "                # Find best performing method (use key metric)\n",
    "                key_metric = 'eval_accuracy' if task == 'classification' else 'eval_rougeL'\n",
    "                best_method = max(task_exps, key=lambda k: task_exps[k][\"test_metrics\"].get(key_metric, 0))\n",
    "                best_score = task_exps[best_method][\"test_metrics\"].get(key_metric, 0)\n",
    "                print(f\"- {best_method.split('_')[0].upper()} achieves the highest {key_metric.replace('eval_', '').upper()} score ({best_score:.4f}) on {task.capitalize()}.\")\n",
    "        \n",
    "        # Ablation-specific insights\n",
    "        if RUN_ABLATIONS:\n",
    "            for exp_name, exp_data in results.items():\n",
    "                if \"_ablated_\" in exp_name:\n",
    "                    method_task_split = exp_name.split('_ablated_')[0]\n",
    "                    task = exp_name.split('_', 1)[1] # Get task name\n",
    "                    base_method_name = f\"{method_task_split}_{task}\"\n",
    "                    \n",
    "                    if base_method_name in results:\n",
    "                        base_metrics = results[base_method_name][\"test_metrics\"]\n",
    "                        delta = {k: exp_data[\"test_metrics\"].get(k, 0) - base_metrics.get(k, 0) for k in base_metrics if \"eval_\" in k}\n",
    "                        key_delta = delta.get('eval_accuracy' if task == 'classification' else 'eval_rougeL', 0)\n",
    "                        impact = \"degradation\" if key_delta < 0 else \"improvement\" if key_delta > 0 else \"no change\"\n",
    "                        print(f\"- Ablation in {exp_name.upper()} leads to {impact} in performance (delta: {key_delta:.4f}).\")\n",
    "        \n",
    "        print(f\"View plots in {OUTPUT_DIR}/plots/ for detailed curves (loss/metric vs step) and comparisons.\")\n",
    "else:\n",
    "    print(\"\\nNo results were generated. Check the training loop for errors.\")\n",
    "\n",
    "# In[15]:\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUCCESS - Prefix-Tuning method completed!\" + (\" With ablations!\" if RUN_ABLATIONS else \"\"))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480230ea-9d0c-4e9d-a4fc-d946728f955c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL Virtual Environment",
   "language": "python",
   "name": "dl-virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
